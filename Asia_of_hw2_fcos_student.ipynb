{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/dnn2022/blob/main/Asia_of_hw2_fcos_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VBJpAMzglRo"
      },
      "source": [
        "# Anchor-free single-stage object detection with FCOS (v2)\n",
        "\n",
        "In this exercise your goal will be to solve an object detection training and prediction task using the anchor-free single-stage approach.\n",
        "\n",
        "There are 10 points to get in total.\n",
        "\n",
        "## TLDR; overview\n",
        "\n",
        "In this task one should:\n",
        "- build an object detection model using the variant of `FCOS`,\n",
        "- train an object detection model.\n",
        "\n",
        "Hints and comments:\n",
        "\n",
        "- Model architecture and loss are heavily inspired by [FCOS](https://arxiv.org/pdf/1904.01355.pdf) paper,\n",
        "- you can freely subclass and extend the interface of classes in this exercise,\n",
        "- be sure that you understand the concept of anchor-free object detection. There are many tutorials and articles about it (e.g. [this](https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c) one)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcKkG56SItK"
      },
      "source": [
        "### Notebook changelog (compared to the initial version)\n",
        "Changed in v2:\n",
        "- Added definition of $\\sigma$ in the scoring formula.\n",
        "- Added the description how the `target` variable should look like.\n",
        "- Fixed the typo about mismatched `in_channels` and `out_channels` in the classification head description and added the whole info about it in the regression head description.\n",
        "- Added information about 1-element batch.\n",
        "- Fixed typehint in `BackboneWithFPN` (`forward(self, x: MnistCanvas)` -> `forward(self, x: torch.Tensor)`)\n",
        "- Removed info about non-existing exercise (\"...so use the foreground mask from the previous excercise.\" -> \"... so use the foreground mask.\")\n",
        "- Fixed typo: \"use `self.box_coder.decode_single` and `self.box_coder.decode_single`\" -> use \"`self.box_coder.encode_single` and `self.box_coder.decode_single`\"\n",
        "- Removed mentions of non-existing `TargetDecoder.get_predictions` and rotation.\n",
        "- Removed additional TODO placeholder from detection post-processing.\n",
        "- Added the information about using the different `evaluate` parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsyO2OdlLLE"
      },
      "source": [
        "### Data description\n",
        "\n",
        "In this task we will paste bounding boxes with digits **from 1 to 5** randomly selected from `MNIST` dataset on a canvas of size `(128, 128)` and **randomly scaled by a factor between 0.5 and 1.0**. We assume that:\n",
        "\n",
        "- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n",
        "- the digits are fully contained in canvas,\n",
        "- boxes are modeled using `MnistBox` class,\n",
        "- canvas is modeled using `MnistCanvas` class.\n",
        "\n",
        "Let us have a look at definition of these classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "id": "L1rAdIiRq2G8"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing import Optional\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MnistBox:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        x_max: int,\n",
        "        y_max: int,\n",
        "        class_nb: Optional[int] = None,\n",
        "        rotated: Optional[bool] = None,\n",
        "    ):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "        self.y_min = y_min\n",
        "        self.y_max = y_max\n",
        "        self.class_nb = class_nb\n",
        "        self.rotated = rotated\n",
        "    \n",
        "    @property\n",
        "    def x_diff(self):\n",
        "        return self.x_max - self.x_min\n",
        "    \n",
        "    @property\n",
        "    def y_diff(self):\n",
        "        return self.y_max - self.y_min\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Mnist Box: x_min = {self.x_min},' +\\\n",
        "               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n",
        "               f' y_max = {self.y_max}. Class = {self.class_nb}.' +\\\n",
        "               f' Rotated = {self.rotated}.'\n",
        "\n",
        "    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n",
        "        ax.add_patch(\n",
        "            patches.Rectangle(\n",
        "                (self.y_min, self.x_min),\n",
        "                 self.y_diff,\n",
        "                 self.x_diff,\n",
        "                 linewidth=1,\n",
        "                 edgecolor=color,\n",
        "                 facecolor='none',\n",
        "            )\n",
        "        )\n",
        "        ax.text(\n",
        "            self.y_min,\n",
        "            self.x_min,\n",
        "            f'{self.class_nb}' if not self.rotated else f'{self.class_nb}*',\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def area(self):\n",
        "        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n",
        "\n",
        "    def iou_with(self, other_box: \"MnistBox\"):\n",
        "        aux_box = MnistBox(\n",
        "            x_min=max(self.x_min, other_box.x_min),\n",
        "            x_max=min(self.x_max, other_box.x_max),\n",
        "            y_min=max(self.y_min, other_box.y_min),\n",
        "            y_max=min(self.y_max, other_box.y_max),\n",
        "        ) \n",
        "        return aux_box.area / (self.area + other_box.area - aux_box.area)\n",
        "\n",
        "\n",
        "class MnistCanvas:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        boxes: List[MnistBox],\n",
        "    ):\n",
        "        self.image = image\n",
        "        self.boxes = boxes\n",
        "        self.shape = (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "\n",
        "    def add_digit(\n",
        "        self,\n",
        "        digit: np.ndarray,\n",
        "        class_nb: int,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        rotated=None,\n",
        "        iou_threshold=0.1,\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Add a digit to an image if it does not overlap with existing boxes\n",
        "        above iou_threshold.\n",
        "        \"\"\"\n",
        "        image_x, image_y = digit.shape\n",
        "        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n",
        "            raise ValueError('Wrong initial corner box')\n",
        "        new_box_x_min = x_min\n",
        "        new_box_y_min = y_min\n",
        "        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n",
        "        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n",
        "        new_box = MnistBox(\n",
        "            x_min=new_box_x_min,\n",
        "            x_max=new_box_x_max,\n",
        "            y_min=new_box_y_min,\n",
        "            y_max=new_box_y_max,\n",
        "            class_nb=class_nb,\n",
        "            rotated=rotated,\n",
        "        )\n",
        "        old_background = self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ]\n",
        "        for box in self.boxes:\n",
        "            if new_box.iou_with(box) > iou_threshold:\n",
        "                return False\n",
        "        self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ] = np.maximum(old_background, digit)\n",
        "        self.boxes.append(\n",
        "            new_box\n",
        "        ) \n",
        "        return True\n",
        "        \n",
        "    def get_torch_tensor(self) -> torch.Tensor:\n",
        "        np_image = self.image.astype('float32')\n",
        "        np_image = np_image.reshape(\n",
        "            (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "        )\n",
        "        return torch.from_numpy(np_image).to(DEVICE)\n",
        "\n",
        "    @classmethod\n",
        "    def get_empty_of_size(cls, size: Tuple[int, int]):\n",
        "        return cls(\n",
        "            image=np.zeros(size),\n",
        "            boxes=[],\n",
        "        )\n",
        "\n",
        "    def plot(self, boxes: Optional[List[MnistBox]] = None):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(self.image)\n",
        "        boxes = boxes or self.boxes\n",
        "        for box in boxes:\n",
        "            box.plot_on_ax(ax)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqrSxb66RK-f"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWMxgsgFtlze"
      },
      "source": [
        "Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "HezSZXw4z-cx"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import skimage.transform as st\n",
        "\n",
        "\n",
        "mnist_data = mnist.load_data()\n",
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n",
        "\n",
        "\n",
        "def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n",
        "    bool_digit = digit > threshold\n",
        "    x_range = bool_digit.max(axis=0)\n",
        "    y_range = bool_digit.max(axis=1)\n",
        "    start_x = (x_range.cumsum() == 0).sum()\n",
        "    end_x = (x_range[::-1].cumsum() == 0).sum()\n",
        "    start_y = (y_range.cumsum() == 0).sum()\n",
        "    end_y = (y_range[::-1].cumsum() == 0).sum()\n",
        "    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n",
        "\n",
        "\n",
        "TRAIN_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_train[:10000])\n",
        "]\n",
        "TRAIN_CLASSES = mnist_y_train[:10000]\n",
        "\n",
        "TEST_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_test[:1000])\n",
        "]\n",
        "TEST_CLASSES = mnist_y_test[:1000]\n",
        "\n",
        "\n",
        "def get_random_canvas(\n",
        "    digits: Optional[List[np.ndarray]] = None,\n",
        "    classes: Optional[List[int]] = None,\n",
        "    nb_of_digits: Optional[int] = None,\n",
        "    labels = [0, 1, 2, 3, 4]\n",
        "    ) -> MnistCanvas:\n",
        "    digits = digits if digits is not None else TRAIN_DIGITS\n",
        "    classes = classes if classes is not None else TRAIN_CLASSES\n",
        "    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n",
        "    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n",
        "    attempts_done = 0\n",
        "    while attempts_done < nb_of_digits:\n",
        "        current_digit_index = np.random.randint(len(digits))\n",
        "        current_digit_class = classes[current_digit_index]\n",
        "        if current_digit_class not in labels:\n",
        "            continue\n",
        "        rescale = np.random.random() > 0.5\n",
        "        current_digit = digits[current_digit_index]\n",
        "        if rescale:\n",
        "            factor = (np.random.random() / 2) + 0.5\n",
        "            current_digit = st.resize(\n",
        "                current_digit, \n",
        "                (int(current_digit.shape[0] * factor), int(current_digit.shape[1] * factor)))\n",
        "            # current_digit = np.rot90(current_digit)\n",
        "        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n",
        "        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n",
        "        if new_canvas.add_digit(\n",
        "            digit=current_digit,\n",
        "            x_min=random_x_min,\n",
        "            y_min=random_y_min,\n",
        "            class_nb=current_digit_class,\n",
        "            rotated=rescale,\n",
        "        ):\n",
        "            attempts_done += 1\n",
        "    return new_canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i2OjUEC7eaC"
      },
      "source": [
        "Let us have a look at example canvas (rescaled digits have additional *added to description)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "OsLpINOtvhd8",
        "outputId": "9f635439-66d4-4bc0-a922-fa9f84009a9f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "mnist_canvas = get_random_canvas()\n",
        "mnist_canvas.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgU2bKyJ3pVh"
      },
      "source": [
        "For training one can either:\n",
        "- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n",
        "- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GpJZUkDGJSi"
      },
      "source": [
        "### Model building (5 pt.)\n",
        "\n",
        "\n",
        "One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdCBH9PkT2C"
      },
      "source": [
        "#### $\\texttt{backbone}$:\n",
        "\n",
        "We provided you with a backbone model architecture paired with Feature Pyramid Network (`BackboneWithFPN`) that accepts a `MnistCanvas` instance and output a dictionary, which has a FPN group name as a keys and their tensors as value.\n",
        "For a FPN with strides set to [32, 64, 128] and number of output channels set to 64, the sizes of the tensors will be [1, 64, 128, 128], [1, 64, 64, 64], [1, 64, 32, 32] consecutively. This module should be trained together with the rest of your solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 290,
      "metadata": {
        "id": "kXco8riNGHhl"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from torch import nn, Tensor\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "\n",
        "\n",
        "class Backbone(torch.nn.Module):\n",
        "    def __init__(self, strides = [8, 16, 32]):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.first_block = torch.nn.Sequential(\n",
        "            nn.Conv2d(1, strides[0], (3, 3), padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [torch.nn.Sequential(*[\n",
        "                nn.Conv2d(strides[i-1], strides[i], (3, 3), padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2),\n",
        "              ]) for i in range(1, len(strides))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        image = x.to(DEVICE).view(1, 1, 128, 128)\n",
        "        x = self.first_block(image)\n",
        "        aux = [x]\n",
        "        for block in self.blocks:\n",
        "            x = block(aux[-1])\n",
        "            aux.append(x)\n",
        "        return aux\n",
        "\n",
        "\n",
        "class BackboneWithFPN(torch.nn.Module):\n",
        "    def __init__(self, strides, out_channels=32) -> None:\n",
        "        super().__init__()\n",
        "        self.strides = strides\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone = Backbone(self.strides)\n",
        "        self.fpn = FeaturePyramidNetwork(self.strides, self.out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output_backbone = self.backbone(x)\n",
        "        \n",
        "        x = OrderedDict()\n",
        "        for i, f in enumerate(output_backbone):\n",
        "            x[f'feat{i}'] = f\n",
        "        output_fpn = self.fpn(x)\n",
        "        return output_fpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkOLl12nkiI8"
      },
      "source": [
        "#### $\\texttt{anchor generator}$:\n",
        "\n",
        "FCOS is anchor-free in a typical sense of this word, but it can also be said that there is one pixel-wise \"anchor\" per localisation on a given feature map.\n",
        "Therefore, anchor generator from `torchvision` is used for convenience.\n",
        "You will obtain $128^2 + 64^2 + 32^2 = 21504$ locations in total for the previously chosen strides.\n",
        "They will be called anchors in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyd2-zOxf9VM",
        "outputId": "6738babe-1f12-4090-e558-5e71eb7efcea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ],
      "source": [
        "# example code - anchor generator is already included in the code later\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "\n",
        "anchor_sizes = ((32,), (64,), (128,))  # equal to strides of FPN multi-level feature map\n",
        "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor for each level\n",
        "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "anchor_generator.num_anchors_per_location()\n",
        "# notice that effectively one anchor is one location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 292,
      "metadata": {
        "id": "iM3oSesif-F5"
      },
      "outputs": [],
      "source": [
        "# Later in the code you will use the anchor generator in the following way:\n",
        "# anchors = anchor_generator(images, features)\n",
        "# [x.size(2) * x.size(3) for x in features] # recover level sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTYyCuMdmBk7"
      },
      "source": [
        "#### $\\texttt{FCOSClassificationHead}$ (1 pt.):\n",
        "\n",
        "Write a classification head to be used in FCOS.\n",
        "The input is is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$).\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The additional final block should be `nn.Conv2d` outputting `C` channels.\n",
        "The final output should consist of classification logits of shape `(N, A, C)`, where `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_all(tensors: List[Tensor]) -> Tensor: # (N, C, S, S)\n",
        "  tensors = [tensor.transpose(2, 3) for tensor in tensors]  # (N, C, S, S) to work with get_tensor\n",
        "  tensors = [tensor.reshape(*tensor.shape[:2], -1) for tensor in tensors]  # (N, C, S * S)\n",
        "  tensor =  torch.cat(tensors, dim=2) # (N, C, sum S*S)\n",
        "  tensor = tensor.transpose(1, 2) # (N, sum S*S, C)\n",
        "  return tensor \n"
      ],
      "metadata": {
        "id": "kz1Bui2q6NTr"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 304,
      "metadata": {
        "id": "kpDP9QVjnn05"
      },
      "outputs": [],
      "source": [
        "class FCOSClassificationHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_convs: int = 4,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        size = in_channels\n",
        "        c = num_classes\n",
        "\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs\n",
        "        \n",
        "\n",
        "        self.final = nn.Conv2d(size, c, kernel_size=3, padding=1)\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tensor:\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        \n",
        "        output = [self.blocks(layer) for layer in x]  # (N, in_channels, S, S)\n",
        "        output = [self.final(layer) for layer in output]  # (N, C, S, S)\n",
        "        # TODO: check dims\n",
        "        output = reshape_all(output)\n",
        "        print(f\"out: {output.shape}\") # (N, S*S, C)\n",
        "        return output\n",
        "\n",
        "        ################################################################################################\n",
        "        # end of your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcHAnQy2mFJU"
      },
      "source": [
        "#### $\\texttt{FCOSRegressionHead}$  (1 pt.):\n",
        "\n",
        "Write a regression head to be used in FCOS - both for bounding boxes and center-ness.\n",
        "The input is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$), which will be shared for regression and center-ness.\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The final block for bounding box regression should be `nn.Conv2d` and have `4` channels and it should be followed by relu functional to get rid of negative values.\n",
        "The final block for center-ness regression should be `nn.Conv2d` and have `1` channel.\n",
        "The output should consist of a tuple of tensors (bounding box regression and center-ness).\n",
        "Bounding box regression logits should be of shape `(N, A, 4)`, whereas for center-ness that would be `(N, A, 1)`.\n",
        "Similarly, `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "3H6KNzi1mEhg"
      },
      "outputs": [],
      "source": [
        "class FCOSRegressionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_convs: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        size = in_channels\n",
        "\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs\n",
        "\n",
        "        self.bounding = nn.Sequential(\n",
        "            nn.Conv2d(size, 4, kernel_size=3, padding=1), \n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.center = nn.Conv2d(size, 4, kernel_size=3, padding=1)\n",
        "        # end of your code\n",
        "        ################################################################################################\n",
        "        \n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
        "        pass # TODO: your code here\n",
        "        ################################################################################################\n",
        "        output = [self.blocks(i) for i in x] # (N, in_channels, S, S)\n",
        "        bounding = [self.bounding(out) for out in output] # (N, 4, S, S)\n",
        "        center = [self.center(out) for out in output] # (N, 4, S, S)\n",
        "        \n",
        "        bounding = reshape_all(bounding)\n",
        "        center = reshape_all(center)\n",
        "\n",
        "        print(f'bounding: {bounding.shape}, center: {center.shape}')\n",
        "\n",
        "        return bounding, center\n",
        "        ################################################################################################\n",
        "        # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0I47ydkvUL"
      },
      "source": [
        "#### $\\texttt{FCOSHead}$ (2 pt.):\n",
        "\n",
        "Here, the computation of the foreground indices and losses takes place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "PX8P0s-jOK-F"
      },
      "outputs": [],
      "source": [
        "class BoxLinearCoder:\n",
        "    \"\"\"\n",
        "    The linear box-to-box transform defined in FCOS. The transformation is parameterized\n",
        "    by the distance from the center of (square) src box to 4 edges of the target box.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalize_by_size: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            normalize_by_size (bool): normalize deltas by the size of src (anchor) boxes.\n",
        "        \"\"\"\n",
        "        self.normalize_by_size = normalize_by_size\n",
        "\n",
        "    def encode_single(self, reference_boxes: Tensor, proposals: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Encode a set of proposals with respect to some reference boxes\n",
        "\n",
        "        Args:\n",
        "            reference_boxes (Tensor): reference boxes\n",
        "            proposals (Tensor): boxes to be encoded\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the encoded relative box offsets that can be used to\n",
        "            decode the boxes.\n",
        "        \"\"\"\n",
        "        # get the center of reference_boxes\n",
        "        reference_boxes_ctr_x = 0.5 * (reference_boxes[:, 0] + reference_boxes[:, 2])\n",
        "        reference_boxes_ctr_y = 0.5 * (reference_boxes[:, 1] + reference_boxes[:, 3])\n",
        "\n",
        "        # get box regression transformation deltas\n",
        "        target_l = reference_boxes_ctr_x - proposals[:, 0]\n",
        "        target_t = reference_boxes_ctr_y - proposals[:, 1]\n",
        "        target_r = proposals[:, 2] - reference_boxes_ctr_x\n",
        "        target_b = proposals[:, 3] - reference_boxes_ctr_y\n",
        "\n",
        "        targets = torch.stack((target_l, target_t, target_r, target_b), dim=1)\n",
        "        if self.normalize_by_size:\n",
        "            reference_boxes_w = reference_boxes[:, 2] - reference_boxes[:, 0]\n",
        "            reference_boxes_h = reference_boxes[:, 3] - reference_boxes[:, 1]\n",
        "            reference_boxes_size = torch.stack(\n",
        "                (reference_boxes_w, reference_boxes_h, reference_boxes_w, reference_boxes_h), dim=1\n",
        "            )\n",
        "            targets = targets / reference_boxes_size\n",
        "\n",
        "        return targets\n",
        "\n",
        "    def decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        From a set of original boxes and encoded relative box offsets,\n",
        "        get the decoded boxes.\n",
        "\n",
        "        Args:\n",
        "            rel_codes (Tensor): encoded boxes\n",
        "            boxes (Tensor): reference boxes.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the predicted boxes with the encoded relative box offsets.\n",
        "        \"\"\"\n",
        "\n",
        "        boxes = boxes.to(rel_codes.dtype)\n",
        "\n",
        "        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n",
        "        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n",
        "        if self.normalize_by_size:\n",
        "            boxes_w = boxes[:, 2] - boxes[:, 0]\n",
        "            boxes_h = boxes[:, 3] - boxes[:, 1]\n",
        "            boxes_size = torch.stack((boxes_w, boxes_h, boxes_w, boxes_h), dim=1)\n",
        "            rel_codes = rel_codes * boxes_size\n",
        "\n",
        "        pred_boxes1 = ctr_x - rel_codes[:, 0]\n",
        "        pred_boxes2 = ctr_y - rel_codes[:, 1]\n",
        "        pred_boxes3 = ctr_x + rel_codes[:, 2]\n",
        "        pred_boxes4 = ctr_y + rel_codes[:, 3]\n",
        "        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=1)\n",
        "        return pred_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loss calculation\n",
        "Compute the losses. \n",
        "They should be calculated on the positive locations/anchors, so use the foreground mask.\n",
        "For regression, use `self.box_coder.encode_single` and `self.box_coder.decode_single` to move between standard (x, y, x, y) and FCOS (l, t, r, b) bounding box format.\n",
        "There are three losses to be written.\n",
        "- classification loss (with `torchvision.ops.sigmoid_focal_loss`). (1 pt.)\n",
        "- Bounding box regression (with `torchvision.ops.generalized_box_iou_loss`). Decode predictions with `self.box_coder.decode_single` before regressing against the ground truth. (1 pt.)\n",
        "- ctrness loss (`torchvision.ops.sigmoid_focal_loss`). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)"
      ],
      "metadata": {
        "id": "qIGit7Yp_zEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "nDX5c0lw9ZxM"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from torchvision.ops import sigmoid_focal_loss, generalized_box_iou_loss\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "\n",
        "class FCOSHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A regression and classification head for use in FCOS.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_classes (int): number of classes to be predicted\n",
        "        num_convs (Optional[int]): number of conv layer of head. Default: 4.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, num_classes: int, num_convs: Optional[int] = 4) -> None:\n",
        "        super().__init__()\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.classification_head = FCOSClassificationHead(in_channels, num_classes, num_convs)\n",
        "        self.regression_head = FCOSRegressionHead(in_channels, num_convs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],             # anchors/locations\n",
        "        matched_idxs: List[Tensor],        # tells to which bounding box anchors are matched, -1 mean no matches\n",
        "    ) -> Dict[str, Tensor]:\n",
        "\n",
        "        cls_logits = head_outputs[\"cls_logits\"]  # [N, A, C]\n",
        "        bbox_regression = head_outputs[\"bbox_regression\"]  # [N, A, 4]\n",
        "        bbox_ctrness = head_outputs[\"bbox_ctrness\"]  # [N, A, 1]\n",
        "        \n",
        "      #  print(f\"cls: {cls_logits[0][0].shape}, reg: {bbox_regression[0][0].shape}, ctr: {bbox_ctrness[0][0].shape}\")\n",
        "\n",
        "        all_gt_classes_targets = []\n",
        "        all_gt_boxes_targets = []\n",
        "        print(len(matched_idxs))\n",
        "        \n",
        "        for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):\n",
        "            im = targets_per_image[\"labels\"]\n",
        "            print(im)\n",
        "            matches = matched_idxs_per_image\n",
        "            print(matches.shape)\n",
        "            t = targets_per_image[\"boxes\"]\n",
        "            print(t)\n",
        "            gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n",
        "            gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n",
        "            gt_classes_targets[matched_idxs_per_image < 0] = -1  # background\n",
        "            all_gt_classes_targets.append(gt_classes_targets)\n",
        "            all_gt_boxes_targets.append(gt_boxes_targets)\n",
        "\n",
        "        all_gt_classes_targets = torch.stack(all_gt_classes_targets)\n",
        "        \n",
        "        foregroud_mask = all_gt_classes_targets >= 0        \n",
        "        num_foreground = foregroud_mask.sum().item()\n",
        "        \n",
        "        loss_cls = self.compute_loss_cls(cls_logits, all_gt_classes_targets, foregroud_mask)\n",
        "        loss_bbox_reg = self.compute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n",
        "        loss_bbox_ctrness = self.compute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n",
        "\n",
        "        return {\n",
        "            \"classification\": loss_cls / max(1, num_foreground),\n",
        "            \"bbox_regression\": loss_bbox_reg / max(1, num_foreground),\n",
        "            \"bbox_ctrness\": loss_bbox_ctrness / max(1, num_foreground),\n",
        "        }\n",
        "\n",
        "    def compute_loss_ctrness(self, anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_regression = bbox_regression[foregroud_mask]\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        targets = self.box_coder.encode_single(anchors, all_gt_boxes_targets)\n",
        "        l = targets[:, 0]\n",
        "        t = targets[:, 1]\n",
        "        r = targets[:, 2]\n",
        "        b = targets[:, 3]\n",
        "        # Equation 3 from paper\n",
        "        targets = torch.sqrt((torch.min(l, r) / torch.max(l, r)) * (torch.min(t, b) / torch.max(t, b)))\n",
        "        return torchvision.ops.sigmoid_focal_loss(bbox_regression, targets, alpha=0.9, reduction=\"sum\")\n",
        "\n",
        "    def compute_loss_bbox_reg(self, anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_regression = bbox_regression[foregroud_mask]\n",
        "        bbox_regression = self.box_coder.decode_single(bbox_regression, anchors)\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        # Both sets of boxes are expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and 0 <= y1 < y2\n",
        "        # So we need to change the order of\n",
        "        return torchvision.ops.generalized_box_iou_loss(bbox_regression, all_gt_boxes_targets, reduction=\"sum\")\n",
        "\n",
        "    def compute_loss_cls(self, cls_logits, all_gt_classes_targets, foregroud_mask):\n",
        "        masked = torch.zeros(cls_logits.shape, device=cls_logits.device)\n",
        "        masked[foregroud_mask] = nn.functional.one_hot(all_gt_classes_targets[foregroud_mask], num_classes=cls_logits.shape[-1])\n",
        "        return torchvision.ops.sigmoid_focal_loss(cls_logits, masked, alpha=0.9, reduction=\"sum\")\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n",
        "        cls_logits = self.classification_head(x)\n",
        "        bbox_regression, bbox_ctrness = self.regression_head(x)\n",
        "        return {\n",
        "            \"cls_logits\": cls_logits,\n",
        "            \"bbox_regression\": bbox_regression,\n",
        "            \"bbox_ctrness\": bbox_ctrness,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFYNzC9KOK-G"
      },
      "source": [
        "#### Post-processing (1 pt.)\n",
        "Fill the gaps in the postprocessing routine.\n",
        "The paper states: \n",
        "\n",
        "> (...) the final score (used for ranking the detected bounding boxes) \n",
        "> is computed by multiplying the predicted center-ness with the corresponding classification score. \n",
        "> Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. \n",
        "> As a result, with high probability, these low-quality bounding boxes might be filtered out by \n",
        "> the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n",
        "\n",
        "1. Remove boxes with score smaller than `self.score_thresh`. The score is given by `sqrt`($\\sigma$(`classification_score`) * $\\sigma$(`cente-ness_score`)), where $\\sigma$ stands for the sigmoid function. (1pt.)\n",
        "2. Keep only top `self.topk_candidates` scoring predictions (1pt.)\n",
        "\n",
        "The `compute_loss` function here calculates the indexes of matched classes for each anchor/location for your convenience in later calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "id": "EB17o3zs1JiR"
      },
      "outputs": [],
      "source": [
        "class FCOS(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        num_classes: int,\n",
        "        # transform parameters\n",
        "        image_mean: Optional[List[float]] = None,\n",
        "        image_std: Optional[List[float]] = None,\n",
        "        # Anchor parameters\n",
        "        anchor_generator: AnchorGenerator = None,\n",
        "        center_sampling_radius: float = 1.5,\n",
        "        score_thresh: float = 0.2,\n",
        "        nms_thresh: float = 0.6,\n",
        "        detections_per_img: int = 100,\n",
        "        topk_candidates: int = 1000,\n",
        "        num_convs_in_heads:int = 4,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.anchor_generator = anchor_generator\n",
        "        self.head = FCOSHead(backbone.out_channels, num_classes, num_convs=num_convs_in_heads)\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.transform = GeneralizedRCNNTransform(128, 128, image_mean, image_std, **kwargs)\n",
        "\n",
        "        self.center_sampling_radius = center_sampling_radius\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "        self.topk_candidates = topk_candidates\n",
        "\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],\n",
        "        num_anchors_per_level: List[int],\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        matched_idxs = []\n",
        "        for anchors_per_image, targets_per_image in zip(anchors, targets): # batch\n",
        "            if targets_per_image[\"boxes\"].numel() == 0:\n",
        "                matched_idxs.append(\n",
        "                    torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device)\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            gt_boxes = targets_per_image[\"boxes\"]\n",
        "            print(anchors_per_image)\n",
        "            print(gt_boxes)\n",
        "            gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2                 # Calculate centres of bounding boxes\n",
        "            anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N  \n",
        "            anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]            # Match anchors\n",
        "            # center sampling: anchor point must be close enough to gt center.\n",
        "            pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n",
        "                dim=2\n",
        "            ).values < self.center_sampling_radius * anchor_sizes[:, None]\n",
        "            # compute pairwise distance between N points and M boxes\n",
        "            x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n",
        "            x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
        "            pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n",
        "\n",
        "            # anchor point must be inside gt\n",
        "            pairwise_match &= pairwise_dist.min(dim=2).values > 0\n",
        "\n",
        "            # each anchor is only responsible for certain scale range.\n",
        "            lower_bound = anchor_sizes * 4\n",
        "            lower_bound[: num_anchors_per_level[0]] = 0\n",
        "            upper_bound = anchor_sizes * 8\n",
        "            upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n",
        "            pairwise_dist = pairwise_dist.max(dim=2).values\n",
        "            pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])\n",
        "\n",
        "            # match the GT box with minimum area, if there are multiple GT matches\n",
        "            gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n",
        "            pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n",
        "            min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n",
        "            matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n",
        "\n",
        "            print(matched_idx.shape)\n",
        "            matched_idxs.append(matched_idx)\n",
        "        # end of your code\n",
        "        # matched index - anchor-to-target match\n",
        "        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n",
        "\n",
        "    def postprocess_detections(\n",
        "        self, head_outputs: Dict[str, List[Tensor]], anchors: List[List[Tensor]], image_shapes: List[Tuple[int, int]]\n",
        "    ) -> List[Dict[str, Tensor]]:\n",
        "        class_logits = head_outputs[\"cls_logits\"]\n",
        "        box_regression = head_outputs[\"bbox_regression\"]\n",
        "        box_ctrness = head_outputs[\"bbox_ctrness\"]\n",
        "\n",
        "        \n",
        "        num_images = len(image_shapes)\n",
        "\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "\n",
        "        for index in range(num_images):\n",
        "            box_regression_per_image = [br[index] for br in box_regression]\n",
        "            logits_per_image = [cl[index] for cl in class_logits]\n",
        "            box_ctrness_per_image = [bc[index] for bc in box_ctrness]\n",
        "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
        "\n",
        "            image_boxes = []\n",
        "            image_scores = []\n",
        "            image_labels = []\n",
        "\n",
        "            for box_regression_per_level, logits_per_level, box_ctrness_per_level, anchors_per_level in zip(\n",
        "                box_regression_per_image, logits_per_image, box_ctrness_per_image, anchors_per_image\n",
        "            ):\n",
        "                num_classes = logits_per_level.shape[-1]\n",
        "                \n",
        "                # TODO: your code here\n",
        "                # Remove low scoring boxes and keep only top k scoring predictions\n",
        "                ################################################################################################\n",
        "                max_logits = logits_per_level.max(dim=-1)\n",
        "                scores_per_level = torch.sqrt(\n",
        "                    torch.sigmoid(max_logits.values) * torch.sigmoid(box_ctrness_per_level.squeeze(dim=1)))\n",
        "                \n",
        "                top = scores_per_level > self.score_thresh\n",
        "\n",
        "                box_regression_per_level = box_regression_per_level[top]\n",
        "                anchors_per_level = anchors_per_level[top]\n",
        "                scores_per_level = scores_per_level[top]\n",
        "\n",
        "                if self.topk_candidates >= scores_per_level.shape[0]:\n",
        "                    topk_idxs = torch.sort(scores_per_level, descending=True).indices\n",
        "                else:\n",
        "                    topk_idxs = torch.topk(scores_per_level, self.topk_candidates).indices\n",
        "                \n",
        "\n",
        "                scores_per_level = scores_per_level[topk_idxs]\n",
        "\n",
        "                topk_idxs = max_logits.indices[top][topk_idxs] + topk_idxs * num_classes\n",
        "                ################################################################################################\n",
        "                # end of your code\n",
        "\n",
        "\n",
        "                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n",
        "                labels_per_level = topk_idxs % num_classes\n",
        "\n",
        "                boxes_per_level = self.box_coder.decode_single(\n",
        "                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n",
        "                )\n",
        "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
        "\n",
        "                image_boxes.append(boxes_per_level)\n",
        "                image_scores.append(scores_per_level)\n",
        "                image_labels.append(labels_per_level)\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)\n",
        "            image_scores = torch.cat(image_scores, dim=0)\n",
        "            image_labels = torch.cat(image_labels, dim=0)\n",
        "\n",
        "            # non-maximum suppression\n",
        "            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n",
        "            keep = keep[: self.detections_per_img]\n",
        "\n",
        "            detections.append(\n",
        "                {\n",
        "                    \"boxes\": image_boxes[keep],\n",
        "                    \"scores\": image_scores[keep],\n",
        "                    \"labels\": image_labels[keep],\n",
        "                }\n",
        "            )\n",
        "        return detections\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: List[Tensor],\n",
        "        targets: Optional[List[Dict[str, Tensor]]] = None,\n",
        "    ) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (list[Tensor]): images to be processed\n",
        "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
        "\n",
        "        Returns:\n",
        "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
        "                During training, it returns a dict[Tensor] which contains the losses.\n",
        "                During testing, it returns list[BoxList] contains additional fields\n",
        "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
        "        \"\"\"\n",
        "        \n",
        "        # transform the input (normalise with std and )\n",
        "        images, targets = self.transform(images, targets)\n",
        "\n",
        "        # get the features from the backbone\n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([(\"0\", features)])\n",
        "        features = list(features.values())\n",
        "\n",
        "        # compute the fcos heads outputs using the features\n",
        "        head_outputs = self.head(features)\n",
        "\n",
        "        # create the set of anchors\n",
        "        anchors = self.anchor_generator(images, features)\n",
        "        # recover level sizes\n",
        "        num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
        "        \n",
        "        losses = {}\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "        if self.training:\n",
        "            \n",
        "            losses = self.compute_loss(targets, head_outputs, anchors, num_anchors_per_level)\n",
        "            return losses\n",
        "        else:\n",
        "            # split outputs per level\n",
        "            split_head_outputs: Dict[str, List[Tensor]] = {}\n",
        "            for k in head_outputs:\n",
        "                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
        "            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
        "\n",
        "            # compute the detections\n",
        "            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
        "            return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA1Nvz6jagyP"
      },
      "source": [
        "### Metrics and evaluation (2 pt.)\n",
        "\n",
        "#### Digit Accuracy (1 pt.)\n",
        "\n",
        "This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]`, and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n",
        "\n",
        "- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class and `iou` overlap greater than `0.5`,\n",
        "- the number of `canvas` boxes match `len(predicted_boxes)`.\n",
        "\n",
        "The method shoud output `1` if there is a matching and `0` otherwise.\n",
        "\n",
        "#### Evaluation function (1 pt.)\n",
        "\n",
        "Write an evaluation function for your model.\n",
        "If needed, perform the final NMS with `torchvision.ops.nms` (threshold at `iou`) and remove redundant boxes with scores smaller than `min_score`.\n",
        "Then, calculate the average `DigitAccuracy` for each.\n",
        "You may experiment with parameters for this method, but the default ones are fine (this is not subject of our grading).\n",
        "If you are fine what you achiveded with `postprocess_detections`, you may focus solely on evaluation (although playing with this second stage NMS and score thresholding might be useful for diagnostics).\n",
        "\n",
        "The output of the method is the average digit accuracy on the test set. \n",
        "Use it to track your model performance over epochs.\n",
        "\n",
        "In principle, you can use different `iou` and `min_score`, although itd be slightly preferred to use the defaults here. Itll ease the comparison, we were able to get around 60% in 15 epochs with these values. With that said, if you tune these values for your model to improve the score there will be no point lost.\n",
        "unless you will do some sort of a hack to exploit the metric in a malicious way, but that will perhaps mean that theres something wrong in other parts of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "bfRZnbAP-XGG"
      },
      "outputs": [],
      "source": [
        "class DigitAccuracy:\n",
        "    def compute_metric  (\n",
        "        self,\n",
        "        predicted_boxes: List[MnistBox],\n",
        "        canvas: MnistCanvas,\n",
        "        iou: float = 0.5,\n",
        "    ):\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        pass\n",
        "        if len(predicted_boxes) != len(canvas.boxes):\n",
        "          return False\n",
        "\n",
        "        for true_box in canvas.boxes:\n",
        "          match_box = False\n",
        "          for predicted_box in predicted_boxes:\n",
        "            if true_box.eq(predicted_box, iou):\n",
        "              if match_box:\n",
        "                return False\n",
        "              match_box = True\n",
        "          if not match_box:\n",
        "            return False\n",
        "            \n",
        "        return True\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "\"\"\"\n",
        "detections.append(\n",
        "                {\n",
        "                    \"boxes\": image_boxes[keep],\n",
        "                    \"scores\": image_scores[keep],\n",
        "                    \"labels\": image_labels[keep],\n",
        "                }\n",
        "            )\n",
        "\"\"\"\n",
        "\n",
        "def evaluate(model: nn.Module, TEST_CANVAS, min_score: float = .2, iou: float = .05) -> float:\n",
        "    # TODO: your code here\n",
        "    ################################################################################################\n",
        "    model_ouputs = model(TEST_CANVAS)\n",
        "    scores = [d.scores for d in model_ouputs]\n",
        "\n",
        "    boxes = [canvas.boxes for canvas in TEST_CANVAS]\n",
        "\n",
        "    ids = torchvision.ops.nms(boxes, scores, iou_threshold=iou)\n",
        "\n",
        "    final_boxes = []\n",
        "\n",
        "    for idx in ids:\n",
        "      if model_ouputs[idx] > min_score:\n",
        "        fin = DigitAccuracy().compute_metric(boxes[idx], TEST_CANVAS[idx])\n",
        "        final_boxes.append(fin)\n",
        "\n",
        "    return final_boxes\n",
        "\n",
        "    ################################################################################################\n",
        "    # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15n5w-hhvRbS"
      },
      "source": [
        "### Train your model (3pt)\n",
        "\n",
        "One should use all classes defined above to train the model.\n",
        "\n",
        "- Train the model. A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data (2 pt.).\n",
        "\n",
        "What does the target variable look like?\n",
        "So, as in the typehint, the target is `List[Dict[str, Tensor]]`. Assuming a 1-element batch, it should then be a 1-element list containing a dictionary with two keys: `boxes` and their corresponding `labels`. The values are tensors. Assuming that we have 5 boxes in GT, the shapes are `(5, 4)` for boxes and (5) for labels. Naturally, you have to fill these tensors using values from `MnistCanvas`.\n",
        "\n",
        "**Hint:** Training can take a while to achieve the expected accuracy. It is normal that for many epochs at the beginning accuracy is constantly $0$. Do not worry as long as the loss is on average decreasing across epochs. You may want to reduce number of digit classes (for example only to generate `0`s on canvas) to test the convergence (the hyperparameters might change, though!). A model with around 500k parameters should be able to hit 10% of the metric in 20 minutes (tested on a 2021 MacBook on CPU). On Google Colab with GPU it will be matter of 2 minutes, but notice that the free GPU is limited.\n",
        "\n",
        "**Hint:** Use the 1-element batches. Some portions of the code are not ready for higher values.\n",
        "\n",
        "**Even more important hint:** Pay attention to the ordering of the X/Y values of `get_torch_tensor` and align it with your target!\n",
        "\n",
        "Good luck and have fun!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "\n",
        "TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n",
        "np.random.seed(TEST_SEED)\n",
        "\n",
        "N_TRAINING_EXAMPLES = 500\n",
        "LR = 0.0004 # for SGD with momentum=0.9\n",
        "EPOCHS = 16\n",
        "STRIDES = [32, 64, 128]\n",
        "CONVS_IN_HEADS = 4\n",
        "OUT_CHANNELS = 64\n",
        "LABELS = list(range(5))  # lower it for quick convergence testing\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "GyWNGqHhTJfa"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_targets(dataset: List[MnistCanvas]):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for input in dataset:\n",
        "    boxes = input.boxes\n",
        "    labels = [str(box.class_nb) for box in boxes]\n",
        "    inputs.append(input.get_torch_tensor())\n",
        "    x_mins = [b.x_min for b in boxes]\n",
        "    y_mins = [b.y_min for b in boxes]\n",
        "    x_maxs = [b.x_max for b in boxes]\n",
        "    y_maxs = [b.y_max for b in boxes]\n",
        "\n",
        "    d = {}\n",
        "    ten = torch.tensor([[x_min, y_min, x_max, y_max] for x_min, y_min, x_max, y_max in zip(x_mins, y_mins, x_maxs, y_maxs)], dtype=torch.float32, device=DEVICE)\n",
        "    d[\"boxes\"] = ten\n",
        "    d[\"labels\"] = labels\n",
        "    targets.append(d)\n",
        "  print(targets)\n",
        "  return inputs, targets\n",
        "\"\"\"\n",
        "  for canvas in dataset:\n",
        "    boxes = canvas.boxes\n",
        "    labels = [str(box.class_nb) for box in boxes]\n",
        "    tensor = canvas.get_torch_tensor()\n",
        "    print(f\"l: {len(labels)}, t: {tensor.shape}\")\n",
        "    targets.append({\n",
        "                \"labels\": labels,\n",
        "                \"boxes\": tensor,\n",
        "            })\n",
        "    \n",
        "    return targets\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "duTtHwPHXVNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "686198c4-6287-459f-c835-f9e8ee888837"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  for canvas in dataset:\\n    boxes = canvas.boxes\\n    labels = [str(box.class_nb) for box in boxes]\\n    tensor = canvas.get_torch_tensor()\\n    print(f\"l: {len(labels)}, t: {tensor.shape}\")\\n    targets.append({\\n                \"labels\": labels,\\n                \"boxes\": tensor,\\n            })\\n    \\n    return targets\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 301
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self, train_canvas_size: int, epochs: int):\n",
        "    self.train_canvas_size = train_canvas_size\n",
        "    self.epochs = epochs\n",
        "    self.digit_accuracy = DigitAccuracy()\n",
        "\n",
        "    self.train_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "    self.train_inputs, self.train_targets = get_targets(self.train_set)\n",
        "\n",
        "    self.test_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "    self.test_inputs, self.test_targets = get_targets(self.test_set)\n",
        "\n",
        "    self.anchor_sizes = tuple([(x,) for x in STRIDES])  # equal to strides of multi-level feature map\n",
        "    self.aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one \"anchor\" per location\n",
        "    self.anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "\n",
        "    self.fcos = FCOS(\n",
        "        backbone = BackboneWithFPN(strides=STRIDES, out_channels=OUT_CHANNELS), \n",
        "        num_classes = len(LABELS), \n",
        "        image_mean = [0.0233],\n",
        "        image_std = [0.14],\n",
        "        num_convs_in_heads = CONVS_IN_HEADS, \n",
        "        anchor_generator = anchor_generator,\n",
        "        detections_per_img = 100\n",
        "        )\n",
        "    self.fcos.to(DEVICE)\n",
        "    self.optimizer = optim.SGD(self.fcos.parameters(), 0.01, momentum=0.9) # change to adagrad\n",
        "\n",
        "# TODO: write your code here\n",
        "################################################################################################\n",
        "  def train(self):\n",
        "    train_loss, test_loss, test_acc = [], [], []\n",
        "    correct, not_correct = 0, 0\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        temp_loss = 0\n",
        "        self.fcos.train()\n",
        "        for i in range(len(self.train_inputs)):\n",
        "          train_input = self.train_inputs[i]\n",
        "          train_target = self.train_targets[i]\n",
        "          print(f'in: {train_input}, tar{type(train_target)}')\n",
        "          model_output = self.fcos.forward(train_input, [train_target])\n",
        "\n",
        "          loss = self.fcos.compute_loss(train_target, model_output, )\n",
        "          \n",
        "          temp_loss += loss\n",
        "\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "        \n",
        "        train_loss.append(temp_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch}, loss: {temp_loss}')\n",
        "\n",
        "################################################################################################"
      ],
      "metadata": {
        "id": "u5kwfx3RTGGq"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot example results of matched and mismatched predictions (0.5 pt.).\n",
        "- Plot particular losses and evaluation score per (0.5 pt.).\n"
      ],
      "metadata": {
        "id": "XmZgfAQbXIxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(100, 20)\n",
        "train_loss, test_loss, test_acc = trainer.train()\n"
      ],
      "metadata": {
        "id": "OVw6fdTG1JPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c08720bc-5203-4f1e-eec2-4998ff285bbc"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 24.,  13.,  40.,  22.],\n",
            "        [ 17.,  26.,  34.,  45.],\n",
            "        [ 74.,  20.,  91.,  39.],\n",
            "        [ 88., 104., 104., 117.]], device='cuda:0'), 'labels': ['4', '4', '0', '2']}, {'boxes': tensor([[64., 14., 81., 18.],\n",
            "        [46., 80., 59., 90.],\n",
            "        [49., 32., 68., 51.],\n",
            "        [39., 19., 58., 32.]], device='cuda:0'), 'labels': ['1', '1', '2', '4']}, {'boxes': tensor([[12., 76., 24., 94.],\n",
            "        [53., 25., 66., 40.],\n",
            "        [21., 19., 33., 29.],\n",
            "        [29., 80., 48., 94.]], device='cuda:0'), 'labels': ['0', '0', '0', '0']}, {'boxes': tensor([[ 79.,  44.,  94.,  47.],\n",
            "        [ 86.,  87.,  99.,  97.],\n",
            "        [105.,  80., 114.,  88.],\n",
            "        [ 78.,  94.,  93., 106.],\n",
            "        [ 19., 102.,  38., 115.],\n",
            "        [ 63.,  41.,  80.,  53.]], device='cuda:0'), 'labels': ['1', '4', '2', '0', '0', '4']}, {'boxes': tensor([[ 10.,  97.,  29., 116.],\n",
            "        [ 57.,  33.,  76.,  50.],\n",
            "        [101.,  87., 120., 102.],\n",
            "        [ 39.,  55.,  54.,  73.]], device='cuda:0'), 'labels': ['2', '2', '4', '0']}, {'boxes': tensor([[ 55.,  59.,  66.,  66.],\n",
            "        [ 35.,  12.,  54.,  29.],\n",
            "        [  3.,  94.,  14., 103.],\n",
            "        [ 87.,  86., 106.,  97.],\n",
            "        [ 63., 101.,  82., 118.]], device='cuda:0'), 'labels': ['4', '2', '3', '4', '4']}, {'boxes': tensor([[ 23.,  98.,  42., 113.],\n",
            "        [ 61.,  86.,  80., 102.],\n",
            "        [ 54., 105.,  67., 118.],\n",
            "        [  8.,  21.,  27.,  24.]], device='cuda:0'), 'labels': ['2', '2', '0', '1']}, {'boxes': tensor([[ 16.,  14.,  34.,  27.],\n",
            "        [ 38.,  12.,  57.,  28.],\n",
            "        [ 89.,  90., 107.,  98.]], device='cuda:0'), 'labels': ['3', '3', '1']}, {'boxes': tensor([[ 37.,  82.,  51.,  96.],\n",
            "        [ 98.,  47., 117.,  56.],\n",
            "        [ 95.,  61., 114.,  75.]], device='cuda:0'), 'labels': ['4', '0', '4']}, {'boxes': tensor([[  5.,  82.,  22.,  95.],\n",
            "        [ 38.,  71.,  47.,  82.],\n",
            "        [ 62.,  37.,  74.,  49.],\n",
            "        [ 93.,  66., 112.,  81.],\n",
            "        [ 97.,  79., 110.,  98.],\n",
            "        [ 71.,  61.,  85.,  75.]], device='cuda:0'), 'labels': ['4', '0', '3', '2', '0', '4']}, {'boxes': tensor([[ 98.,   9., 117.,  18.],\n",
            "        [ 48.,  58.,  67.,  65.],\n",
            "        [ 51.,  88.,  63.,  92.]], device='cuda:0'), 'labels': ['1', '1', '1']}, {'boxes': tensor([[ 21.,   3.,  35.,  12.],\n",
            "        [ 48.,  19.,  63.,  34.],\n",
            "        [ 35.,  96.,  54., 103.],\n",
            "        [ 16.,  23.,  35.,  42.],\n",
            "        [ 90., 108., 109., 114.]], device='cuda:0'), 'labels': ['4', '3', '1', '0', '1']}, {'boxes': tensor([[ 89.,  58., 108.,  74.],\n",
            "        [ 27.,  88.,  46., 107.],\n",
            "        [  1.,  81.,  20.,  89.],\n",
            "        [ 92.,  93., 110., 111.],\n",
            "        [ 54.,  80.,  70.,  96.]], device='cuda:0'), 'labels': ['0', '3', '1', '3', '3']}, {'boxes': tensor([[ 94.,  52., 108.,  64.],\n",
            "        [ 53., 110.,  66., 120.],\n",
            "        [ 97.,  79., 114.,  98.],\n",
            "        [106., 103., 122., 117.],\n",
            "        [ 49., 101.,  62., 111.],\n",
            "        [ 71., 104.,  84., 117.]], device='cuda:0'), 'labels': ['3', '3', '2', '0', '2', '0']}, {'boxes': tensor([[65., 62., 84., 75.],\n",
            "        [16.,  5., 28., 15.],\n",
            "        [65., 37., 84., 56.]], device='cuda:0'), 'labels': ['3', '4', '0']}, {'boxes': tensor([[ 42.,  35.,  61.,  41.],\n",
            "        [ 57.,  31.,  76.,  37.],\n",
            "        [109.,  82., 124.,  91.],\n",
            "        [103.,  55., 122.,  61.],\n",
            "        [ 81.,  47., 100.,  56.]], device='cuda:0'), 'labels': ['1', '1', '3', '1', '3']}, {'boxes': tensor([[ 79., 113.,  90., 123.],\n",
            "        [  8.,  15.,  27.,  32.],\n",
            "        [ 13., 108.,  23., 118.],\n",
            "        [ 23., 105.,  34., 116.],\n",
            "        [  9.,  82.,  25.,  96.],\n",
            "        [ 26.,  79.,  45.,  94.]], device='cuda:0'), 'labels': ['0', '3', '0', '2', '2', '4']}, {'boxes': tensor([[104.,  52., 123.,  68.],\n",
            "        [ 33.,  86.,  52., 101.],\n",
            "        [ 11.,  86.,  30.,  93.]], device='cuda:0'), 'labels': ['3', '4', '1']}, {'boxes': tensor([[ 88.,  65., 100.,  73.],\n",
            "        [ 83.,  48.,  98.,  56.],\n",
            "        [ 96., 115., 115., 121.],\n",
            "        [ 51.,  78.,  69.,  92.],\n",
            "        [  4.,  69.,  23.,  84.],\n",
            "        [ 98.,  85., 108.,  88.]], device='cuda:0'), 'labels': ['3', '3', '1', '2', '4', '1']}, {'boxes': tensor([[ 88.,  51., 101.,  57.],\n",
            "        [108.,  52., 118.,  57.],\n",
            "        [ 58., 108.,  77., 119.],\n",
            "        [  1.,  64.,  20.,  69.],\n",
            "        [107.,  75., 124.,  83.],\n",
            "        [ 60.,  53.,  72.,  57.]], device='cuda:0'), 'labels': ['1', '1', '3', '1', '3', '1']}, {'boxes': tensor([[ 57.,  84.,  66.,  86.],\n",
            "        [ 30.,  51.,  49.,  56.],\n",
            "        [ 39., 100.,  58., 119.],\n",
            "        [ 20.,  78.,  32.,  85.],\n",
            "        [ 98.,  79., 115.,  98.],\n",
            "        [ 89.,  12., 101.,  24.]], device='cuda:0'), 'labels': ['1', '1', '0', '4', '2', '0']}, {'boxes': tensor([[44.,  5., 63., 14.],\n",
            "        [71., 60., 90., 73.],\n",
            "        [30., 34., 46., 48.]], device='cuda:0'), 'labels': ['1', '0', '0']}, {'boxes': tensor([[22., 88., 32., 92.],\n",
            "        [ 4., 89., 23., 94.],\n",
            "        [58., 63., 77., 74.],\n",
            "        [13., 77., 27., 85.],\n",
            "        [14., 18., 33., 31.],\n",
            "        [80., 34., 95., 46.]], device='cuda:0'), 'labels': ['1', '1', '3', '3', '4', '3']}, {'boxes': tensor([[ 43., 103.,  61., 119.],\n",
            "        [106.,  49., 116.,  62.],\n",
            "        [ 12.,  69.,  31.,  83.],\n",
            "        [ 67.,  39.,  86.,  58.],\n",
            "        [ 27.,  95.,  44., 114.]], device='cuda:0'), 'labels': ['0', '2', '1', '0', '2']}, {'boxes': tensor([[ 23.,  14.,  42.,  31.],\n",
            "        [102.,  44., 121.,  55.],\n",
            "        [102., 105., 121., 120.]], device='cuda:0'), 'labels': ['4', '4', '2']}, {'boxes': tensor([[ 29.,  73.,  48.,  88.],\n",
            "        [ 92.,  38., 111.,  53.],\n",
            "        [ 44.,  22.,  63.,  37.]], device='cuda:0'), 'labels': ['0', '3', '4']}, {'boxes': tensor([[107.,   3., 124.,  14.],\n",
            "        [ 14.,  39.,  27.,  41.],\n",
            "        [ 83.,   5.,  95.,   8.]], device='cuda:0'), 'labels': ['3', '1', '1']}, {'boxes': tensor([[ 42.,  16.,  59.,  27.],\n",
            "        [ 40.,  82.,  53.,  93.],\n",
            "        [ 10.,  76.,  29.,  95.],\n",
            "        [ 75.,  86.,  93., 100.],\n",
            "        [104.,  72., 112.,  82.],\n",
            "        [ 46.,  68.,  65.,  75.]], device='cuda:0'), 'labels': ['3', '4', '0', '4', '0', '1']}, {'boxes': tensor([[ 27.,  97.,  40., 106.],\n",
            "        [ 73.,  77.,  86.,  87.],\n",
            "        [ 57.,  93.,  76.,  98.],\n",
            "        [ 97.,   9., 112.,  21.],\n",
            "        [ 80.,  63.,  92.,  73.]], device='cuda:0'), 'labels': ['3', '2', '1', '4', '2']}, {'boxes': tensor([[ 81.,  53., 100.,  68.],\n",
            "        [ 17.,  98.,  36., 101.],\n",
            "        [107., 106., 116., 114.],\n",
            "        [ 41.,  75.,  60.,  81.]], device='cuda:0'), 'labels': ['4', '1', '2', '1']}, {'boxes': tensor([[89., 58., 98., 60.],\n",
            "        [ 6., 62., 18., 72.],\n",
            "        [11., 12., 30., 27.],\n",
            "        [62., 80., 81., 93.],\n",
            "        [79., 35., 97., 50.],\n",
            "        [46., 75., 65., 84.]], device='cuda:0'), 'labels': ['1', '2', '3', '4', '4', '3']}, {'boxes': tensor([[ 13.,  73.,  32.,  88.],\n",
            "        [ 66.,  14.,  85.,  33.],\n",
            "        [ 76., 105.,  90., 113.]], device='cuda:0'), 'labels': ['3', '0', '4']}, {'boxes': tensor([[ 54.,  28.,  73.,  43.],\n",
            "        [  1.,  21.,  13.,  31.],\n",
            "        [ 53.,  99.,  72., 102.]], device='cuda:0'), 'labels': ['3', '2', '1']}, {'boxes': tensor([[33., 12., 52., 23.],\n",
            "        [51., 51., 62., 63.],\n",
            "        [18., 37., 37., 48.],\n",
            "        [19.,  8., 36., 24.]], device='cuda:0'), 'labels': ['3', '4', '4', '0']}, {'boxes': tensor([[  4.,  19.,  21.,  38.],\n",
            "        [ 57.,  67.,  69.,  77.],\n",
            "        [  8., 105.,  17., 107.],\n",
            "        [ 56.,  84.,  71.,  88.],\n",
            "        [ 31.,  65.,  49.,  82.],\n",
            "        [  1.,  65.,  20.,  81.]], device='cuda:0'), 'labels': ['2', '0', '1', '1', '0', '2']}, {'boxes': tensor([[ 38.,  30.,  53.,  42.],\n",
            "        [ 10.,  84.,  29., 102.],\n",
            "        [ 95.,  64., 114.,  72.],\n",
            "        [ 23.,   3.,  34.,   4.],\n",
            "        [ 74.,  39.,  93.,  50.],\n",
            "        [ 66., 105.,  76., 111.]], device='cuda:0'), 'labels': ['4', '2', '1', '1', '1', '4']}, {'boxes': tensor([[ 29.,  83.,  43.,  95.],\n",
            "        [ 95.,   9., 114.,  28.],\n",
            "        [104., 117., 116., 122.],\n",
            "        [ 79., 108.,  98., 112.],\n",
            "        [ 26.,  40.,  45.,  56.]], device='cuda:0'), 'labels': ['2', '3', '0', '1', '4']}, {'boxes': tensor([[ 65.,  11.,  84.,  22.],\n",
            "        [ 89.,  72., 108.,  87.],\n",
            "        [ 38.,  59.,  55.,  74.],\n",
            "        [ 35.,  23.,  54.,  38.],\n",
            "        [ 65.,  28.,  82.,  43.],\n",
            "        [102.,  23., 116.,  34.]], device='cuda:0'), 'labels': ['4', '2', '2', '3', '2', '3']}, {'boxes': tensor([[ 87.,  13., 104.,  28.],\n",
            "        [ 69.,  35.,  88.,  54.],\n",
            "        [ 60.,  99.,  76., 101.]], device='cuda:0'), 'labels': ['2', '3', '1']}, {'boxes': tensor([[ 44.,  35.,  63.,  50.],\n",
            "        [ 95., 108., 107., 119.],\n",
            "        [ 67.,  20.,  86.,  37.],\n",
            "        [102.,  94., 121., 111.],\n",
            "        [108.,  86., 116.,  98.],\n",
            "        [ 26.,  86.,  45., 103.]], device='cuda:0'), 'labels': ['2', '2', '4', '4', '0', '2']}, {'boxes': tensor([[ 96., 116., 106., 119.],\n",
            "        [ 82., 109., 101., 123.],\n",
            "        [ 31.,  39.,  50.,  44.],\n",
            "        [ 72.,  28.,  86.,  39.],\n",
            "        [ 65.,  44.,  78.,  52.],\n",
            "        [ 88.,  71., 107.,  79.]], device='cuda:0'), 'labels': ['1', '1', '1', '4', '4', '1']}, {'boxes': tensor([[ 86.,  95., 105., 101.],\n",
            "        [ 74.,  28.,  92.,  43.],\n",
            "        [ 57.,  52.,  70.,  65.],\n",
            "        [ 83.,  69., 102.,  78.],\n",
            "        [ 59.,  34.,  71.,  43.]], device='cuda:0'), 'labels': ['1', '0', '4', '1', '0']}, {'boxes': tensor([[ 72.,  38.,  88.,  50.],\n",
            "        [ 42.,  12.,  61.,  29.],\n",
            "        [ 85.,  91., 104., 106.]], device='cuda:0'), 'labels': ['2', '0', '4']}, {'boxes': tensor([[ 71., 115.,  82., 116.],\n",
            "        [ 25.,  22.,  35.,  30.],\n",
            "        [  8.,  19.,  25.,  38.],\n",
            "        [ 78.,  94.,  96., 110.],\n",
            "        [ 63.,   5.,  82.,  22.],\n",
            "        [ 41.,  91.,  60., 104.]], device='cuda:0'), 'labels': ['1', '2', '2', '4', '2', '3']}, {'boxes': tensor([[ 46.,   6.,  56.,  13.],\n",
            "        [ 25., 101.,  44., 114.],\n",
            "        [ 52.,  87.,  66., 100.]], device='cuda:0'), 'labels': ['3', '3', '3']}, {'boxes': tensor([[ 53.,  56.,  68.,  73.],\n",
            "        [ 48.,   6.,  61.,  13.],\n",
            "        [ 91.,  53., 110.,  66.],\n",
            "        [ 27., 106.,  46., 121.],\n",
            "        [ 53.,  28.,  72.,  43.],\n",
            "        [ 34.,  29.,  53.,  46.]], device='cuda:0'), 'labels': ['2', '4', '0', '4', '0', '4']}, {'boxes': tensor([[ 29., 115.,  48., 118.],\n",
            "        [105.,  46., 124.,  60.],\n",
            "        [ 37.,  63.,  56.,  66.],\n",
            "        [ 81., 112.,  96., 123.],\n",
            "        [ 56.,   6.,  75.,  11.],\n",
            "        [ 90.,  51., 109.,  70.]], device='cuda:0'), 'labels': ['1', '3', '1', '3', '1', '0']}, {'boxes': tensor([[ 59.,  33.,  78.,  49.],\n",
            "        [ 89.,   3., 100.,   8.],\n",
            "        [ 49.,  72.,  68.,  88.]], device='cuda:0'), 'labels': ['2', '3', '2']}, {'boxes': tensor([[ 82.,  35., 101.,  46.],\n",
            "        [ 31.,  77.,  46.,  91.],\n",
            "        [ 28.,  15.,  47.,  25.]], device='cuda:0'), 'labels': ['4', '2', '4']}, {'boxes': tensor([[  3.,  36.,  22.,  51.],\n",
            "        [ 47.,  38.,  56.,  50.],\n",
            "        [ 21.,  74.,  40.,  93.],\n",
            "        [ 89.,  86., 108.,  99.],\n",
            "        [ 69.,  43.,  88.,  51.]], device='cuda:0'), 'labels': ['4', '3', '2', '2', '1']}, {'boxes': tensor([[100.,  17., 119.,  21.],\n",
            "        [ 32.,  73.,  47.,  83.],\n",
            "        [ 38.,   8.,  53.,  15.],\n",
            "        [ 28.,  32.,  45.,  51.],\n",
            "        [  1.,  51.,  20.,  70.],\n",
            "        [ 17.,   2.,  36.,  15.]], device='cuda:0'), 'labels': ['1', '4', '1', '3', '0', '0']}, {'boxes': tensor([[ 37.,  36.,  56.,  39.],\n",
            "        [ 10.,  67.,  29.,  82.],\n",
            "        [ 92.,  87., 104.,  95.],\n",
            "        [ 24.,  56.,  38.,  71.],\n",
            "        [ 27.,  95.,  41., 106.]], device='cuda:0'), 'labels': ['1', '2', '3', '3', '4']}, {'boxes': tensor([[ 44.,  21.,  58.,  24.],\n",
            "        [ 95.,  58., 111.,  71.],\n",
            "        [ 70.,  28.,  83.,  30.],\n",
            "        [ 78.,  34.,  97.,  39.],\n",
            "        [ 66.,   7.,  84.,  19.],\n",
            "        [  3.,  50.,  13.,  56.]], device='cuda:0'), 'labels': ['1', '2', '1', '1', '4', '4']}, {'boxes': tensor([[ 86.,  15., 105.,  28.],\n",
            "        [ 77.,  76.,  88.,  79.],\n",
            "        [101.,  97., 120., 116.]], device='cuda:0'), 'labels': ['3', '1', '2']}, {'boxes': tensor([[  0.,   9.,  15.,  18.],\n",
            "        [112.,  88., 122., 101.],\n",
            "        [ 80.,  59.,  99.,  78.]], device='cuda:0'), 'labels': ['4', '0', '0']}, {'boxes': tensor([[22., 42., 33., 53.],\n",
            "        [12., 42., 22., 47.],\n",
            "        [58., 59., 77., 78.],\n",
            "        [26., 31., 41., 44.]], device='cuda:0'), 'labels': ['3', '1', '0', '2']}, {'boxes': tensor([[ 83.,  91., 102., 103.],\n",
            "        [ 97., 111., 116., 120.],\n",
            "        [ 47., 102.,  63., 117.],\n",
            "        [ 24.,  19.,  43.,  34.],\n",
            "        [ 73.,  44.,  84.,  54.]], device='cuda:0'), 'labels': ['0', '1', '2', '4', '4']}, {'boxes': tensor([[113., 110., 123., 116.],\n",
            "        [102.,  52., 119.,  62.],\n",
            "        [ 43.,  87.,  60.,  93.],\n",
            "        [ 85.,  22., 104.,  41.]], device='cuda:0'), 'labels': ['2', '4', '1', '0']}, {'boxes': tensor([[ 59.,  58.,  71.,  68.],\n",
            "        [ 48.,  36.,  65.,  42.],\n",
            "        [ 39.,  79.,  54.,  83.],\n",
            "        [ 86.,  53., 105.,  68.],\n",
            "        [ 34.,  59.,  53.,  68.],\n",
            "        [ 24., 104.,  42., 113.]], device='cuda:0'), 'labels': ['3', '1', '1', '2', '0', '3']}, {'boxes': tensor([[ 95.,  29., 111.,  41.],\n",
            "        [ 78.,  96.,  92., 115.],\n",
            "        [ 22.,  90.,  35., 109.]], device='cuda:0'), 'labels': ['3', '0', '4']}, {'boxes': tensor([[ 46.,  39.,  65.,  55.],\n",
            "        [ 90.,   5., 109.,  20.],\n",
            "        [ 30.,  20.,  44.,  28.]], device='cuda:0'), 'labels': ['2', '4', '0']}, {'boxes': tensor([[ 86.,  19., 100.,  29.],\n",
            "        [ 16.,  27.,  35.,  38.],\n",
            "        [ 40.,   5.,  59.,  24.],\n",
            "        [ 98.,  96., 116., 100.]], device='cuda:0'), 'labels': ['4', '4', '2', '1']}, {'boxes': tensor([[ 87., 108.,  99., 111.],\n",
            "        [ 48., 106.,  60., 118.],\n",
            "        [ 15.,  53.,  34.,  56.],\n",
            "        [ 41.,  18.,  60.,  30.],\n",
            "        [103.,  54., 122.,  63.],\n",
            "        [ 44.,   5.,  63.,  21.]], device='cuda:0'), 'labels': ['1', '0', '1', '0', '1', '0']}, {'boxes': tensor([[104.,  47., 123.,  62.],\n",
            "        [105.,   9., 119.,  28.],\n",
            "        [ 87.,  72., 102.,  83.]], device='cuda:0'), 'labels': ['2', '0', '4']}, {'boxes': tensor([[ 39.,   8.,  58.,  13.],\n",
            "        [ 15.,  14.,  33.,  29.],\n",
            "        [  3.,  41.,  20.,  54.],\n",
            "        [  7.,  66.,  19.,  76.],\n",
            "        [  8.,  93.,  27., 102.]], device='cuda:0'), 'labels': ['1', '0', '3', '0', '1']}, {'boxes': tensor([[35., 12., 44., 21.],\n",
            "        [72., 51., 91., 56.],\n",
            "        [55., 59., 74., 70.]], device='cuda:0'), 'labels': ['4', '1', '4']}, {'boxes': tensor([[ 22.,  28.,  37.,  33.],\n",
            "        [ 68.,  94.,  87., 107.],\n",
            "        [ 80., 108.,  93., 112.],\n",
            "        [ 69.,  66.,  80.,  75.],\n",
            "        [ 74.,  83.,  93.,  96.]], device='cuda:0'), 'labels': ['3', '3', '1', '2', '4']}, {'boxes': tensor([[ 58.,  59.,  77.,  72.],\n",
            "        [ 14.,  88.,  31.,  92.],\n",
            "        [101.,  99., 112., 109.],\n",
            "        [ 88.,  15., 107.,  30.],\n",
            "        [ 79., 115.,  89., 122.],\n",
            "        [ 76.,   6.,  95.,   9.]], device='cuda:0'), 'labels': ['0', '1', '0', '4', '3', '1']}, {'boxes': tensor([[  5.,  23.,  24.,  39.],\n",
            "        [ 45.,  23.,  64.,  27.],\n",
            "        [ 97.,  27., 107.,  28.],\n",
            "        [ 89.,  47.,  99.,  49.],\n",
            "        [ 71.,  50.,  87.,  62.]], device='cuda:0'), 'labels': ['0', '1', '1', '1', '4']}, {'boxes': tensor([[  0.,  28.,  17.,  31.],\n",
            "        [ 67.,  34.,  86.,  43.],\n",
            "        [  6.,  26.,  18.,  28.],\n",
            "        [ 32.,  49.,  47.,  59.],\n",
            "        [ 80.,  98.,  94., 109.],\n",
            "        [ 46.,  61.,  64.,  72.]], device='cuda:0'), 'labels': ['1', '1', '1', '0', '3', '1']}, {'boxes': tensor([[60., 30., 79., 41.],\n",
            "        [22., 81., 38., 87.],\n",
            "        [78., 78., 97., 93.],\n",
            "        [49., 35., 68., 53.],\n",
            "        [ 2., 61., 21., 80.]], device='cuda:0'), 'labels': ['4', '1', '4', '0', '0']}, {'boxes': tensor([[ 87., 103., 106., 122.],\n",
            "        [ 33.,  61.,  52.,  75.],\n",
            "        [ 81.,  70.,  98.,  89.],\n",
            "        [ 63.,  24.,  82.,  29.],\n",
            "        [ 90.,  53., 109.,  71.]], device='cuda:0'), 'labels': ['0', '4', '2', '1', '2']}, {'boxes': tensor([[ 88.,  55., 105.,  74.],\n",
            "        [ 42.,  16.,  57.,  28.],\n",
            "        [  1.,   3.,  20.,  16.],\n",
            "        [ 19.,  63.,  32.,  75.]], device='cuda:0'), 'labels': ['2', '4', '3', '0']}, {'boxes': tensor([[103.,  72., 122.,  79.],\n",
            "        [106.,  99., 116., 106.],\n",
            "        [ 33.,  54.,  52.,  59.],\n",
            "        [ 70.,  64.,  87.,  81.],\n",
            "        [ 87.,   3., 106.,  20.],\n",
            "        [ 33.,  86.,  43.,  92.]], device='cuda:0'), 'labels': ['1', '4', '1', '2', '3', '4']}, {'boxes': tensor([[ 12.,  55.,  31.,  71.],\n",
            "        [ 60.,  41.,  76.,  57.],\n",
            "        [ 29.,  97.,  46., 108.],\n",
            "        [105.,   6., 119.,  16.]], device='cuda:0'), 'labels': ['2', '0', '3', '4']}, {'boxes': tensor([[ 64.,  67.,  83.,  85.],\n",
            "        [ 28.,  20.,  47.,  39.],\n",
            "        [ 46.,  75.,  65.,  78.],\n",
            "        [ 64.,  53.,  73.,  58.],\n",
            "        [ 85.,  62.,  96.,  73.],\n",
            "        [ 98.,  61., 117.,  66.]], device='cuda:0'), 'labels': ['0', '3', '1', '3', '4', '1']}, {'boxes': tensor([[ 28., 114.,  46., 118.],\n",
            "        [ 29.,  17.,  46.,  32.],\n",
            "        [ 13.,  86.,  32., 101.],\n",
            "        [ 66.,  55.,  84.,  57.]], device='cuda:0'), 'labels': ['1', '0', '4', '1']}, {'boxes': tensor([[  0.,  51.,  19.,  67.],\n",
            "        [ 64.,  50.,  83.,  55.],\n",
            "        [ 52.,  15.,  71.,  34.],\n",
            "        [ 79.,  91.,  88.,  97.],\n",
            "        [ 95.,  18., 108.,  27.]], device='cuda:0'), 'labels': ['2', '1', '0', '3', '3']}, {'boxes': tensor([[104.,  83., 119.,  95.],\n",
            "        [ 57.,  69.,  76.,  88.],\n",
            "        [ 44.,  96.,  63., 113.],\n",
            "        [ 78.,  43.,  89.,  52.]], device='cuda:0'), 'labels': ['4', '3', '4', '2']}, {'boxes': tensor([[ 54.,  63.,  73.,  72.],\n",
            "        [ 54.,  96.,  73., 108.],\n",
            "        [ 77.,  28.,  90.,  36.],\n",
            "        [ 33.,  96.,  44., 106.],\n",
            "        [ 57.,  31.,  73.,  35.]], device='cuda:0'), 'labels': ['1', '3', '3', '0', '1']}, {'boxes': tensor([[  0.,  60.,  19.,  73.],\n",
            "        [  9.,  70.,  23.,  88.],\n",
            "        [ 59.,  80.,  76.,  87.],\n",
            "        [ 62., 110.,  81., 119.],\n",
            "        [ 40.,  31.,  55.,  36.]], device='cuda:0'), 'labels': ['4', '0', '1', '1', '1']}, {'boxes': tensor([[109.,  58., 120.,  67.],\n",
            "        [ 35., 113.,  54., 121.],\n",
            "        [ 88.,  86., 107.,  97.],\n",
            "        [ 62.,  89.,  81.,  98.],\n",
            "        [ 37.,  40.,  48.,  51.]], device='cuda:0'), 'labels': ['3', '4', '4', '0', '2']}, {'boxes': tensor([[ 96.,  16., 115.,  33.],\n",
            "        [ 61.,  87.,  70.,  93.],\n",
            "        [ 16.,  49.,  35.,  66.]], device='cuda:0'), 'labels': ['3', '3', '3']}, {'boxes': tensor([[ 74.,  84.,  93.,  96.],\n",
            "        [ 28.,  23.,  47.,  37.],\n",
            "        [ 83., 100., 102., 119.],\n",
            "        [ 72.,  13.,  91.,  32.],\n",
            "        [  3.,  81.,  22.,  94.],\n",
            "        [ 76.,  78.,  86.,  85.]], device='cuda:0'), 'labels': ['3', '4', '3', '4', '4', '3']}, {'boxes': tensor([[ 18.,  89.,  37., 106.],\n",
            "        [ 25.,  45.,  44.,  52.],\n",
            "        [ 52.,   2.,  71.,  15.],\n",
            "        [107.,  32., 120.,  46.],\n",
            "        [  2., 106.,  21., 109.]], device='cuda:0'), 'labels': ['3', '1', '4', '3', '1']}, {'boxes': tensor([[ 6., 45., 23., 57.],\n",
            "        [61.,  0., 78., 17.],\n",
            "        [79., 15., 98., 34.]], device='cuda:0'), 'labels': ['1', '2', '2']}, {'boxes': tensor([[15., 19., 27., 29.],\n",
            "        [25., 89., 36., 95.],\n",
            "        [44., 83., 63., 94.],\n",
            "        [67., 25., 86., 40.]], device='cuda:0'), 'labels': ['2', '3', '4', '3']}, {'boxes': tensor([[ 29.,  48.,  48.,  65.],\n",
            "        [ 77.,  87.,  91., 100.],\n",
            "        [ 47.,  99.,  61., 111.],\n",
            "        [ 86.,  78., 105.,  93.],\n",
            "        [  1.,  83.,  18.,  89.]], device='cuda:0'), 'labels': ['2', '4', '2', '3', '1']}, {'boxes': tensor([[ 39.,  65.,  53.,  79.],\n",
            "        [ 33.,  43.,  47.,  51.],\n",
            "        [ 63.,  99.,  82., 114.],\n",
            "        [ 50., 103.,  65., 118.],\n",
            "        [ 26.,  52.,  45.,  71.],\n",
            "        [ 68.,  10.,  87.,  22.]], device='cuda:0'), 'labels': ['4', '4', '2', '1', '3', '3']}, {'boxes': tensor([[ 52.,  36.,  68.,  43.],\n",
            "        [ 69.,  16.,  88.,  32.],\n",
            "        [ 37.,  71.,  54.,  85.],\n",
            "        [ 13.,  13.,  32.,  31.],\n",
            "        [109.,  33., 123.,  36.]], device='cuda:0'), 'labels': ['1', '3', '2', '2', '1']}, {'boxes': tensor([[66., 61., 85., 76.],\n",
            "        [ 3., 41., 17., 48.],\n",
            "        [ 3., 81., 19., 97.]], device='cuda:0'), 'labels': ['2', '3', '2']}, {'boxes': tensor([[ 97.,   5., 113.,  21.],\n",
            "        [ 56.,  11.,  75.,  28.],\n",
            "        [ 10.,  52.,  29.,  62.]], device='cuda:0'), 'labels': ['3', '2', '1']}, {'boxes': tensor([[ 75.,  13.,  94.,  29.],\n",
            "        [107.,  49., 124.,  66.],\n",
            "        [  6.,   8.,  24.,  24.],\n",
            "        [ 43.,  85.,  52.,  88.],\n",
            "        [  9.,  71.,  28.,  86.],\n",
            "        [ 38.,  87.,  48., 102.]], device='cuda:0'), 'labels': ['4', '3', '2', '1', '2', '4']}, {'boxes': tensor([[ 95.,  75., 111.,  87.],\n",
            "        [ 73., 109.,  92., 121.],\n",
            "        [ 97.,  56., 107.,  62.]], device='cuda:0'), 'labels': ['3', '3', '1']}, {'boxes': tensor([[ 63.,  70.,  73.,  77.],\n",
            "        [ 46.,  32.,  57.,  40.],\n",
            "        [ 34.,  50.,  53.,  59.],\n",
            "        [ 17., 109.,  36., 122.],\n",
            "        [ 51.,  48.,  70.,  62.],\n",
            "        [ 88.,  67., 100.,  77.]], device='cuda:0'), 'labels': ['3', '3', '1', '4', '4', '4']}, {'boxes': tensor([[57., 22., 76., 33.],\n",
            "        [64., 11., 83., 24.],\n",
            "        [ 7., 17., 26., 33.],\n",
            "        [53., 12., 64., 21.]], device='cuda:0'), 'labels': ['4', '4', '0', '4']}, {'boxes': tensor([[103.,  57., 122.,  74.],\n",
            "        [ 27.,  81.,  45.,  91.],\n",
            "        [100.,   6., 110.,  14.],\n",
            "        [ 39.,  59.,  49.,  69.],\n",
            "        [ 73.,  51.,  88.,  65.]], device='cuda:0'), 'labels': ['2', '2', '4', '2', '3']}, {'boxes': tensor([[ 49.,  14.,  68.,  19.],\n",
            "        [ 14., 111.,  29., 118.],\n",
            "        [ 25.,  60.,  44.,  73.]], device='cuda:0'), 'labels': ['1', '4', '3']}, {'boxes': tensor([[ 90., 102., 109., 117.],\n",
            "        [ 67., 103.,  86., 108.],\n",
            "        [ 43.,  11.,  55.,  18.]], device='cuda:0'), 'labels': ['2', '1', '3']}, {'boxes': tensor([[ 53.,  27.,  66.,  40.],\n",
            "        [ 97.,  97., 116., 114.],\n",
            "        [ 77.,  74.,  92.,  79.],\n",
            "        [ 32.,  70.,  51.,  89.]], device='cuda:0'), 'labels': ['2', '0', '3', '2']}]\n",
            "[{'boxes': tensor([[ 42.,  14.,  60.,  28.],\n",
            "        [ 92.,  82., 111., 101.],\n",
            "        [  3.,  94.,  22., 111.],\n",
            "        [ 75.,  97.,  93., 109.],\n",
            "        [ 52.,  17.,  67.,  19.],\n",
            "        [ 97., 104., 108., 114.]], device='cuda:0'), 'labels': ['2', '3', '2', '3', '1', '3']}, {'boxes': tensor([[ 49.,  78.,  68.,  95.],\n",
            "        [ 75.,  68.,  94.,  82.],\n",
            "        [ 38.,  95.,  57., 106.],\n",
            "        [ 90.,  87., 109., 102.]], device='cuda:0'), 'labels': ['0', '4', '3', '4']}, {'boxes': tensor([[ 61.,  37.,  70.,  42.],\n",
            "        [ 48.,  58.,  67.,  61.],\n",
            "        [ 48.,  87.,  59.,  92.],\n",
            "        [110.,  39., 124.,  53.],\n",
            "        [ 89.,  79., 102.,  91.]], device='cuda:0'), 'labels': ['4', '1', '4', '0', '2']}, {'boxes': tensor([[ 79.,  63.,  98.,  79.],\n",
            "        [ 35.,  66.,  54.,  83.],\n",
            "        [ 67.,  94.,  86., 105.]], device='cuda:0'), 'labels': ['4', '2', '3']}, {'boxes': tensor([[ 78.,  22.,  92.,  32.],\n",
            "        [ 36.,  76.,  49.,  95.],\n",
            "        [ 17.,  77.,  36.,  96.],\n",
            "        [ 51.,  18.,  70.,  21.],\n",
            "        [ 67.,  33.,  86.,  50.],\n",
            "        [ 95.,  41., 114.,  54.]], device='cuda:0'), 'labels': ['4', '4', '3', '1', '4', '0']}, {'boxes': tensor([[ 9.,  9., 28., 28.],\n",
            "        [50., 20., 69., 23.],\n",
            "        [84., 29., 96., 41.]], device='cuda:0'), 'labels': ['0', '1', '0']}, {'boxes': tensor([[68., 57., 87., 72.],\n",
            "        [51., 60., 70., 65.],\n",
            "        [62., 18., 81., 32.]], device='cuda:0'), 'labels': ['4', '1', '4']}, {'boxes': tensor([[ 77.,  96.,  96., 115.],\n",
            "        [ 94.,   5., 105.,  13.],\n",
            "        [ 20.,  91.,  32.,  95.],\n",
            "        [ 86.,  19., 104.,  38.],\n",
            "        [ 27.,   4.,  46.,  17.],\n",
            "        [ 56.,  92.,  75.,  96.]], device='cuda:0'), 'labels': ['0', '3', '1', '0', '0', '1']}, {'boxes': tensor([[ 98., 104., 115., 121.],\n",
            "        [ 51.,  43.,  62.,  52.],\n",
            "        [ 54., 111.,  73., 122.],\n",
            "        [ 76., 112.,  95., 123.],\n",
            "        [ 65.,  18.,  84.,  35.]], device='cuda:0'), 'labels': ['0', '2', '3', '3', '3']}, {'boxes': tensor([[ 62.,  41.,  81.,  60.],\n",
            "        [ 46.,   1.,  55.,   2.],\n",
            "        [ 38.,  35.,  49.,  45.],\n",
            "        [101.,  24., 120.,  39.],\n",
            "        [ 67.,  87.,  86., 100.]], device='cuda:0'), 'labels': ['2', '1', '0', '2', '4']}, {'boxes': tensor([[23., 13., 42., 22.],\n",
            "        [11., 81., 29., 88.],\n",
            "        [ 8., 14., 27., 20.]], device='cuda:0'), 'labels': ['4', '1', '1']}, {'boxes': tensor([[ 34.,  85.,  49., 104.],\n",
            "        [ 75.,  19.,  94.,  36.],\n",
            "        [ 58.,  95.,  69., 103.],\n",
            "        [ 38.,  48.,  55.,  60.],\n",
            "        [ 71.,  53.,  90.,  58.],\n",
            "        [ 37.,  58.,  50.,  67.]], device='cuda:0'), 'labels': ['2', '0', '4', '3', '1', '4']}, {'boxes': tensor([[ 50.,  34.,  69.,  46.],\n",
            "        [ 38.,  83.,  54.,  93.],\n",
            "        [ 75.,  65.,  94.,  76.],\n",
            "        [ 19.,  37.,  27.,  49.],\n",
            "        [ 87.,  88., 106.,  99.]], device='cuda:0'), 'labels': ['4', '1', '4', '0', '3']}, {'boxes': tensor([[105.,   4., 124.,   9.],\n",
            "        [101.,  11., 120.,  22.],\n",
            "        [ 40.,  94.,  59., 109.]], device='cuda:0'), 'labels': ['1', '1', '3']}, {'boxes': tensor([[102., 118., 121., 124.],\n",
            "        [ 33.,  43.,  49.,  49.],\n",
            "        [ 49., 100.,  68., 117.],\n",
            "        [ 75.,  92.,  85.,  97.],\n",
            "        [ 12.,  65.,  31.,  79.]], device='cuda:0'), 'labels': ['1', '4', '3', '3', '3']}, {'boxes': tensor([[ 16.,  51.,  35.,  60.],\n",
            "        [ 73.,   8.,  92.,  24.],\n",
            "        [ 23., 101.,  42., 108.],\n",
            "        [ 67., 100.,  80., 106.],\n",
            "        [ 94.,  55., 113.,  69.],\n",
            "        [ 76.,  67.,  95.,  78.]], device='cuda:0'), 'labels': ['1', '4', '1', '3', '3', '1']}, {'boxes': tensor([[  5.,  16.,  24.,  35.],\n",
            "        [ 98.,  80., 117.,  83.],\n",
            "        [ 91.,  42., 102.,  48.],\n",
            "        [ 54.,  66.,  73.,  69.]], device='cuda:0'), 'labels': ['0', '1', '4', '1']}, {'boxes': tensor([[ 87.,  68., 106.,  77.],\n",
            "        [ 52., 108.,  63., 112.],\n",
            "        [ 56.,  39.,  69.,  42.],\n",
            "        [  4.,  89.,  13.,  94.],\n",
            "        [  9., 105.,  28., 122.]], device='cuda:0'), 'labels': ['0', '3', '1', '3', '4']}, {'boxes': tensor([[ 74., 108.,  93., 123.],\n",
            "        [ 69.,  40.,  88.,  57.],\n",
            "        [ 49.,  79.,  68.,  90.],\n",
            "        [ 44.,  10.,  56.,  12.],\n",
            "        [ 92.,  71., 105.,  78.],\n",
            "        [110.,   4., 123.,  14.]], device='cuda:0'), 'labels': ['3', '4', '3', '1', '3', '4']}, {'boxes': tensor([[ 63.,   9.,  77.,  21.],\n",
            "        [ 47.,  43.,  66.,  60.],\n",
            "        [101.,  34., 120.,  42.],\n",
            "        [ 40.,  79.,  54.,  91.],\n",
            "        [102.,  44., 115.,  47.]], device='cuda:0'), 'labels': ['0', '3', '1', '2', '1']}, {'boxes': tensor([[ 57.,  26.,  76.,  44.],\n",
            "        [ 41.,  46.,  60.,  64.],\n",
            "        [ 68.,  86.,  87.,  99.],\n",
            "        [ 94.,  18., 110.,  29.],\n",
            "        [ 10.,  96.,  25., 102.],\n",
            "        [ 64.,  62.,  79.,  75.]], device='cuda:0'), 'labels': ['0', '2', '0', '0', '1', '4']}, {'boxes': tensor([[ 0., 24., 19., 41.],\n",
            "        [66., 62., 85., 75.],\n",
            "        [33., 59., 50., 69.],\n",
            "        [35., 84., 47., 95.],\n",
            "        [48.,  2., 67., 17.]], device='cuda:0'), 'labels': ['4', '4', '4', '0', '4']}, {'boxes': tensor([[60., 84., 79., 96.],\n",
            "        [71.,  4., 83.,  6.],\n",
            "        [15., 36., 25., 43.],\n",
            "        [79., 54., 96., 68.]], device='cuda:0'), 'labels': ['1', '1', '4', '2']}, {'boxes': tensor([[ 89.,  57., 105.,  69.],\n",
            "        [ 45.,  88.,  62., 107.],\n",
            "        [ 70.,  63.,  89.,  70.],\n",
            "        [ 62.,  84.,  81.,  97.],\n",
            "        [ 45.,  30.,  59.,  42.]], device='cuda:0'), 'labels': ['0', '4', '1', '0', '4']}, {'boxes': tensor([[ 93.,  25., 108.,  38.],\n",
            "        [ 83., 108.,  94., 110.],\n",
            "        [ 22.,  40.,  41.,  51.],\n",
            "        [ 30.,  13.,  40.,  20.]], device='cuda:0'), 'labels': ['4', '1', '3', '3']}, {'boxes': tensor([[ 90.,  87., 102.,  92.],\n",
            "        [ 50.,  48.,  69.,  67.],\n",
            "        [ 92.,   4., 108.,  18.]], device='cuda:0'), 'labels': ['1', '0', '3']}, {'boxes': tensor([[ 57.,  45.,  69.,  57.],\n",
            "        [ 86.,  73., 101.,  88.],\n",
            "        [ 53.,  66.,  72.,  85.],\n",
            "        [ 65.,  57.,  80.,  64.]], device='cuda:0'), 'labels': ['2', '2', '4', '1']}, {'boxes': tensor([[ 36.,  19.,  55.,  38.],\n",
            "        [ 64.,  64.,  83.,  83.],\n",
            "        [ 93., 104., 110., 123.],\n",
            "        [ 85.,  63.,  96.,  72.]], device='cuda:0'), 'labels': ['2', '4', '2', '4']}, {'boxes': tensor([[ 23.,  73.,  42.,  91.],\n",
            "        [103.,  15., 114.,  25.],\n",
            "        [ 78.,  15.,  97.,  27.],\n",
            "        [ 78.,  89.,  93., 101.],\n",
            "        [ 74.,  81.,  83.,  92.],\n",
            "        [ 86.,  70., 105.,  89.]], device='cuda:0'), 'labels': ['2', '0', '3', '3', '2', '0']}, {'boxes': tensor([[ 25.,  32.,  37.,  42.],\n",
            "        [ 70.,  31.,  81.,  37.],\n",
            "        [ 55.,  34.,  74.,  39.],\n",
            "        [ 54.,  88.,  73., 103.],\n",
            "        [ 50.,  21.,  69.,  34.]], device='cuda:0'), 'labels': ['2', '2', '1', '3', '3']}, {'boxes': tensor([[ 67.,  35.,  81.,  49.],\n",
            "        [ 79.,  40.,  98.,  43.],\n",
            "        [ 99.,  43., 118.,  59.],\n",
            "        [ 28.,  53.,  47.,  60.],\n",
            "        [ 72.,  94.,  86., 105.],\n",
            "        [ 58.,  96.,  73., 110.]], device='cuda:0'), 'labels': ['2', '1', '4', '3', '4', '2']}, {'boxes': tensor([[ 83.,   1.,  94.,   9.],\n",
            "        [100.,  81., 119.,  86.],\n",
            "        [ 11.,  98.,  23., 111.],\n",
            "        [ 73.,  42.,  91.,  53.],\n",
            "        [ 48.,  32.,  59.,  37.]], device='cuda:0'), 'labels': ['4', '1', '2', '1', '1']}, {'boxes': tensor([[ 56.,  28.,  75.,  43.],\n",
            "        [ 24.,  14.,  43.,  32.],\n",
            "        [ 81.,  56., 100.,  69.],\n",
            "        [ 13.,   2.,  24.,  10.],\n",
            "        [ 44., 105.,  63., 121.],\n",
            "        [111.,  88., 121.,  99.]], device='cuda:0'), 'labels': ['3', '2', '3', '3', '2', '0']}, {'boxes': tensor([[ 77.,  96.,  96., 115.],\n",
            "        [107.,   4., 119.,  18.],\n",
            "        [105.,  31., 124.,  44.]], device='cuda:0'), 'labels': ['0', '3', '2']}, {'boxes': tensor([[28.,  2., 38., 13.],\n",
            "        [ 2., 45., 21., 63.],\n",
            "        [34., 54., 52., 72.]], device='cuda:0'), 'labels': ['3', '2', '2']}, {'boxes': tensor([[ 43.,  63.,  62.,  80.],\n",
            "        [ 81.,  76.,  99.,  88.],\n",
            "        [ 73.,  55.,  92.,  61.],\n",
            "        [ 33., 100.,  52., 103.],\n",
            "        [ 56.,  23.,  75.,  42.]], device='cuda:0'), 'labels': ['2', '3', '1', '1', '2']}, {'boxes': tensor([[ 99.,  65., 116.,  76.],\n",
            "        [ 99., 103., 118., 121.],\n",
            "        [  7.,  97.,  26., 116.]], device='cuda:0'), 'labels': ['4', '2', '0']}, {'boxes': tensor([[ 74.,  78.,  89.,  90.],\n",
            "        [ 86.,  26., 105.,  39.],\n",
            "        [ 49.,  41.,  62.,  53.]], device='cuda:0'), 'labels': ['4', '0', '3']}, {'boxes': tensor([[100.,  82., 110.,  90.],\n",
            "        [ 22.,   6.,  41.,  24.],\n",
            "        [ 71.,   4.,  90.,  11.]], device='cuda:0'), 'labels': ['4', '0', '1']}, {'boxes': tensor([[ 48.,  25.,  67.,  44.],\n",
            "        [106.,  22., 115.,  30.],\n",
            "        [ 25.,  15.,  44.,  26.],\n",
            "        [ 64., 112.,  78., 117.]], device='cuda:0'), 'labels': ['2', '4', '3', '1']}, {'boxes': tensor([[ 87.,  17., 106.,  32.],\n",
            "        [ 16.,  61.,  31.,  74.],\n",
            "        [ 12.,  84.,  31., 101.],\n",
            "        [ 78.,  51.,  89.,  54.]], device='cuda:0'), 'labels': ['4', '2', '4', '1']}, {'boxes': tensor([[ 33.,  34.,  49.,  37.],\n",
            "        [105.,  58., 124.,  70.],\n",
            "        [ 63.,  18.,  82.,  37.],\n",
            "        [ 75.,   4.,  94.,  11.]], device='cuda:0'), 'labels': ['1', '2', '2', '3']}, {'boxes': tensor([[ 62.,  28.,  77.,  38.],\n",
            "        [ 40.,   1.,  57.,  20.],\n",
            "        [ 23.,  98.,  42., 117.],\n",
            "        [ 45.,  28.,  61.,  43.],\n",
            "        [104.,  42., 123.,  60.]], device='cuda:0'), 'labels': ['3', '0', '2', '4', '4']}, {'boxes': tensor([[  1.,  23.,  13.,  31.],\n",
            "        [ 66.,  77.,  85.,  90.],\n",
            "        [ 66., 117.,  75., 120.],\n",
            "        [105.,  69., 118.,  78.],\n",
            "        [ 44.,  16.,  63.,  31.],\n",
            "        [  2.,  75.,  21.,  79.]], device='cuda:0'), 'labels': ['3', '3', '1', '0', '4', '1']}, {'boxes': tensor([[ 45.,  85.,  63., 104.],\n",
            "        [ 84.,  10., 103.,  23.],\n",
            "        [ 65.,  87.,  84.,  99.],\n",
            "        [ 36., 114.,  51., 124.],\n",
            "        [103., 104., 114., 112.]], device='cuda:0'), 'labels': ['2', '0', '3', '2', '3']}, {'boxes': tensor([[100.,  20., 119.,  39.],\n",
            "        [ 44.,  84.,  63.,  90.],\n",
            "        [ 37.,  19.,  56.,  31.],\n",
            "        [ 69.,  40.,  85.,  56.],\n",
            "        [ 18.,  11.,  35.,  26.]], device='cuda:0'), 'labels': ['2', '1', '4', '2', '4']}, {'boxes': tensor([[ 34.,  75.,  53.,  79.],\n",
            "        [ 46.,  53.,  57.,  61.],\n",
            "        [ 87., 103., 101., 115.],\n",
            "        [ 83.,  15.,  97.,  23.],\n",
            "        [ 43.,   8.,  53.,  17.],\n",
            "        [ 49.,  76.,  68.,  84.]], device='cuda:0'), 'labels': ['1', '0', '2', '4', '3', '4']}, {'boxes': tensor([[62., 68., 78., 75.],\n",
            "        [73., 23., 92., 33.],\n",
            "        [ 6.,  3., 25., 11.],\n",
            "        [54., 15., 73., 23.],\n",
            "        [13., 78., 29., 80.],\n",
            "        [20., 16., 39., 29.]], device='cuda:0'), 'labels': ['1', '3', '1', '1', '1', '4']}, {'boxes': tensor([[ 57.,  13.,  76.,  29.],\n",
            "        [ 76.,  81.,  90.,  90.],\n",
            "        [103.,  17., 122.,  22.],\n",
            "        [ 60.,  53.,  79.,  69.],\n",
            "        [ 83.,  24.,  97.,  33.]], device='cuda:0'), 'labels': ['4', '0', '1', '2', '4']}, {'boxes': tensor([[ 89.,  60., 101.,  71.],\n",
            "        [ 79.,   6.,  96.,  25.],\n",
            "        [ 11.,  34.,  23.,  43.],\n",
            "        [111.,  16., 123.,  25.],\n",
            "        [ 87.,  26., 104.,  45.],\n",
            "        [ 52.,  60.,  71.,  67.]], device='cuda:0'), 'labels': ['3', '2', '0', '4', '4', '4']}, {'boxes': tensor([[102.,  96., 121., 111.],\n",
            "        [ 46.,  78.,  64.,  82.],\n",
            "        [ 53.,  70.,  64.,  78.],\n",
            "        [ 58.,  94.,  77., 109.]], device='cuda:0'), 'labels': ['2', '1', '2', '2']}, {'boxes': tensor([[103.,  35., 122.,  39.],\n",
            "        [ 23., 108.,  42., 115.],\n",
            "        [ 31.,  81.,  50.,  98.]], device='cuda:0'), 'labels': ['1', '1', '2']}, {'boxes': tensor([[ 19.,  53.,  37.,  64.],\n",
            "        [ 91.,  98., 102., 107.],\n",
            "        [102.,  99., 119., 109.],\n",
            "        [ 28.,  25.,  47.,  44.]], device='cuda:0'), 'labels': ['0', '2', '3', '2']}, {'boxes': tensor([[ 37.,  22.,  53.,  35.],\n",
            "        [  7.,  28.,  26.,  46.],\n",
            "        [ 13., 117.,  23., 121.],\n",
            "        [ 62., 107.,  80., 123.],\n",
            "        [ 38.,  77.,  57.,  92.],\n",
            "        [ 20.,   9.,  39.,  14.]], device='cuda:0'), 'labels': ['3', '3', '1', '0', '0', '1']}, {'boxes': tensor([[ 85.,  20., 104.,  25.],\n",
            "        [ 22.,  12.,  31.,  16.],\n",
            "        [ 77.,  71.,  96.,  83.],\n",
            "        [ 57.,  17.,  67.,  23.],\n",
            "        [ 42.,  94.,  52.,  96.]], device='cuda:0'), 'labels': ['1', '3', '3', '2', '1']}, {'boxes': tensor([[ 11.,  99.,  30., 109.],\n",
            "        [ 14.,  22.,  33.,  37.],\n",
            "        [ 80.,  31.,  99.,  50.],\n",
            "        [ 85.,  23., 104.,  33.]], device='cuda:0'), 'labels': ['3', '2', '2', '1']}, {'boxes': tensor([[ 27., 103.,  46., 118.],\n",
            "        [ 84.,  33., 103.,  46.],\n",
            "        [ 77.,  81.,  96.,  90.]], device='cuda:0'), 'labels': ['4', '4', '3']}, {'boxes': tensor([[ 52.,  71.,  69.,  73.],\n",
            "        [ 71., 102.,  90., 115.],\n",
            "        [ 36.,  97.,  55., 110.],\n",
            "        [ 61., 102.,  72., 112.],\n",
            "        [ 24.,   6.,  43.,  25.],\n",
            "        [ 94., 110., 108., 119.]], device='cuda:0'), 'labels': ['1', '4', '3', '4', '2', '2']}, {'boxes': tensor([[ 10., 103.,  25., 112.],\n",
            "        [ 33., 105.,  44., 110.],\n",
            "        [ 81.,  40.,  97.,  56.],\n",
            "        [ 24.,  77.,  43.,  89.]], device='cuda:0'), 'labels': ['2', '4', '3', '3']}, {'boxes': tensor([[ 78.,  79.,  97.,  98.],\n",
            "        [ 67.,  18.,  86.,  33.],\n",
            "        [  9.,  82.,  22., 101.],\n",
            "        [ 39., 107.,  48., 117.],\n",
            "        [ 72., 110.,  91., 114.],\n",
            "        [ 54.,  71.,  69.,  75.]], device='cuda:0'), 'labels': ['3', '1', '0', '2', '1', '1']}, {'boxes': tensor([[  5., 107.,  24., 118.],\n",
            "        [ 26.,   0.,  40.,  14.],\n",
            "        [ 31.,  68.,  50.,  75.]], device='cuda:0'), 'labels': ['4', '0', '1']}, {'boxes': tensor([[ 86.,  34.,  99.,  44.],\n",
            "        [ 83., 113.,  95., 120.],\n",
            "        [ 61.,  54.,  80.,  69.]], device='cuda:0'), 'labels': ['2', '3', '2']}, {'boxes': tensor([[ 10.,  47.,  29.,  61.],\n",
            "        [ 91.,  39., 105.,  42.],\n",
            "        [ 16.,  32.,  33.,  46.],\n",
            "        [ 79.,  92.,  98., 105.]], device='cuda:0'), 'labels': ['0', '1', '0', '3']}, {'boxes': tensor([[ 76.,  20.,  95.,  29.],\n",
            "        [114.,  12., 123.,  18.],\n",
            "        [ 45.,   7.,  64.,  22.]], device='cuda:0'), 'labels': ['0', '4', '4']}, {'boxes': tensor([[ 11.,   9.,  24.,  15.],\n",
            "        [ 88.,  10., 107.,  14.],\n",
            "        [ 52.,  89.,  71., 106.],\n",
            "        [ 55.,   3.,  62.,  14.]], device='cuda:0'), 'labels': ['4', '1', '2', '0']}, {'boxes': tensor([[106.,  86., 121., 100.],\n",
            "        [ 92.,  74., 109.,  87.],\n",
            "        [ 63., 102.,  82., 119.]], device='cuda:0'), 'labels': ['4', '4', '4']}, {'boxes': tensor([[ 54., 102.,  73., 119.],\n",
            "        [ 89., 103., 108., 119.],\n",
            "        [ 76.,  54.,  95.,  69.],\n",
            "        [  1.,  12.,  19.,  28.],\n",
            "        [ 32.,  19.,  46.,  31.],\n",
            "        [ 76.,  84.,  91., 103.]], device='cuda:0'), 'labels': ['4', '2', '2', '2', '2', '0']}, {'boxes': tensor([[ 53.,   4.,  63.,  10.],\n",
            "        [ 98.,  46., 107.,  47.],\n",
            "        [108.,  38., 122.,  51.]], device='cuda:0'), 'labels': ['3', '1', '4']}, {'boxes': tensor([[ 80.,  51.,  92.,  70.],\n",
            "        [ 26., 111.,  44., 117.],\n",
            "        [105., 112., 124., 117.]], device='cuda:0'), 'labels': ['0', '1', '1']}, {'boxes': tensor([[ 79.,  18.,  97.,  22.],\n",
            "        [ 37.,  52.,  56.,  71.],\n",
            "        [ 78.,  72.,  97.,  89.],\n",
            "        [ 88., 102., 107., 116.],\n",
            "        [ 42.,  34.,  59.,  52.],\n",
            "        [ 53.,  61.,  72.,  78.]], device='cuda:0'), 'labels': ['1', '2', '2', '3', '2', '2']}, {'boxes': tensor([[ 85.,  30., 104.,  47.],\n",
            "        [ 91.,  69., 105.,  72.],\n",
            "        [ 58.,  92.,  77.,  99.]], device='cuda:0'), 'labels': ['4', '1', '1']}, {'boxes': tensor([[ 63.,   7.,  82.,  26.],\n",
            "        [ 36.,  49.,  47.,  51.],\n",
            "        [ 53.,  43.,  66.,  45.],\n",
            "        [ 61.,  22.,  78.,  41.],\n",
            "        [ 88.,  60., 107.,  77.],\n",
            "        [ 83.,  33.,  96.,  52.]], device='cuda:0'), 'labels': ['3', '1', '1', '4', '2', '0']}, {'boxes': tensor([[ 58.,   9.,  74.,  26.],\n",
            "        [ 90.,  58., 107.,  70.],\n",
            "        [ 86.,  31., 101.,  47.]], device='cuda:0'), 'labels': ['2', '0', '4']}, {'boxes': tensor([[ 33.,  99.,  49., 115.],\n",
            "        [ 53.,  75.,  72.,  94.],\n",
            "        [  5.,  15.,  14.,  16.],\n",
            "        [ 51.,  47.,  70.,  62.]], device='cuda:0'), 'labels': ['2', '0', '1', '2']}, {'boxes': tensor([[ 11., 108.,  30., 123.],\n",
            "        [ 48.,  35.,  59.,  40.],\n",
            "        [ 70.,  23.,  83.,  38.],\n",
            "        [ 34.,  62.,  53.,  78.],\n",
            "        [ 22.,  82.,  40.,  94.]], device='cuda:0'), 'labels': ['0', '1', '2', '0', '3']}, {'boxes': tensor([[ 79.,  96.,  98., 105.],\n",
            "        [ 30.,   1.,  45.,  14.],\n",
            "        [ 67.,  74.,  83.,  86.],\n",
            "        [ 76.,  36.,  87.,  44.]], device='cuda:0'), 'labels': ['1', '4', '3', '0']}, {'boxes': tensor([[ 18.,  70.,  37.,  86.],\n",
            "        [100.,  65., 119.,  76.],\n",
            "        [ 60.,  25.,  78.,  43.]], device='cuda:0'), 'labels': ['2', '2', '0']}, {'boxes': tensor([[106., 102., 120., 107.],\n",
            "        [ 56.,  36.,  75.,  49.],\n",
            "        [ 64., 114.,  83., 123.],\n",
            "        [ 64.,  76.,  80.,  79.],\n",
            "        [ 81., 104.,  90., 109.]], device='cuda:0'), 'labels': ['3', '3', '4', '1', '2']}, {'boxes': tensor([[ 54., 109.,  73., 116.],\n",
            "        [ 85.,  24.,  96.,  30.],\n",
            "        [ 35.,   5.,  45.,  14.],\n",
            "        [ 15., 109.,  34., 118.]], device='cuda:0'), 'labels': ['1', '4', '2', '1']}, {'boxes': tensor([[ 18.,  96.,  32., 110.],\n",
            "        [ 81.,  21.,  91.,  24.],\n",
            "        [ 32.,   4.,  51.,  13.],\n",
            "        [ 23.,  66.,  42.,  76.],\n",
            "        [ 98.,  47., 112.,  58.],\n",
            "        [ 48.,  84.,  63.,  92.]], device='cuda:0'), 'labels': ['2', '4', '3', '4', '0', '3']}, {'boxes': tensor([[ 74.,  56.,  93.,  72.],\n",
            "        [ 73., 106.,  92., 119.],\n",
            "        [ 42.,  22.,  60.,  38.]], device='cuda:0'), 'labels': ['0', '2', '2']}, {'boxes': tensor([[  5.,  64.,  24.,  81.],\n",
            "        [ 69., 117.,  79., 119.],\n",
            "        [ 35.,  57.,  52.,  74.],\n",
            "        [ 11.,  41.,  29.,  60.],\n",
            "        [ 32.,  12.,  51.,  29.]], device='cuda:0'), 'labels': ['4', '1', '2', '2', '2']}, {'boxes': tensor([[ 30.,  54.,  42.,  65.],\n",
            "        [101.,  14., 120.,  19.],\n",
            "        [ 55.,  62.,  71.,  76.],\n",
            "        [ 70.,  14.,  87.,  31.]], device='cuda:0'), 'labels': ['0', '1', '4', '2']}, {'boxes': tensor([[  1., 103.,  20., 122.],\n",
            "        [ 93.,  41., 103.,  49.],\n",
            "        [ 60.,  86.,  73.,  89.],\n",
            "        [ 21.,  19.,  33.,  28.]], device='cuda:0'), 'labels': ['2', '0', '1', '0']}, {'boxes': tensor([[ 96.,  23., 115.,  29.],\n",
            "        [ 59.,  94.,  66., 103.],\n",
            "        [ 23.,  53.,  38.,  61.],\n",
            "        [ 37., 109.,  56., 116.]], device='cuda:0'), 'labels': ['1', '0', '3', '1']}, {'boxes': tensor([[36., 80., 48., 90.],\n",
            "        [41., 27., 60., 40.],\n",
            "        [ 9., 58., 28., 75.]], device='cuda:0'), 'labels': ['0', '4', '3']}, {'boxes': tensor([[ 31.,  41.,  47.,  58.],\n",
            "        [105.,   1., 117.,  13.],\n",
            "        [ 51.,  90.,  67.,  97.],\n",
            "        [ 48.,   0.,  67.,  16.]], device='cuda:0'), 'labels': ['2', '0', '1', '4']}, {'boxes': tensor([[39., 44., 58., 57.],\n",
            "        [21., 17., 40., 30.],\n",
            "        [38., 72., 56., 91.]], device='cuda:0'), 'labels': ['0', '3', '2']}, {'boxes': tensor([[ 79.,  68.,  90.,  77.],\n",
            "        [ 82., 117.,  92., 118.],\n",
            "        [ 61.,  71.,  80.,  80.],\n",
            "        [ 19.,  59.,  36.,  69.]], device='cuda:0'), 'labels': ['4', '1', '1', '1']}, {'boxes': tensor([[  3.,  36.,  15.,  47.],\n",
            "        [ 87.,  65.,  98.,  76.],\n",
            "        [ 34., 102.,  51., 113.],\n",
            "        [ 59., 110.,  74., 114.]], device='cuda:0'), 'labels': ['2', '2', '4', '1']}, {'boxes': tensor([[104.,  21., 123.,  30.],\n",
            "        [ 59.,  48.,  77.,  64.],\n",
            "        [ 69.,  36.,  88.,  44.],\n",
            "        [ 13.,  35.,  31.,  48.],\n",
            "        [ 35.,  47.,  47.,  48.]], device='cuda:0'), 'labels': ['1', '4', '1', '0', '1']}, {'boxes': tensor([[ 75.,  79.,  84.,  86.],\n",
            "        [ 41., 100.,  60., 107.],\n",
            "        [  0.,  38.,  10.,  47.]], device='cuda:0'), 'labels': ['4', '1', '3']}, {'boxes': tensor([[60., 19., 79., 36.],\n",
            "        [47., 32., 65., 40.],\n",
            "        [64., 76., 76., 88.]], device='cuda:0'), 'labels': ['3', '1', '0']}, {'boxes': tensor([[ 94.,  22., 113.,  39.],\n",
            "        [ 98.,  60., 117.,  69.],\n",
            "        [  7.,  20.,  25.,  26.]], device='cuda:0'), 'labels': ['3', '3', '1']}, {'boxes': tensor([[69., 29., 88., 32.],\n",
            "        [ 1., 36., 18., 53.],\n",
            "        [53., 24., 72., 43.]], device='cuda:0'), 'labels': ['1', '3', '0']}, {'boxes': tensor([[ 73.,  33.,  92.,  50.],\n",
            "        [  1.,  93.,  17., 104.],\n",
            "        [ 49.,  12.,  60.,  24.],\n",
            "        [ 45.,  68.,  63.,  81.],\n",
            "        [ 19.,  20.,  31.,  22.]], device='cuda:0'), 'labels': ['0', '4', '2', '3', '1']}, {'boxes': tensor([[ 29.,  72.,  45.,  78.],\n",
            "        [ 67.,  60.,  86.,  77.],\n",
            "        [ 67.,  89.,  86., 106.],\n",
            "        [ 50., 112.,  68., 124.]], device='cuda:0'), 'labels': ['1', '4', '2', '4']}, {'boxes': tensor([[ 24., 103.,  40., 116.],\n",
            "        [ 21.,  88.,  31.,  90.],\n",
            "        [ 40.,  99.,  59., 114.],\n",
            "        [ 65.,  65.,  84.,  75.]], device='cuda:0'), 'labels': ['2', '1', '3', '1']}, {'boxes': tensor([[ 76.,  30.,  90.,  33.],\n",
            "        [ 65., 101.,  84., 108.],\n",
            "        [ 27.,  36.,  46.,  53.],\n",
            "        [ 54.,   3.,  73.,  22.],\n",
            "        [ 37.,  11.,  51.,  29.],\n",
            "        [ 74.,  40.,  88.,  54.]], device='cuda:0'), 'labels': ['1', '1', '4', '0', '3', '4']}, {'boxes': tensor([[ 33.,  48.,  52.,  60.],\n",
            "        [103.,  41., 122.,  56.],\n",
            "        [ 18.,  71.,  37.,  86.],\n",
            "        [107.,  85., 123., 101.],\n",
            "        [ 13.,   8.,  32.,  27.]], device='cuda:0'), 'labels': ['4', '3', '2', '2', '3']}]\n",
            "in: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'), tar<class 'dict'>\n",
            "out: torch.Size([1, 21504, 5])\n",
            "bounding: torch.Size([1, 21504, 4]), center: torch.Size([1, 21504, 4])\n",
            "tensor([[-16., -16.,  16.,  16.],\n",
            "        [-15., -16.,  17.,  16.],\n",
            "        [-14., -16.,  18.,  16.],\n",
            "        ...,\n",
            "        [ 52.,  60., 180., 188.],\n",
            "        [ 56.,  60., 184., 188.],\n",
            "        [ 60.,  60., 188., 188.]], device='cuda:0')\n",
            "tensor([[ 24.,  13.,  40.,  22.],\n",
            "        [ 17.,  26.,  34.,  45.],\n",
            "        [ 74.,  20.,  91.,  39.],\n",
            "        [ 88., 104., 104., 117.]], device='cuda:0')\n",
            "torch.Size([21504])\n",
            "1\n",
            "['4', '4', '0', '2']\n",
            "torch.Size([21504])\n",
            "tensor([[ 24.,  13.,  40.,  22.],\n",
            "        [ 17.,  26.,  34.,  45.],\n",
            "        [ 74.,  20.,  91.,  39.],\n",
            "        [ 88., 104., 104., 117.]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-305-e004e5a4332d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-302-93ee592ba517>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'in: {train_input}, tar{type(train_target)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m           \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-298-ca3d1c83d6d0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_anchors_per_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-298-ca3d1c83d6d0>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, targets, head_outputs, anchors, num_anchors_per_level)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# end of your code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# matched index - anchor-to-target match\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatched_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     def postprocess_detections(\n",
            "\u001b[0;32m<ipython-input-297-c0f8f362a62f>\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, targets, head_outputs, anchors, matched_idxs)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_per_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mgt_classes_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_per_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatched_idxs_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mgt_boxes_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_per_image\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatched_idxs_per_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mgt_classes_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatched_idxs_per_image\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m  \u001b[0;31m# background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot example results of matched and mismatched predictions\n",
        "def plot_results(good, bad):\n",
        "  total = len(good) + len(bad)\n",
        "  print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                    total, 100 * (len(good)) / total))\n",
        "  \n",
        "  print('Correct object detection:')\n",
        "  for g in good:\n",
        "    g.plot()\n",
        "\n",
        "  print('Incorrect object detection:')\n",
        "  for b in bad:\n",
        "    b.plot()"
      ],
      "metadata": {
        "id": "F3fH9tQpS-Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot particular losses and evaluation score per\n",
        "def plot_losses(train_loss, test_loss):\n",
        "    xs = range(1, len(train_loss) + 1)\n",
        "    plt.clf()\n",
        "    plt.plot(xs, train_loss, label='train loss')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0Xx90TE6S-rL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n"
      ],
      "metadata": {
        "id": "KqupNuvv3S3i"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JkOLl12nkiI8"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}