{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/dnn2022/blob/main/Asia_of_hw2_fcos_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VBJpAMzglRo"
      },
      "source": [
        "# Anchor-free single-stage object detection with FCOS (v2)\n",
        "\n",
        "In this exercise your goal will be to solve an object detection training and prediction task using the anchor-free single-stage approach.\n",
        "\n",
        "There are 10 points to get in total.\n",
        "\n",
        "## TLDR; overview\n",
        "\n",
        "In this task one should:\n",
        "- build an object detection model using the variant of `FCOS`,\n",
        "- train an object detection model.\n",
        "\n",
        "Hints and comments:\n",
        "\n",
        "- Model architecture and loss are heavily inspired by [FCOS](https://arxiv.org/pdf/1904.01355.pdf) paper,\n",
        "- you can freely subclass and extend the interface of classes in this exercise,\n",
        "- be sure that you understand the concept of anchor-free object detection. There are many tutorials and articles about it (e.g. [this](https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c) one)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcKkG56SItK"
      },
      "source": [
        "### Notebook changelog (compared to the initial version)\n",
        "Changed in v2:\n",
        "- Added definition of $\\sigma$ in the scoring formula.\n",
        "- Added the description how the `target` variable should look like.\n",
        "- Fixed the typo about mismatched `in_channels` and `out_channels` in the classification head description and added the whole info about it in the regression head description.\n",
        "- Added information about 1-element batch.\n",
        "- Fixed typehint in `BackboneWithFPN` (`forward(self, x: MnistCanvas)` -> `forward(self, x: torch.Tensor)`)\n",
        "- Removed info about non-existing exercise (\"...so use the foreground mask from the previous excercise.\" -> \"... so use the foreground mask.\")\n",
        "- Fixed typo: \"use `self.box_coder.decode_single` and `self.box_coder.decode_single`\" -> use \"`self.box_coder.encode_single` and `self.box_coder.decode_single`\"\n",
        "- Removed mentions of non-existing `TargetDecoder.get_predictions` and rotation.\n",
        "- Removed additional TODO placeholder from detection post-processing.\n",
        "- Added the information about using the different `evaluate` parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsyO2OdlLLE"
      },
      "source": [
        "### Data description\n",
        "\n",
        "In this task we will paste bounding boxes with digits **from 1 to 5** randomly selected from `MNIST` dataset on a canvas of size `(128, 128)` and **randomly scaled by a factor between 0.5 and 1.0**. We assume that:\n",
        "\n",
        "- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n",
        "- the digits are fully contained in canvas,\n",
        "- boxes are modeled using `MnistBox` class,\n",
        "- canvas is modeled using `MnistCanvas` class.\n",
        "\n",
        "Let us have a look at definition of these classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "L1rAdIiRq2G8"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing import Optional\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MnistBox:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        x_max: int,\n",
        "        y_max: int,\n",
        "        class_nb: Optional[int] = None,\n",
        "        rotated: Optional[bool] = None,\n",
        "    ):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "        self.y_min = y_min\n",
        "        self.y_max = y_max\n",
        "        self.class_nb = class_nb\n",
        "        self.rotated = rotated\n",
        "    \n",
        "    @property\n",
        "    def x_diff(self):\n",
        "        return self.x_max - self.x_min\n",
        "    \n",
        "    @property\n",
        "    def y_diff(self):\n",
        "        return self.y_max - self.y_min\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Mnist Box: x_min = {self.x_min},' +\\\n",
        "               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n",
        "               f' y_max = {self.y_max}. Class = {self.class_nb}.' +\\\n",
        "               f' Rotated = {self.rotated}.'\n",
        "\n",
        "    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n",
        "        ax.add_patch(\n",
        "            patches.Rectangle(\n",
        "                (self.y_min, self.x_min),\n",
        "                 self.y_diff,\n",
        "                 self.x_diff,\n",
        "                 linewidth=1,\n",
        "                 edgecolor=color,\n",
        "                 facecolor='none',\n",
        "            )\n",
        "        )\n",
        "        ax.text(\n",
        "            self.y_min,\n",
        "            self.x_min,\n",
        "            f'{self.class_nb}' if not self.rotated else f'{self.class_nb}*',\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def area(self):\n",
        "        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n",
        "\n",
        "    def iou_with(self, other_box: \"MnistBox\"):\n",
        "        aux_box = MnistBox(\n",
        "            x_min=max(self.x_min, other_box.x_min),\n",
        "            x_max=min(self.x_max, other_box.x_max),\n",
        "            y_min=max(self.y_min, other_box.y_min),\n",
        "            y_max=min(self.y_max, other_box.y_max),\n",
        "        ) \n",
        "        return aux_box.area / (self.area + other_box.area - aux_box.area)\n",
        "\n",
        "\n",
        "class MnistCanvas:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        boxes: List[MnistBox],\n",
        "    ):\n",
        "        self.image = image\n",
        "        self.boxes = boxes\n",
        "        self.shape = (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "\n",
        "    def add_digit(\n",
        "        self,\n",
        "        digit: np.ndarray,\n",
        "        class_nb: int,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        rotated=None,\n",
        "        iou_threshold=0.1,\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Add a digit to an image if it does not overlap with existing boxes\n",
        "        above iou_threshold.\n",
        "        \"\"\"\n",
        "        image_x, image_y = digit.shape\n",
        "        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n",
        "            raise ValueError('Wrong initial corner box')\n",
        "        new_box_x_min = x_min\n",
        "        new_box_y_min = y_min\n",
        "        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n",
        "        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n",
        "        new_box = MnistBox(\n",
        "            x_min=new_box_x_min,\n",
        "            x_max=new_box_x_max,\n",
        "            y_min=new_box_y_min,\n",
        "            y_max=new_box_y_max,\n",
        "            class_nb=class_nb,\n",
        "            rotated=rotated,\n",
        "        )\n",
        "        old_background = self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ]\n",
        "        for box in self.boxes:\n",
        "            if new_box.iou_with(box) > iou_threshold:\n",
        "                return False\n",
        "        self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ] = np.maximum(old_background, digit)\n",
        "        self.boxes.append(\n",
        "            new_box\n",
        "        ) \n",
        "        return True\n",
        "        \n",
        "    def get_torch_tensor(self) -> torch.Tensor:\n",
        "        np_image = self.image.astype('float32')\n",
        "        np_image = np_image.reshape(\n",
        "            (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "        )\n",
        "        return torch.from_numpy(np_image).to(DEVICE)\n",
        "\n",
        "    @classmethod\n",
        "    def get_empty_of_size(cls, size: Tuple[int, int]):\n",
        "        return cls(\n",
        "            image=np.zeros(size),\n",
        "            boxes=[],\n",
        "        )\n",
        "\n",
        "    def plot(self, boxes: Optional[List[MnistBox]] = None):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(self.image)\n",
        "        boxes = boxes or self.boxes\n",
        "        for box in boxes:\n",
        "            box.plot_on_ax(ax)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqrSxb66RK-f"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWMxgsgFtlze"
      },
      "source": [
        "Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "HezSZXw4z-cx"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import skimage.transform as st\n",
        "\n",
        "\n",
        "mnist_data = mnist.load_data()\n",
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n",
        "\n",
        "\n",
        "def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n",
        "    bool_digit = digit > threshold\n",
        "    x_range = bool_digit.max(axis=0)\n",
        "    y_range = bool_digit.max(axis=1)\n",
        "    start_x = (x_range.cumsum() == 0).sum()\n",
        "    end_x = (x_range[::-1].cumsum() == 0).sum()\n",
        "    start_y = (y_range.cumsum() == 0).sum()\n",
        "    end_y = (y_range[::-1].cumsum() == 0).sum()\n",
        "    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n",
        "\n",
        "\n",
        "TRAIN_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_train[:10000])\n",
        "]\n",
        "TRAIN_CLASSES = mnist_y_train[:10000]\n",
        "\n",
        "TEST_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_test[:1000])\n",
        "]\n",
        "TEST_CLASSES = mnist_y_test[:1000]\n",
        "\n",
        "\n",
        "def get_random_canvas(\n",
        "    digits: Optional[List[np.ndarray]] = None,\n",
        "    classes: Optional[List[int]] = None,\n",
        "    nb_of_digits: Optional[int] = None,\n",
        "    labels = [0, 1, 2, 3, 4]\n",
        "    ) -> MnistCanvas:\n",
        "    digits = digits if digits is not None else TRAIN_DIGITS\n",
        "    classes = classes if classes is not None else TRAIN_CLASSES\n",
        "    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n",
        "    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n",
        "    attempts_done = 0\n",
        "    while attempts_done < nb_of_digits:\n",
        "        current_digit_index = np.random.randint(len(digits))\n",
        "        current_digit_class = classes[current_digit_index]\n",
        "        if current_digit_class not in labels:\n",
        "            continue\n",
        "        rescale = np.random.random() > 0.5\n",
        "        current_digit = digits[current_digit_index]\n",
        "        if rescale:\n",
        "            factor = (np.random.random() / 2) + 0.5\n",
        "            current_digit = st.resize(\n",
        "                current_digit, \n",
        "                (int(current_digit.shape[0] * factor), int(current_digit.shape[1] * factor)))\n",
        "            # current_digit = np.rot90(current_digit)\n",
        "        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n",
        "        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n",
        "        if new_canvas.add_digit(\n",
        "            digit=current_digit,\n",
        "            x_min=random_x_min,\n",
        "            y_min=random_y_min,\n",
        "            class_nb=current_digit_class,\n",
        "            rotated=rescale,\n",
        "        ):\n",
        "            attempts_done += 1\n",
        "    return new_canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i2OjUEC7eaC"
      },
      "source": [
        "Let us have a look at example canvas (rescaled digits have additional *added to description)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "OsLpINOtvhd8",
        "outputId": "949104e1-f474-46ca-ea6f-32a6e3434fbd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "mnist_canvas = get_random_canvas()\n",
        "mnist_canvas.plot()\n",
        "print(mnist_canvas.get_torch_tensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgU2bKyJ3pVh"
      },
      "source": [
        "For training one can either:\n",
        "- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n",
        "- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GpJZUkDGJSi"
      },
      "source": [
        "### Model building (5 pt.)\n",
        "\n",
        "\n",
        "One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdCBH9PkT2C"
      },
      "source": [
        "#### $\\texttt{backbone}$:\n",
        "\n",
        "We provided you with a backbone model architecture paired with Feature Pyramid Network (`BackboneWithFPN`) that accepts a `MnistCanvas` instance and output a dictionary, which has a FPN group name as a keys and their tensors as value.\n",
        "For a FPN with strides set to [32, 64, 128] and number of output channels set to 64, the sizes of the tensors will be [1, 64, 128, 128], [1, 64, 64, 64], [1, 64, 32, 32] consecutively. This module should be trained together with the rest of your solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "kXco8riNGHhl"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from torch import nn, Tensor\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "\n",
        "\n",
        "class Backbone(torch.nn.Module):\n",
        "    def __init__(self, strides = [8, 16, 32]):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.first_block = torch.nn.Sequential(\n",
        "            nn.Conv2d(1, strides[0], (3, 3), padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [torch.nn.Sequential(*[\n",
        "                nn.Conv2d(strides[i-1], strides[i], (3, 3), padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2),\n",
        "              ]) for i in range(1, len(strides))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        image = x.to(DEVICE).view(1, 1, 128, 128)\n",
        "        x = self.first_block(image)\n",
        "        aux = [x]\n",
        "        for block in self.blocks:\n",
        "            x = block(aux[-1])\n",
        "            aux.append(x)\n",
        "        return aux\n",
        "\n",
        "\n",
        "class BackboneWithFPN(torch.nn.Module):\n",
        "    def __init__(self, strides, out_channels=32) -> None:\n",
        "        super().__init__()\n",
        "        self.strides = strides\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone = Backbone(self.strides)\n",
        "        self.fpn = FeaturePyramidNetwork(self.strides, self.out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output_backbone = self.backbone(x)\n",
        "        \n",
        "        x = OrderedDict()\n",
        "        for i, f in enumerate(output_backbone):\n",
        "            x[f'feat{i}'] = f\n",
        "        output_fpn = self.fpn(x)\n",
        "        return output_fpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkOLl12nkiI8"
      },
      "source": [
        "#### $\\texttt{anchor generator}$:\n",
        "\n",
        "FCOS is anchor-free in a typical sense of this word, but it can also be said that there is one pixel-wise \"anchor\" per localisation on a given feature map.\n",
        "Therefore, anchor generator from `torchvision` is used for convenience.\n",
        "You will obtain $128^2 + 64^2 + 32^2 = 21504$ locations in total for the previously chosen strides.\n",
        "They will be called anchors in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyd2-zOxf9VM",
        "outputId": "7ea3c496-fb44-4fc3-bb60-945adcbf69a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ],
      "source": [
        "# example code - anchor generator is already included in the code later\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "\n",
        "anchor_sizes = ((32,), (64,), (128,))  # equal to strides of FPN multi-level feature map\n",
        "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor for each level\n",
        "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "anchor_generator.num_anchors_per_location()\n",
        "# notice that effectively one anchor is one location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "iM3oSesif-F5"
      },
      "outputs": [],
      "source": [
        "# Later in the code you will use the anchor generator in the following way:\n",
        "# anchors = anchor_generator(images, features)\n",
        "# [x.size(2) * x.size(3) for x in features] # recover level sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTYyCuMdmBk7"
      },
      "source": [
        "#### $\\texttt{FCOSClassificationHead}$ (1 pt.):\n",
        "\n",
        "Write a classification head to be used in FCOS.\n",
        "The input is is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$).\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The additional final block should be `nn.Conv2d` outputting `C` channels.\n",
        "The final output should consist of classification logits of shape `(N, A, C)`, where `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {
        "id": "kpDP9QVjnn05"
      },
      "outputs": [],
      "source": [
        "class FCOSClassificationHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_convs: int = 4,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        size = in_channels\n",
        "        c = num_classes\n",
        "\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs]\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(size, c-1, kernel_size=3, padding=1)\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tensor:\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        output = []\n",
        "        for i in x:\n",
        "          out = i\n",
        "          print(i.shape)\n",
        "          for b in self.blocks:\n",
        "            out = b(out)\n",
        "            print(out.shape) #torch.Size([1, 64, 128, 128])\n",
        "          output.append(self.final(out))\n",
        "          print(self.final(out).shape)#torch.Size([1, 5, 128, 128])\n",
        "        output =  torch.cat(output, dim=2)\n",
        "        print(f'class_head: {output.shape}')\n",
        "\n",
        "        return output\n",
        "        ################################################################################################\n",
        "        # end of your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcHAnQy2mFJU"
      },
      "source": [
        "#### $\\texttt{FCOSRegressionHead}$  (1 pt.):\n",
        "\n",
        "Write a regression head to be used in FCOS - both for bounding boxes and center-ness.\n",
        "The input is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$), which will be shared for regression and center-ness.\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The final block for bounding box regression should be `nn.Conv2d` and have `4` channels and it should be followed by relu functional to get rid of negative values.\n",
        "The final block for center-ness regression should be `nn.Conv2d` and have `1` channel.\n",
        "The output should consist of a tuple of tensors (bounding box regression and center-ness).\n",
        "Bounding box regression logits should be of shape `(N, A, 4)`, whereas for center-ness that would be `(N, A, 1)`.\n",
        "Similarly, `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "3H6KNzi1mEhg"
      },
      "outputs": [],
      "source": [
        "class FCOSRegressionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_convs: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        size = in_channels\n",
        "\n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs]\n",
        "        )\n",
        "\n",
        "        self.bounding = nn.Sequential(\n",
        "            nn.Conv2d(size, 4, kernel_size=3, padding=1), \n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.center = nn.Conv2d(size, 4, kernel_size=3, padding=1)\n",
        "        # end of your code\n",
        "        ################################################################################################\n",
        "        \n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
        "        pass # TODO: your code here\n",
        "        ################################################################################################\n",
        "        output = []\n",
        "        bounding = []\n",
        "        center = []\n",
        "        for i in x:\n",
        "          out = i\n",
        "          for b in self.blocks:\n",
        "            out = b(out)\n",
        "          bounding.append(self.bounding(out))\n",
        "          center.append(self.center(out))\n",
        "        bounding =  torch.cat(bounding)\n",
        "        center =  torch.cat(center, dim=2)\n",
        "\n",
        "        print(f'bounding: {bounding.shape}, center: {center.shape}')\n",
        "\n",
        "        return bounding, center\n",
        "        ################################################################################################\n",
        "        # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0I47ydkvUL"
      },
      "source": [
        "#### $\\texttt{FCOSHead}$ (2 pt.):\n",
        "\n",
        "Here, the computation of the foreground indices and losses takes place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "PX8P0s-jOK-F"
      },
      "outputs": [],
      "source": [
        "class BoxLinearCoder:\n",
        "    \"\"\"\n",
        "    The linear box-to-box transform defined in FCOS. The transformation is parameterized\n",
        "    by the distance from the center of (square) src box to 4 edges of the target box.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalize_by_size: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            normalize_by_size (bool): normalize deltas by the size of src (anchor) boxes.\n",
        "        \"\"\"\n",
        "        self.normalize_by_size = normalize_by_size\n",
        "\n",
        "    def encode_single(self, reference_boxes: Tensor, proposals: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Encode a set of proposals with respect to some reference boxes\n",
        "\n",
        "        Args:\n",
        "            reference_boxes (Tensor): reference boxes\n",
        "            proposals (Tensor): boxes to be encoded\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the encoded relative box offsets that can be used to\n",
        "            decode the boxes.\n",
        "        \"\"\"\n",
        "        # get the center of reference_boxes\n",
        "        reference_boxes_ctr_x = 0.5 * (reference_boxes[:, 0] + reference_boxes[:, 2])\n",
        "        reference_boxes_ctr_y = 0.5 * (reference_boxes[:, 1] + reference_boxes[:, 3])\n",
        "\n",
        "        # get box regression transformation deltas\n",
        "        target_l = reference_boxes_ctr_x - proposals[:, 0]\n",
        "        target_t = reference_boxes_ctr_y - proposals[:, 1]\n",
        "        target_r = proposals[:, 2] - reference_boxes_ctr_x\n",
        "        target_b = proposals[:, 3] - reference_boxes_ctr_y\n",
        "\n",
        "        targets = torch.stack((target_l, target_t, target_r, target_b), dim=1)\n",
        "        if self.normalize_by_size:\n",
        "            reference_boxes_w = reference_boxes[:, 2] - reference_boxes[:, 0]\n",
        "            reference_boxes_h = reference_boxes[:, 3] - reference_boxes[:, 1]\n",
        "            reference_boxes_size = torch.stack(\n",
        "                (reference_boxes_w, reference_boxes_h, reference_boxes_w, reference_boxes_h), dim=1\n",
        "            )\n",
        "            targets = targets / reference_boxes_size\n",
        "\n",
        "        return targets\n",
        "\n",
        "    def decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        From a set of original boxes and encoded relative box offsets,\n",
        "        get the decoded boxes.\n",
        "\n",
        "        Args:\n",
        "            rel_codes (Tensor): encoded boxes\n",
        "            boxes (Tensor): reference boxes.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the predicted boxes with the encoded relative box offsets.\n",
        "        \"\"\"\n",
        "\n",
        "        boxes = boxes.to(rel_codes.dtype)\n",
        "\n",
        "        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n",
        "        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n",
        "        if self.normalize_by_size:\n",
        "            boxes_w = boxes[:, 2] - boxes[:, 0]\n",
        "            boxes_h = boxes[:, 3] - boxes[:, 1]\n",
        "            boxes_size = torch.stack((boxes_w, boxes_h, boxes_w, boxes_h), dim=1)\n",
        "            rel_codes = rel_codes * boxes_size\n",
        "\n",
        "        pred_boxes1 = ctr_x - rel_codes[:, 0]\n",
        "        pred_boxes2 = ctr_y - rel_codes[:, 1]\n",
        "        pred_boxes3 = ctr_x + rel_codes[:, 2]\n",
        "        pred_boxes4 = ctr_y + rel_codes[:, 3]\n",
        "        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=1)\n",
        "        return pred_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loss calculation\n",
        "Compute the losses. \n",
        "They should be calculated on the positive locations/anchors, so use the foreground mask.\n",
        "For regression, use `self.box_coder.encode_single` and `self.box_coder.decode_single` to move between standard (x, y, x, y) and FCOS (l, t, r, b) bounding box format.\n",
        "There are three losses to be written.\n",
        "- classification loss (with `torchvision.ops.sigmoid_focal_loss`). (1 pt.)\n",
        "- Bounding box regression (with `torchvision.ops.generalized_box_iou_loss`). Decode predictions with `self.box_coder.decode_single` before regressing against the ground truth. (1 pt.)\n",
        "- ctrness loss (`torchvision.ops.sigmoid_focal_loss`). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)"
      ],
      "metadata": {
        "id": "qIGit7Yp_zEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "nDX5c0lw9ZxM"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from torchvision.ops import sigmoid_focal_loss, generalized_box_iou_loss\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "\n",
        "class FCOSHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A regression and classification head for use in FCOS.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_classes (int): number of classes to be predicted\n",
        "        num_convs (Optional[int]): number of conv layer of head. Default: 4.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, num_classes: int, num_convs: Optional[int] = 4) -> None:\n",
        "        super().__init__()\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.classification_head = FCOSClassificationHead(in_channels, num_classes, num_convs)\n",
        "        self.regression_head = FCOSRegressionHead(in_channels, num_convs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],             # anchors/locations\n",
        "        matched_idxs: List[Tensor],        # tells to which bounding box anchors are matched, -1 mean no matches\n",
        "    ) -> Dict[str, Tensor]:\n",
        "\n",
        "        cls_logits = head_outputs[\"cls_logits\"]  # [N, A, C]\n",
        "        bbox_regression = head_outputs[\"bbox_regression\"]  # [N, A, 4]\n",
        "        bbox_ctrness = head_outputs[\"bbox_ctrness\"]  # [N, A, 1]\n",
        "        \n",
        "      #  print(f\"cls: {cls_logits[0][0].shape}, reg: {bbox_regression[0][0].shape}, ctr: {bbox_ctrness[0][0].shape}\")\n",
        "\n",
        "        all_gt_classes_targets = []\n",
        "        all_gt_boxes_targets = []\n",
        "        print(len(matched_idxs))\n",
        "        \n",
        "        for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):\n",
        "            im = targets_per_image[\"labels\"]\n",
        "            print(im)\n",
        "            matches = matched_idxs_per_image\n",
        "            print(matches.shape)\n",
        "            t = targets_per_image[\"boxes\"]\n",
        "            print(t)\n",
        "            gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n",
        "            gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n",
        "            gt_classes_targets[matched_idxs_per_image < 0] = -1  # background\n",
        "            all_gt_classes_targets.append(gt_classes_targets)\n",
        "            all_gt_boxes_targets.append(gt_boxes_targets)\n",
        "\n",
        "        all_gt_classes_targets = torch.stack(all_gt_classes_targets)\n",
        "        \n",
        "        foregroud_mask = all_gt_classes_targets >= 0        \n",
        "        num_foreground = foregroud_mask.sum().item()\n",
        "        \n",
        "        loss_cls = self.compute_loss_cls(cls_logits, all_gt_classes_targets, foregroud_mask)\n",
        "        loss_bbox_reg = self.compute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n",
        "        loss_bbox_ctrness = self.compute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n",
        "\n",
        "        return {\n",
        "            \"classification\": loss_cls / max(1, num_foreground),\n",
        "            \"bbox_regression\": loss_bbox_reg / max(1, num_foreground),\n",
        "            \"bbox_ctrness\": loss_bbox_ctrness / max(1, num_foreground),\n",
        "        }\n",
        "\n",
        "    def compute_loss_ctrness(self, anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_regression = bbox_regression[foregroud_mask]\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        targets = self.box_coder.encode_single(anchors, all_gt_boxes_targets)\n",
        "        l = targets[:, 0]\n",
        "        t = targets[:, 1]\n",
        "        r = targets[:, 2]\n",
        "        b = targets[:, 3]\n",
        "        # Equation 3 from paper\n",
        "        targets = torch.sqrt((torch.min(l, r) / torch.max(l, r)) * (torch.min(t, b) / torch.max(t, b)))\n",
        "        return torchvision.ops.sigmoid_focal_loss(bbox_regression, targets, alpha=0.9, reduction=\"sum\")\n",
        "\n",
        "    def compute_loss_bbox_reg(self, anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_regression = bbox_regression[foregroud_mask]\n",
        "        bbox_regression = self.box_coder.decode_single(bbox_regression, anchors)\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        # Both sets of boxes are expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and 0 <= y1 < y2\n",
        "        # So we need to change the order of\n",
        "        return torchvision.ops.generalized_box_iou_loss(bbox_regression, all_gt_boxes_targets, reduction=\"sum\")\n",
        "\n",
        "    def compute_loss_cls(self, cls_logits, all_gt_classes_targets, foregroud_mask):\n",
        "        masked = torch.zeros(cls_logits.shape, device=cls_logits.device)\n",
        "        masked[foregroud_mask] = nn.functional.one_hot(all_gt_classes_targets[foregroud_mask], num_classes=cls_logits.shape[-1])\n",
        "        return torchvision.ops.sigmoid_focal_loss(cls_logits, masked, alpha=0.9, reduction=\"sum\")\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n",
        "        cls_logits = self.classification_head(x)\n",
        "        bbox_regression, bbox_ctrness = self.regression_head(x)\n",
        "        return {\n",
        "            \"cls_logits\": cls_logits,\n",
        "            \"bbox_regression\": bbox_regression,\n",
        "            \"bbox_ctrness\": bbox_ctrness,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFYNzC9KOK-G"
      },
      "source": [
        "#### Post-processing (1 pt.)\n",
        "Fill the gaps in the postprocessing routine.\n",
        "The paper states: \n",
        "\n",
        "> (...) the final score (used for ranking the detected bounding boxes) \n",
        "> is computed by multiplying the predicted center-ness with the corresponding classification score. \n",
        "> Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. \n",
        "> As a result, with high probability, these low-quality bounding boxes might be filtered out by \n",
        "> the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n",
        "\n",
        "1. Remove boxes with score smaller than `self.score_thresh`. The score is given by `sqrt`($\\sigma$(`classification_score`) * $\\sigma$(`cente-ness_score`)), where $\\sigma$ stands for the sigmoid function. (1pt.)\n",
        "2. Keep only top `self.topk_candidates` scoring predictions (1pt.)\n",
        "\n",
        "The `compute_loss` function here calculates the indexes of matched classes for each anchor/location for your convenience in later calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "EB17o3zs1JiR"
      },
      "outputs": [],
      "source": [
        "class FCOS(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        num_classes: int,\n",
        "        # transform parameters\n",
        "        image_mean: Optional[List[float]] = None,\n",
        "        image_std: Optional[List[float]] = None,\n",
        "        # Anchor parameters\n",
        "        anchor_generator: AnchorGenerator = None,\n",
        "        center_sampling_radius: float = 1.5,\n",
        "        score_thresh: float = 0.2,\n",
        "        nms_thresh: float = 0.6,\n",
        "        detections_per_img: int = 100,\n",
        "        topk_candidates: int = 1000,\n",
        "        num_convs_in_heads:int = 4,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.anchor_generator = anchor_generator\n",
        "        self.head = FCOSHead(backbone.out_channels, num_classes, num_convs=num_convs_in_heads)\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.transform = GeneralizedRCNNTransform(128, 128, image_mean, image_std, **kwargs)\n",
        "\n",
        "        self.center_sampling_radius = center_sampling_radius\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "        self.topk_candidates = topk_candidates\n",
        "\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],\n",
        "        num_anchors_per_level: List[int],\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        matched_idxs = []\n",
        "        for anchors_per_image, targets_per_image in zip(anchors, targets): # batch\n",
        "            if targets_per_image[\"boxes\"].numel() == 0:\n",
        "                matched_idxs.append(\n",
        "                    torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device)\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            gt_boxes = targets_per_image[\"boxes\"]\n",
        "            print(anchors_per_image)\n",
        "            print(gt_boxes)\n",
        "            gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2                 # Calculate centres of bounding boxes\n",
        "            anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N  \n",
        "            anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]            # Match anchors\n",
        "            # center sampling: anchor point must be close enough to gt center.\n",
        "            pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n",
        "                dim=2\n",
        "            ).values < self.center_sampling_radius * anchor_sizes[:, None]\n",
        "            # compute pairwise distance between N points and M boxes\n",
        "            x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n",
        "            x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
        "            pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n",
        "\n",
        "            # anchor point must be inside gt\n",
        "            pairwise_match &= pairwise_dist.min(dim=2).values > 0\n",
        "\n",
        "            # each anchor is only responsible for certain scale range.\n",
        "            lower_bound = anchor_sizes * 4\n",
        "            lower_bound[: num_anchors_per_level[0]] = 0\n",
        "            upper_bound = anchor_sizes * 8\n",
        "            upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n",
        "            pairwise_dist = pairwise_dist.max(dim=2).values\n",
        "            pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])\n",
        "\n",
        "            # match the GT box with minimum area, if there are multiple GT matches\n",
        "            gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n",
        "            pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n",
        "            min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n",
        "            matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n",
        "\n",
        "            print(matched_idx.shape)\n",
        "            matched_idxs.append(matched_idx)\n",
        "        # end of your code\n",
        "        # matched index - anchor-to-target match\n",
        "        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n",
        "\n",
        "    def postprocess_detections(\n",
        "        self, head_outputs: Dict[str, List[Tensor]], anchors: List[List[Tensor]], image_shapes: List[Tuple[int, int]]\n",
        "    ) -> List[Dict[str, Tensor]]:\n",
        "        class_logits = head_outputs[\"cls_logits\"]\n",
        "        box_regression = head_outputs[\"bbox_regression\"]\n",
        "        box_ctrness = head_outputs[\"bbox_ctrness\"]\n",
        "\n",
        "        \n",
        "        num_images = len(image_shapes)\n",
        "\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "\n",
        "        for index in range(num_images):\n",
        "            box_regression_per_image = [br[index] for br in box_regression]\n",
        "            logits_per_image = [cl[index] for cl in class_logits]\n",
        "            box_ctrness_per_image = [bc[index] for bc in box_ctrness]\n",
        "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
        "\n",
        "            image_boxes = []\n",
        "            image_scores = []\n",
        "            image_labels = []\n",
        "\n",
        "            for box_regression_per_level, logits_per_level, box_ctrness_per_level, anchors_per_level in zip(\n",
        "                box_regression_per_image, logits_per_image, box_ctrness_per_image, anchors_per_image\n",
        "            ):\n",
        "                num_classes = logits_per_level.shape[-1]\n",
        "                \n",
        "                # TODO: your code here\n",
        "                # Remove low scoring boxes and keep only top k scoring predictions\n",
        "                ################################################################################################\n",
        "                max_logits = logits_per_level.max(dim=-1)\n",
        "                scores_per_level = torch.sqrt(\n",
        "                    torch.sigmoid(max_logits.values) * torch.sigmoid(box_ctrness_per_level.squeeze(dim=1)))\n",
        "                \n",
        "                top = scores_per_level > self.score_thresh\n",
        "\n",
        "                box_regression_per_level = box_regression_per_level[top]\n",
        "                anchors_per_level = anchors_per_level[top]\n",
        "                scores_per_level = scores_per_level[top]\n",
        "\n",
        "                if self.topk_candidates >= scores_per_level.shape[0]:\n",
        "                    topk_idxs = torch.sort(scores_per_level, descending=True).indices\n",
        "                else:\n",
        "                    topk_idxs = torch.topk(scores_per_level, self.topk_candidates).indices\n",
        "                \n",
        "\n",
        "                scores_per_level = scores_per_level[topk_idxs]\n",
        "\n",
        "                topk_idxs = max_logits.indices[top][topk_idxs] + topk_idxs * num_classes\n",
        "                ################################################################################################\n",
        "                # end of your code\n",
        "\n",
        "\n",
        "                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n",
        "                labels_per_level = topk_idxs % num_classes\n",
        "\n",
        "                boxes_per_level = self.box_coder.decode_single(\n",
        "                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n",
        "                )\n",
        "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
        "\n",
        "                image_boxes.append(boxes_per_level)\n",
        "                image_scores.append(scores_per_level)\n",
        "                image_labels.append(labels_per_level)\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)\n",
        "            image_scores = torch.cat(image_scores, dim=0)\n",
        "            image_labels = torch.cat(image_labels, dim=0)\n",
        "\n",
        "            # non-maximum suppression\n",
        "            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n",
        "            keep = keep[: self.detections_per_img]\n",
        "\n",
        "            detections.append(\n",
        "                {\n",
        "                    \"boxes\": image_boxes[keep],\n",
        "                    \"scores\": image_scores[keep],\n",
        "                    \"labels\": image_labels[keep],\n",
        "                }\n",
        "            )\n",
        "        return detections\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: List[Tensor],\n",
        "        targets: Optional[List[Dict[str, Tensor]]] = None,\n",
        "    ) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (list[Tensor]): images to be processed\n",
        "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
        "\n",
        "        Returns:\n",
        "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
        "                During training, it returns a dict[Tensor] which contains the losses.\n",
        "                During testing, it returns list[BoxList] contains additional fields\n",
        "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
        "        \"\"\"\n",
        "        \n",
        "        # transform the input (normalise with std and )\n",
        "        images, targets = self.transform(images, targets)\n",
        "\n",
        "        # get the features from the backbone\n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([(\"0\", features)])\n",
        "        features = list(features.values())\n",
        "\n",
        "        # compute the fcos heads outputs using the features\n",
        "        head_outputs = self.head(features)\n",
        "\n",
        "        # create the set of anchors\n",
        "        anchors = self.anchor_generator(images, features)\n",
        "        # recover level sizes\n",
        "        num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
        "        \n",
        "        losses = {}\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "        if self.training:\n",
        "            \n",
        "            losses = self.compute_loss(targets, head_outputs, anchors, num_anchors_per_level)\n",
        "            return losses\n",
        "        else:\n",
        "            # split outputs per level\n",
        "            split_head_outputs: Dict[str, List[Tensor]] = {}\n",
        "            for k in head_outputs:\n",
        "                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
        "            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
        "\n",
        "            # compute the detections\n",
        "            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
        "            return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA1Nvz6jagyP"
      },
      "source": [
        "### Metrics and evaluation (2 pt.)\n",
        "\n",
        "#### Digit Accuracy (1 pt.)\n",
        "\n",
        "This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]`, and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n",
        "\n",
        "- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class and `iou` overlap greater than `0.5`,\n",
        "- the number of `canvas` boxes match `len(predicted_boxes)`.\n",
        "\n",
        "The method shoud output `1` if there is a matching and `0` otherwise.\n",
        "\n",
        "#### Evaluation function (1 pt.)\n",
        "\n",
        "Write an evaluation function for your model.\n",
        "If needed, perform the final NMS with `torchvision.ops.nms` (threshold at `iou`) and remove redundant boxes with scores smaller than `min_score`.\n",
        "Then, calculate the average `DigitAccuracy` for each.\n",
        "You may experiment with parameters for this method, but the default ones are fine (this is not subject of our grading).\n",
        "If you are fine what you achiveded with `postprocess_detections`, you may focus solely on evaluation (although playing with this second stage NMS and score thresholding might be useful for diagnostics).\n",
        "\n",
        "The output of the method is the average digit accuracy on the test set. \n",
        "Use it to track your model performance over epochs.\n",
        "\n",
        "In principle, you can use different `iou` and `min_score`, although itd be slightly preferred to use the defaults here. Itll ease the comparison, we were able to get around 60% in 15 epochs with these values. With that said, if you tune these values for your model to improve the score there will be no point lost.\n",
        "unless you will do some sort of a hack to exploit the metric in a malicious way, but that will perhaps mean that theres something wrong in other parts of the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "bfRZnbAP-XGG"
      },
      "outputs": [],
      "source": [
        "class DigitAccuracy:\n",
        "    def compute_metric  (\n",
        "        self,\n",
        "        predicted_boxes: List[MnistBox],\n",
        "        canvas: MnistCanvas,\n",
        "        iou: float = 0.5,\n",
        "    ):\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        pass\n",
        "        if len(predicted_boxes) != len(canvas.boxes):\n",
        "          return False\n",
        "\n",
        "        for true_box in canvas.boxes:\n",
        "          match_box = False\n",
        "          for predicted_box in predicted_boxes:\n",
        "            if true_box.eq(predicted_box, iou):\n",
        "              if match_box:\n",
        "                return False\n",
        "              match_box = True\n",
        "          if not match_box:\n",
        "            return False\n",
        "            \n",
        "        return True\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "\"\"\"\n",
        "detections.append(\n",
        "                {\n",
        "                    \"boxes\": image_boxes[keep],\n",
        "                    \"scores\": image_scores[keep],\n",
        "                    \"labels\": image_labels[keep],\n",
        "                }\n",
        "            )\n",
        "\"\"\"\n",
        "\n",
        "def evaluate(model: nn.Module, TEST_CANVAS, min_score: float = .2, iou: float = .05) -> float:\n",
        "    # TODO: your code here\n",
        "    ################################################################################################\n",
        "    model_ouputs = model(TEST_CANVAS)\n",
        "    scores = [d.scores for d in model_ouputs]\n",
        "\n",
        "    boxes = [canvas.boxes for canvas in TEST_CANVAS]\n",
        "\n",
        "    ids = torchvision.ops.nms(boxes, scores, iou_threshold=iou)\n",
        "\n",
        "    final_boxes = []\n",
        "\n",
        "    for idx in ids:\n",
        "      if model_ouputs[idx] > min_score:\n",
        "        fin = DigitAccuracy().compute_metric(boxes[idx], TEST_CANVAS[idx])\n",
        "        final_boxes.append(fin)\n",
        "\n",
        "    return final_boxes\n",
        "\n",
        "    ################################################################################################\n",
        "    # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15n5w-hhvRbS"
      },
      "source": [
        "### Train your model (3pt)\n",
        "\n",
        "One should use all classes defined above to train the model.\n",
        "\n",
        "- Train the model. A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data (2 pt.).\n",
        "\n",
        "What does the target variable look like?\n",
        "So, as in the typehint, the target is `List[Dict[str, Tensor]]`. Assuming a 1-element batch, it should then be a 1-element list containing a dictionary with two keys: `boxes` and their corresponding `labels`. The values are tensors. Assuming that we have 5 boxes in GT, the shapes are `(5, 4)` for boxes and (5) for labels. Naturally, you have to fill these tensors using values from `MnistCanvas`.\n",
        "\n",
        "**Hint:** Training can take a while to achieve the expected accuracy. It is normal that for many epochs at the beginning accuracy is constantly $0$. Do not worry as long as the loss is on average decreasing across epochs. You may want to reduce number of digit classes (for example only to generate `0`s on canvas) to test the convergence (the hyperparameters might change, though!). A model with around 500k parameters should be able to hit 10% of the metric in 20 minutes (tested on a 2021 MacBook on CPU). On Google Colab with GPU it will be matter of 2 minutes, but notice that the free GPU is limited.\n",
        "\n",
        "**Hint:** Use the 1-element batches. Some portions of the code are not ready for higher values.\n",
        "\n",
        "**Even more important hint:** Pay attention to the ordering of the X/Y values of `get_torch_tensor` and align it with your target!\n",
        "\n",
        "Good luck and have fun!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "\n",
        "TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n",
        "np.random.seed(TEST_SEED)\n",
        "\n",
        "N_TRAINING_EXAMPLES = 500\n",
        "LR = 0.0004 # for SGD with momentum=0.9\n",
        "EPOCHS = 16\n",
        "STRIDES = [32, 64, 128]\n",
        "CONVS_IN_HEADS = 4\n",
        "OUT_CHANNELS = 64\n",
        "LABELS = list(range(5))  # lower it for quick convergence testing\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "GyWNGqHhTJfa"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_targets(dataset: List[MnistCanvas]):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for input in dataset:\n",
        "    boxes = input.boxes\n",
        "    labels = [str(box.class_nb) for box in boxes]\n",
        "    inputs.append(input.get_torch_tensor())\n",
        "    x_mins = [b.x_min for b in boxes]\n",
        "    y_mins = [b.y_min for b in boxes]\n",
        "    x_maxs = [b.x_max for b in boxes]\n",
        "    y_maxs = [b.y_max for b in boxes]\n",
        "\n",
        "    d = {}\n",
        "    ten = torch.tensor([[x_min, y_min, x_max, y_max] for x_min, y_min, x_max, y_max in zip(x_mins, y_mins, x_maxs, y_maxs)], dtype=torch.float32, device=DEVICE)\n",
        "    d[\"boxes\"] = ten\n",
        "    d[\"labels\"] = labels\n",
        "    targets.append(d)\n",
        "  print(targets)\n",
        "  return inputs, targets\n",
        "\"\"\"\n",
        "  for canvas in dataset:\n",
        "    boxes = canvas.boxes\n",
        "    labels = [str(box.class_nb) for box in boxes]\n",
        "    tensor = canvas.get_torch_tensor()\n",
        "    print(f\"l: {len(labels)}, t: {tensor.shape}\")\n",
        "    targets.append({\n",
        "                \"labels\": labels,\n",
        "                \"boxes\": tensor,\n",
        "            })\n",
        "    \n",
        "    return targets\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "duTtHwPHXVNM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "af0faeb2-4ae2-4822-c3f8-bbea3083b611"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n  for canvas in dataset:\\n    boxes = canvas.boxes\\n    labels = [str(box.class_nb) for box in boxes]\\n    tensor = canvas.get_torch_tensor()\\n    print(f\"l: {len(labels)}, t: {tensor.shape}\")\\n    targets.append({\\n                \"labels\": labels,\\n                \"boxes\": tensor,\\n            })\\n    \\n    return targets\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self, train_canvas_size: int, epochs: int):\n",
        "    self.train_canvas_size = train_canvas_size\n",
        "    self.epochs = epochs\n",
        "    self.digit_accuracy = DigitAccuracy()\n",
        "\n",
        "    self.train_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "    self.train_inputs, self.train_targets = get_targets(self.train_set)\n",
        "\n",
        "    self.test_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "    self.test_inputs, self.test_targets = get_targets(self.test_set)\n",
        "\n",
        "    self.anchor_sizes = tuple([(x,) for x in STRIDES])  # equal to strides of multi-level feature map\n",
        "    self.aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one \"anchor\" per location\n",
        "    self.anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "\n",
        "    self.fcos = FCOS(\n",
        "        backbone = BackboneWithFPN(strides=STRIDES, out_channels=OUT_CHANNELS), \n",
        "        num_classes = len(LABELS), \n",
        "        image_mean = [0.0233],\n",
        "        image_std = [0.14],\n",
        "        num_convs_in_heads = CONVS_IN_HEADS, \n",
        "        anchor_generator = anchor_generator,\n",
        "        detections_per_img = 100\n",
        "        )\n",
        "    self.fcos.to(DEVICE)\n",
        "    self.optimizer = optim.SGD(self.fcos.parameters(), 0.01, momentum=0.9) # change to adagrad\n",
        "\n",
        "# TODO: write your code here\n",
        "################################################################################################\n",
        "  def train(self):\n",
        "    train_loss, test_loss, test_acc = [], [], []\n",
        "    correct, not_correct = 0, 0\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        temp_loss = 0\n",
        "        self.fcos.train()\n",
        "        for i in range(len(self.train_inputs)):\n",
        "          train_input = self.train_inputs[i]\n",
        "          train_target = self.train_targets[i]\n",
        "          print(f'in: {train_input}, tar{type(train_target)}')\n",
        "          model_output = self.fcos.forward(train_input, [train_target])\n",
        "\n",
        "          loss = self.fcos.compute_loss(train_target, model_output, )\n",
        "          \n",
        "          temp_loss += loss\n",
        "\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "        \n",
        "        train_loss.append(temp_loss)\n",
        "\n",
        "        print(f'Epoch: {epoch}, loss: {temp_loss}')\n",
        "\n",
        "################################################################################################"
      ],
      "metadata": {
        "id": "u5kwfx3RTGGq"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot example results of matched and mismatched predictions (0.5 pt.).\n",
        "- Plot particular losses and evaluation score per (0.5 pt.).\n"
      ],
      "metadata": {
        "id": "XmZgfAQbXIxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(100, 20)\n",
        "train_loss, test_loss, test_acc = trainer.train()\n"
      ],
      "metadata": {
        "id": "OVw6fdTG1JPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24d873b5-ad5a-4f18-d041-220345c5f6e8"
      },
      "execution_count": 259,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[ 46., 106.,  65., 115.],\n",
            "        [ 87.,  97.,  97., 105.],\n",
            "        [ 30.,  36.,  49.,  41.],\n",
            "        [101.,  35., 120.,  46.],\n",
            "        [ 23.,  98.,  42., 109.],\n",
            "        [ 70.,  43.,  80.,  46.]], device='cuda:0'), 'labels': ['1', '2', '1', '3', '3', '1']}, {'boxes': tensor([[ 69.,  41.,  88.,  44.],\n",
            "        [ 88.,  46., 104.,  58.],\n",
            "        [ 46.,  59.,  65.,  76.],\n",
            "        [ 73.,  76.,  86.,  86.],\n",
            "        [ 19.,  96.,  33., 110.]], device='cuda:0'), 'labels': ['1', '4', '3', '3', '0']}, {'boxes': tensor([[ 86.,  10., 105.,  27.],\n",
            "        [ 48.,  92.,  63., 103.],\n",
            "        [ 53.,  58.,  72.,  71.],\n",
            "        [ 17.,   2.,  30.,   8.]], device='cuda:0'), 'labels': ['3', '2', '3', '3']}, {'boxes': tensor([[ 48., 101.,  63., 112.],\n",
            "        [ 11.,  10.,  30.,  26.],\n",
            "        [ 68.,  87.,  78.,  92.],\n",
            "        [ 24.,  87.,  43., 100.],\n",
            "        [ 28.,  18.,  47.,  25.]], device='cuda:0'), 'labels': ['4', '4', '1', '0', '1']}, {'boxes': tensor([[ 13.,  60.,  32.,  79.],\n",
            "        [ 47.,   2.,  63.,  16.],\n",
            "        [100.,  59., 119.,  78.]], device='cuda:0'), 'labels': ['4', '4', '2']}, {'boxes': tensor([[ 50.,  10.,  69.,  26.],\n",
            "        [ 33.,  44.,  50.,  59.],\n",
            "        [ 37., 105.,  56., 117.]], device='cuda:0'), 'labels': ['2', '4', '3']}, {'boxes': tensor([[ 86.,   7., 105.,  10.],\n",
            "        [  9., 103.,  28., 120.],\n",
            "        [ 57., 111.,  69., 121.],\n",
            "        [ 88.,  18.,  99.,  27.]], device='cuda:0'), 'labels': ['1', '3', '0', '3']}, {'boxes': tensor([[ 10.,   2.,  29.,  17.],\n",
            "        [ 85.,  28., 104.,  43.],\n",
            "        [ 66.,  49.,  85.,  55.]], device='cuda:0'), 'labels': ['4', '4', '1']}, {'boxes': tensor([[ 26., 100.,  43., 103.],\n",
            "        [ 35.,  41.,  54.,  60.],\n",
            "        [ 52., 102.,  67., 117.],\n",
            "        [ 47.,   6.,  66.,  11.]], device='cuda:0'), 'labels': ['1', '3', '2', '1']}, {'boxes': tensor([[ 70.,  89.,  89.,  94.],\n",
            "        [ 99.,  59., 118.,  72.],\n",
            "        [ 39.,  57.,  57.,  69.],\n",
            "        [ 47.,  76.,  66.,  87.],\n",
            "        [  3.,  16.,  14.,  21.]], device='cuda:0'), 'labels': ['1', '3', '4', '4', '1']}, {'boxes': tensor([[ 52.,  15.,  71.,  25.],\n",
            "        [ 33., 106.,  49., 117.],\n",
            "        [ 77.,  92.,  88., 100.],\n",
            "        [ 55.,  82.,  72., 101.],\n",
            "        [ 47., 105.,  61., 117.],\n",
            "        [ 48.,  53.,  57.,  58.]], device='cuda:0'), 'labels': ['4', '3', '3', '3', '3', '4']}, {'boxes': tensor([[ 95.,  40., 110.,  46.],\n",
            "        [ 71.,  27.,  90.,  32.],\n",
            "        [ 46., 110.,  65., 124.],\n",
            "        [ 16.,  93.,  35., 105.]], device='cuda:0'), 'labels': ['1', '1', '4', '3']}, {'boxes': tensor([[ 83.,  71.,  94.,  88.],\n",
            "        [100.,  60., 114.,  74.],\n",
            "        [ 41.,  74.,  51.,  80.],\n",
            "        [ 11.,  94.,  30., 105.],\n",
            "        [ 23.,   8.,  34.,  10.],\n",
            "        [ 69.,  31.,  88.,  44.]], device='cuda:0'), 'labels': ['0', '0', '1', '1', '1', '4']}, {'boxes': tensor([[101.,  58., 120.,  77.],\n",
            "        [  3.,  35.,  17.,  49.],\n",
            "        [ 12.,  14.,  24.,  22.],\n",
            "        [ 94., 115., 113., 118.],\n",
            "        [ 49., 108.,  63., 119.]], device='cuda:0'), 'labels': ['2', '3', '4', '1', '0']}, {'boxes': tensor([[ 3., 36., 15., 46.],\n",
            "        [21., 53., 40., 71.],\n",
            "        [74.,  2., 93., 13.],\n",
            "        [ 2., 88., 21., 97.],\n",
            "        [37., 53., 56., 56.],\n",
            "        [29.,  6., 43.,  8.]], device='cuda:0'), 'labels': ['3', '0', '0', '1', '1', '1']}, {'boxes': tensor([[ 83.,  23., 102.,  42.],\n",
            "        [ 44., 116.,  61., 124.],\n",
            "        [ 41.,  63.,  60.,  72.],\n",
            "        [  1.,  61.,  20.,  66.],\n",
            "        [106.,  77., 114.,  89.]], device='cuda:0'), 'labels': ['2', '1', '1', '1', '0']}, {'boxes': tensor([[ 15.,  24.,  34.,  41.],\n",
            "        [  1.,  26.,  10.,  37.],\n",
            "        [ 54.,  14.,  73.,  27.],\n",
            "        [ 56.,  88.,  75., 101.]], device='cuda:0'), 'labels': ['4', '2', '3', '3']}, {'boxes': tensor([[ 87.,  24.,  98.,  34.],\n",
            "        [ 31.,  22.,  44.,  32.],\n",
            "        [ 76.,  82.,  94.,  96.],\n",
            "        [ 94.,  84., 113., 101.],\n",
            "        [ 96.,  63., 115.,  78.]], device='cuda:0'), 'labels': ['4', '0', '2', '0', '3']}, {'boxes': tensor([[ 43.,   1.,  62.,  14.],\n",
            "        [ 88.,   2.,  98.,   5.],\n",
            "        [ 20.,  85.,  39., 102.],\n",
            "        [102.,  81., 121.,  93.],\n",
            "        [ 50.,  67.,  69.,  80.]], device='cuda:0'), 'labels': ['0', '1', '2', '2', '3']}, {'boxes': tensor([[ 55.,  26.,  71.,  45.],\n",
            "        [ 15.,   2.,  34.,   7.],\n",
            "        [ 40.,  20.,  52.,  26.],\n",
            "        [ 69.,  31.,  88.,  49.],\n",
            "        [102.,  94., 121., 109.]], device='cuda:0'), 'labels': ['0', '1', '3', '2', '0']}, {'boxes': tensor([[56., 31., 75., 45.],\n",
            "        [25., 58., 44., 67.],\n",
            "        [27.,  4., 43., 20.]], device='cuda:0'), 'labels': ['0', '0', '2']}, {'boxes': tensor([[  2.,  12.,  16.,  25.],\n",
            "        [  2.,  86.,  21.,  91.],\n",
            "        [ 74.,   7.,  93.,  22.],\n",
            "        [ 95.,  78., 114.,  97.]], device='cuda:0'), 'labels': ['0', '1', '0', '2']}, {'boxes': tensor([[ 15.,  74.,  30.,  80.],\n",
            "        [ 82.,  70., 101.,  82.],\n",
            "        [ 26.,  13.,  36.,  19.]], device='cuda:0'), 'labels': ['4', '2', '0']}, {'boxes': tensor([[103., 112., 122., 117.],\n",
            "        [ 82.,   6.,  98.,  22.],\n",
            "        [  9.,  72.,  23.,  84.],\n",
            "        [ 40.,  89.,  59.,  92.],\n",
            "        [ 12.,  30.,  23.,  38.]], device='cuda:0'), 'labels': ['1', '0', '2', '1', '3']}, {'boxes': tensor([[ 18., 109.,  32., 123.],\n",
            "        [ 70., 108.,  86., 123.],\n",
            "        [103.,  30., 122.,  47.],\n",
            "        [103.,  45., 122.,  60.],\n",
            "        [ 88.,  47., 107.,  54.],\n",
            "        [ 49.,  73.,  68.,  87.]], device='cuda:0'), 'labels': ['3', '0', '3', '0', '1', '4']}, {'boxes': tensor([[ 61.,  33.,  75.,  44.],\n",
            "        [105.,  50., 124.,  67.],\n",
            "        [ 76.,  80.,  95.,  85.],\n",
            "        [  7.,  22.,  23.,  36.],\n",
            "        [ 31.,  19.,  50.,  24.],\n",
            "        [ 75.,  67.,  87.,  75.]], device='cuda:0'), 'labels': ['4', '2', '1', '2', '1', '0']}, {'boxes': tensor([[ 58.,  69.,  77.,  87.],\n",
            "        [ 71.,  82.,  83.,  93.],\n",
            "        [105.,  93., 124.,  98.],\n",
            "        [ 32.,  88.,  45.,  97.]], device='cuda:0'), 'labels': ['0', '0', '1', '0']}, {'boxes': tensor([[ 56.,  41.,  71.,  54.],\n",
            "        [ 94.,  54., 113.,  70.],\n",
            "        [105.,  78., 124.,  97.]], device='cuda:0'), 'labels': ['4', '2', '0']}, {'boxes': tensor([[ 12.,  99.,  31., 112.],\n",
            "        [ 27., 113.,  36., 117.],\n",
            "        [  3.,  29.,  12.,  38.],\n",
            "        [ 82.,  18., 101.,  27.],\n",
            "        [ 86.,  96., 105., 113.],\n",
            "        [101.,   7., 120.,  12.]], device='cuda:0'), 'labels': ['2', '1', '0', '4', '4', '1']}, {'boxes': tensor([[ 33.,  24.,  52.,  43.],\n",
            "        [ 44.,  61.,  63.,  78.],\n",
            "        [101.,  18., 120.,  35.],\n",
            "        [ 85.,  69.,  97.,  83.],\n",
            "        [ 63.,   0.,  81.,  14.]], device='cuda:0'), 'labels': ['2', '2', '2', '2', '0']}, {'boxes': tensor([[ 50.,  36.,  60.,  38.],\n",
            "        [ 81.,  52., 100.,  64.],\n",
            "        [ 47., 109.,  66., 124.],\n",
            "        [ 20.,  70.,  39.,  89.],\n",
            "        [ 10.,  12.,  29.,  25.]], device='cuda:0'), 'labels': ['1', '2', '2', '3', '3']}, {'boxes': tensor([[55., 66., 74., 83.],\n",
            "        [17., 48., 36., 64.],\n",
            "        [ 7., 82., 18., 93.],\n",
            "        [71., 80., 81., 82.]], device='cuda:0'), 'labels': ['2', '4', '2', '1']}, {'boxes': tensor([[ 85.,  23., 100.,  42.],\n",
            "        [ 81.,  17.,  92.,  28.],\n",
            "        [ 42.,  25.,  56.,  28.],\n",
            "        [ 16.,  24.,  33.,  35.],\n",
            "        [ 25.,  95.,  34., 102.]], device='cuda:0'), 'labels': ['0', '0', '1', '3', '3']}, {'boxes': tensor([[ 44.,  93.,  54., 100.],\n",
            "        [ 67.,  73.,  84.,  77.],\n",
            "        [  0., 106.,  12., 111.],\n",
            "        [ 69.,  48.,  88.,  67.]], device='cuda:0'), 'labels': ['2', '1', '4', '2']}, {'boxes': tensor([[ 71.,   5.,  86.,  13.],\n",
            "        [ 87.,  50., 103.,  63.],\n",
            "        [ 30.,  73.,  48.,  89.]], device='cuda:0'), 'labels': ['4', '2', '0']}, {'boxes': tensor([[50.,  9., 64., 12.],\n",
            "        [21., 76., 33., 85.],\n",
            "        [53., 45., 72., 59.]], device='cuda:0'), 'labels': ['1', '0', '3']}, {'boxes': tensor([[ 61., 104.,  70., 114.],\n",
            "        [ 82.,  68.,  96.,  82.],\n",
            "        [  0.,   6.,  19.,  21.],\n",
            "        [111.,  18., 124.,  30.],\n",
            "        [ 50.,  51.,  60.,  56.],\n",
            "        [ 53.,  61.,  65.,  70.]], device='cuda:0'), 'labels': ['2', '2', '4', '2', '3', '0']}, {'boxes': tensor([[ 22.,  72.,  36.,  77.],\n",
            "        [ 76.,   8.,  95.,  11.],\n",
            "        [ 85.,  90., 104.,  96.],\n",
            "        [104.,  39., 123.,  54.],\n",
            "        [ 27.,  38.,  46.,  49.],\n",
            "        [ 48.,  85.,  67., 102.]], device='cuda:0'), 'labels': ['1', '1', '1', '3', '4', '3']}, {'boxes': tensor([[ 39.,  94.,  58., 106.],\n",
            "        [ 60.,  70.,  73.,  89.],\n",
            "        [ 96.,   2., 112.,  16.],\n",
            "        [ 55.,  47.,  74.,  58.],\n",
            "        [  9.,  94.,  19., 104.]], device='cuda:0'), 'labels': ['4', '4', '3', '4', '4']}, {'boxes': tensor([[ 46., 110.,  55., 116.],\n",
            "        [ 52., 117.,  61., 124.],\n",
            "        [107.,  14., 116.,  19.]], device='cuda:0'), 'labels': ['3', '3', '4']}, {'boxes': tensor([[ 86.,  67.,  95.,  72.],\n",
            "        [ 79.,  81.,  94.,  96.],\n",
            "        [ 83.,  25., 102.,  41.],\n",
            "        [ 83.,   4., 102.,  16.]], device='cuda:0'), 'labels': ['1', '0', '2', '3']}, {'boxes': tensor([[ 82.,  33.,  93.,  38.],\n",
            "        [ 96.,  93., 107., 104.],\n",
            "        [ 60.,   6.,  79.,  23.],\n",
            "        [ 48.,  47.,  67.,  64.]], device='cuda:0'), 'labels': ['1', '2', '4', '4']}, {'boxes': tensor([[105., 118., 124., 123.],\n",
            "        [ 25.,  38.,  44.,  45.],\n",
            "        [ 97.,  12., 116.,  25.],\n",
            "        [ 31., 102.,  44., 108.],\n",
            "        [ 78.,  14.,  89.,  22.],\n",
            "        [ 67.,  73.,  79.,  87.]], device='cuda:0'), 'labels': ['1', '4', '4', '1', '3', '0']}, {'boxes': tensor([[ 62.,  63.,  72.,  70.],\n",
            "        [ 58.,  33.,  77.,  48.],\n",
            "        [ 28.,  47.,  47.,  64.],\n",
            "        [ 28.,  92.,  40., 100.],\n",
            "        [  5.,  37.,  18.,  50.]], device='cuda:0'), 'labels': ['4', '0', '2', '1', '2']}, {'boxes': tensor([[36., 39., 48., 44.],\n",
            "        [47., 78., 66., 90.],\n",
            "        [22., 28., 37., 32.]], device='cuda:0'), 'labels': ['1', '0', '1']}, {'boxes': tensor([[47., 67., 66., 72.],\n",
            "        [62., 91., 71., 97.],\n",
            "        [ 3., 35., 20., 47.],\n",
            "        [ 1., 78., 20., 95.],\n",
            "        [37., 13., 53., 25.]], device='cuda:0'), 'labels': ['1', '4', '3', '2', '4']}, {'boxes': tensor([[ 61.,  22.,  80.,  25.],\n",
            "        [ 90.,  84., 108., 100.],\n",
            "        [ 12., 109.,  31., 124.],\n",
            "        [ 31.,  79.,  45.,  90.],\n",
            "        [ 31.,  27.,  48.,  31.],\n",
            "        [ 73., 110.,  92., 121.]], device='cuda:0'), 'labels': ['1', '2', '4', '2', '1', '3']}, {'boxes': tensor([[  2.,  33.,  16.,  42.],\n",
            "        [ 41.,  46.,  54.,  53.],\n",
            "        [ 25.,  59.,  32.,  69.],\n",
            "        [ 80.,  74.,  99.,  91.],\n",
            "        [104.,   1., 123.,  11.]], device='cuda:0'), 'labels': ['4', '3', '0', '3', '1']}, {'boxes': tensor([[103., 112., 122., 121.],\n",
            "        [ 45.,  45.,  57.,  55.],\n",
            "        [ 16., 111.,  26., 119.],\n",
            "        [ 41.,  30.,  50.,  39.],\n",
            "        [  8.,  18.,  27.,  23.],\n",
            "        [ 23.,  82.,  34.,  91.]], device='cuda:0'), 'labels': ['4', '4', '3', '3', '1', '2']}, {'boxes': tensor([[ 97.,  34., 113.,  42.],\n",
            "        [101.,  83., 111.,  91.],\n",
            "        [ 40.,  15.,  55.,  29.]], device='cuda:0'), 'labels': ['4', '3', '4']}, {'boxes': tensor([[ 10.,  76.,  27.,  95.],\n",
            "        [ 58.,   7.,  77.,  24.],\n",
            "        [ 20.,   5.,  30.,   8.],\n",
            "        [ 90.,  53., 101.,  62.],\n",
            "        [ 52.,  92.,  67.,  99.],\n",
            "        [104.,  68., 123.,  81.]], device='cuda:0'), 'labels': ['4', '2', '1', '0', '1', '0']}, {'boxes': tensor([[ 62., 109.,  81., 120.],\n",
            "        [ 79.,   0.,  92.,   9.],\n",
            "        [ 22.,  11.,  39.,  30.]], device='cuda:0'), 'labels': ['2', '3', '2']}, {'boxes': tensor([[101., 121., 110., 124.],\n",
            "        [ 17., 100.,  36., 118.],\n",
            "        [ 73.,  25.,  92.,  40.],\n",
            "        [ 51.,  83.,  70.,  94.],\n",
            "        [ 50., 100.,  63., 112.],\n",
            "        [ 14.,  77.,  33.,  87.]], device='cuda:0'), 'labels': ['1', '3', '3', '4', '3', '1']}, {'boxes': tensor([[  3.,  32.,  22.,  47.],\n",
            "        [106.,  97., 116.,  99.],\n",
            "        [ 22.,  31.,  41.,  44.]], device='cuda:0'), 'labels': ['4', '1', '3']}, {'boxes': tensor([[61., 87., 72., 95.],\n",
            "        [69., 41., 79., 50.],\n",
            "        [42., 18., 53., 31.],\n",
            "        [ 7., 33., 26., 48.]], device='cuda:0'), 'labels': ['3', '3', '2', '2']}, {'boxes': tensor([[ 9., 97., 23., 99.],\n",
            "        [72., 59., 91., 76.],\n",
            "        [78., 54., 88., 60.]], device='cuda:0'), 'labels': ['1', '2', '0']}, {'boxes': tensor([[ 40.,   8.,  57.,  25.],\n",
            "        [ 15.,   6.,  25.,   8.],\n",
            "        [ 92.,  14., 111.,  31.],\n",
            "        [ 38.,  26.,  57.,  31.],\n",
            "        [ 65.,  89.,  84., 100.],\n",
            "        [ 73.,  45.,  92.,  58.]], device='cuda:0'), 'labels': ['2', '1', '4', '1', '2', '0']}, {'boxes': tensor([[ 15.,  50.,  34.,  69.],\n",
            "        [ 90.,  50., 105.,  60.],\n",
            "        [ 98.,  30., 117.,  47.],\n",
            "        [ 39.,  20.,  56.,  39.],\n",
            "        [ 61.,  85.,  75.,  96.]], device='cuda:0'), 'labels': ['0', '4', '3', '0', '4']}, {'boxes': tensor([[ 73.,  44.,  92.,  53.],\n",
            "        [109., 102., 119., 112.],\n",
            "        [ 69.,  64.,  83.,  78.]], device='cuda:0'), 'labels': ['1', '3', '0']}, {'boxes': tensor([[ 87.,  23., 106.,  42.],\n",
            "        [  3.,  89.,  21.,  91.],\n",
            "        [ 31.,   9.,  50.,  21.],\n",
            "        [ 71.,  10.,  89.,  29.],\n",
            "        [100.,   0., 113.,  12.]], device='cuda:0'), 'labels': ['0', '1', '3', '2', '2']}, {'boxes': tensor([[ 18.,  92.,  37., 105.],\n",
            "        [ 69.,  31.,  88.,  36.],\n",
            "        [ 88.,  97., 107., 114.],\n",
            "        [  2.,  28.,  17.,  42.],\n",
            "        [ 94.,  44., 111.,  54.],\n",
            "        [104.,  74., 123.,  89.]], device='cuda:0'), 'labels': ['4', '1', '4', '3', '3', '4']}, {'boxes': tensor([[ 70.,  41.,  89.,  56.],\n",
            "        [ 73.,  19.,  87.,  22.],\n",
            "        [ 11., 100.,  30., 115.],\n",
            "        [ 54.,  38.,  73.,  51.]], device='cuda:0'), 'labels': ['4', '1', '2', '0']}, {'boxes': tensor([[ 75.,  68.,  91.,  80.],\n",
            "        [ 84.,  36.,  98.,  39.],\n",
            "        [ 86., 101.,  96., 104.]], device='cuda:0'), 'labels': ['3', '1', '1']}, {'boxes': tensor([[ 11., 113.,  30., 124.],\n",
            "        [ 20.,  95.,  37., 109.],\n",
            "        [ 95.,  71., 114.,  82.],\n",
            "        [  8.,  18.,  27.,  31.],\n",
            "        [  8.,  35.,  25.,  54.],\n",
            "        [ 46.,  82.,  53.,  92.]], device='cuda:0'), 'labels': ['3', '4', '4', '4', '2', '4']}, {'boxes': tensor([[  8.,  34.,  22.,  45.],\n",
            "        [112.,  52., 123.,  61.],\n",
            "        [ 34.,  81.,  49.,  91.],\n",
            "        [ 39.,  31.,  51.,  39.],\n",
            "        [ 15.,   2.,  29.,  11.],\n",
            "        [ 89.,  70., 106.,  80.]], device='cuda:0'), 'labels': ['2', '4', '4', '4', '4', '3']}, {'boxes': tensor([[106.,  80., 121.,  90.],\n",
            "        [ 37.,  57.,  49.,  69.],\n",
            "        [ 58.,  43.,  77.,  46.]], device='cuda:0'), 'labels': ['4', '2', '1']}, {'boxes': tensor([[ 52.,  20.,  68.,  39.],\n",
            "        [ 45.,   2.,  64.,  13.],\n",
            "        [102.,  50., 120.,  54.],\n",
            "        [ 61.,  99.,  74., 108.]], device='cuda:0'), 'labels': ['2', '4', '1', '3']}, {'boxes': tensor([[ 39.,  32.,  58.,  51.],\n",
            "        [ 18.,  40.,  31.,  52.],\n",
            "        [ 71., 109.,  85., 117.],\n",
            "        [ 94.,  81., 106.,  91.],\n",
            "        [  8.,  20.,  27.,  37.],\n",
            "        [ 13.,  74.,  32.,  89.]], device='cuda:0'), 'labels': ['2', '2', '0', '2', '2', '4']}, {'boxes': tensor([[ 24.,  45.,  43.,  57.],\n",
            "        [ 63.,   1.,  82.,  12.],\n",
            "        [ 47.,  68.,  65.,  81.],\n",
            "        [ 24.,  76.,  43.,  87.],\n",
            "        [ 22., 102.,  39., 112.]], device='cuda:0'), 'labels': ['0', '4', '0', '3', '4']}, {'boxes': tensor([[36., 74., 49., 85.],\n",
            "        [21.,  9., 40., 28.],\n",
            "        [69., 46., 83., 49.]], device='cuda:0'), 'labels': ['4', '3', '1']}, {'boxes': tensor([[  3.,   3.,  22.,  22.],\n",
            "        [ 83.,  33., 102.,  52.],\n",
            "        [ 18.,  72.,  29.,  78.],\n",
            "        [ 80.,  72.,  99.,  81.]], device='cuda:0'), 'labels': ['2', '2', '4', '3']}, {'boxes': tensor([[ 87.,   7.,  99.,  16.],\n",
            "        [ 93.,  39., 112.,  52.],\n",
            "        [ 97.,  71., 116.,  88.],\n",
            "        [ 73.,  66.,  92.,  75.]], device='cuda:0'), 'labels': ['4', '3', '0', '4']}, {'boxes': tensor([[ 17.,  12.,  27.,  19.],\n",
            "        [ 16.,  87.,  35., 106.],\n",
            "        [ 50.,  45.,  69.,  62.],\n",
            "        [ 18.,  65.,  27.,  68.]], device='cuda:0'), 'labels': ['2', '2', '4', '1']}, {'boxes': tensor([[26., 95., 45., 99.],\n",
            "        [53., 18., 71., 32.],\n",
            "        [79., 29., 98., 40.],\n",
            "        [66.,  2., 79., 15.],\n",
            "        [47., 64., 66., 75.]], device='cuda:0'), 'labels': ['1', '4', '3', '2', '4']}, {'boxes': tensor([[106.,  86., 123., 104.],\n",
            "        [ 28.,  37.,  47.,  54.],\n",
            "        [ 15.,   1.,  34.,  16.]], device='cuda:0'), 'labels': ['2', '2', '3']}, {'boxes': tensor([[ 97.,  91., 114., 110.],\n",
            "        [ 86.,  99.,  94., 108.],\n",
            "        [ 68.,  70.,  87.,  83.],\n",
            "        [ 26.,  78.,  42.,  83.]], device='cuda:0'), 'labels': ['4', '4', '4', '3']}, {'boxes': tensor([[64.,  2., 80., 14.],\n",
            "        [14.,  6., 33., 21.],\n",
            "        [63., 91., 82., 94.]], device='cuda:0'), 'labels': ['1', '1', '1']}, {'boxes': tensor([[ 45.,   4.,  64.,  11.],\n",
            "        [ 75.,  73.,  94.,  83.],\n",
            "        [ 94.,  54., 111.,  73.],\n",
            "        [ 13.,  76.,  32.,  95.]], device='cuda:0'), 'labels': ['1', '4', '2', '3']}, {'boxes': tensor([[ 48.,  45.,  67.,  62.],\n",
            "        [ 40.,  61.,  56.,  72.],\n",
            "        [ 75.,  83.,  92., 102.],\n",
            "        [ 34.,  73.,  53.,  78.],\n",
            "        [ 42.,  84.,  61.,  87.]], device='cuda:0'), 'labels': ['2', '4', '3', '1', '1']}, {'boxes': tensor([[ 63.,  94.,  80., 106.],\n",
            "        [ 12.,  77.,  21.,  86.],\n",
            "        [ 71.,  54.,  87.,  63.],\n",
            "        [ 24.,  66.,  39.,  70.],\n",
            "        [ 48.,  62.,  59.,  71.],\n",
            "        [ 31.,  50.,  50.,  53.]], device='cuda:0'), 'labels': ['0', '2', '3', '1', '2', '1']}, {'boxes': tensor([[ 39.,  54.,  58.,  67.],\n",
            "        [ 44.,  36.,  63.,  47.],\n",
            "        [ 54.,  87.,  73.,  90.],\n",
            "        [100.,  65., 112.,  77.],\n",
            "        [ 59.,  13.,  78.,  30.],\n",
            "        [ 65.,  35.,  80.,  40.]], device='cuda:0'), 'labels': ['3', '3', '1', '2', '0', '1']}, {'boxes': tensor([[110.,  66., 124.,  75.],\n",
            "        [  9.,  97.,  28., 111.],\n",
            "        [ 42., 107.,  61., 118.],\n",
            "        [ 46.,  79.,  65.,  96.],\n",
            "        [ 32.,  24.,  51.,  38.],\n",
            "        [ 50.,  17.,  62.,  24.]], device='cuda:0'), 'labels': ['3', '0', '2', '0', '0', '4']}, {'boxes': tensor([[ 56.,  34.,  73.,  45.],\n",
            "        [ 92.,  56., 103.,  65.],\n",
            "        [106.,  88., 120.,  97.]], device='cuda:0'), 'labels': ['2', '0', '3']}, {'boxes': tensor([[ 39.,   5.,  48.,  15.],\n",
            "        [ 72.,  97.,  90., 116.],\n",
            "        [ 75.,  85.,  88.,  92.]], device='cuda:0'), 'labels': ['0', '2', '2']}, {'boxes': tensor([[ 20.,  93.,  32., 102.],\n",
            "        [ 90.,  46., 101.,  49.],\n",
            "        [ 31.,  12.,  48.,  29.]], device='cuda:0'), 'labels': ['4', '1', '2']}, {'boxes': tensor([[ 53.,  28.,  64.,  36.],\n",
            "        [ 91., 106., 110., 121.],\n",
            "        [  3.,  29.,  18.,  42.]], device='cuda:0'), 'labels': ['4', '2', '0']}, {'boxes': tensor([[ 82.,  20.,  94.,  32.],\n",
            "        [ 70.,   4.,  89.,  17.],\n",
            "        [  4., 100.,  23., 117.],\n",
            "        [  3.,   5.,  15.,  13.],\n",
            "        [ 36.,  52.,  48.,  59.],\n",
            "        [101.,  40., 120.,  57.]], device='cuda:0'), 'labels': ['2', '0', '4', '2', '0', '4']}, {'boxes': tensor([[106.,  68., 121.,  76.],\n",
            "        [ 33.,   1.,  52.,  16.],\n",
            "        [ 12.,   3.,  31.,  20.],\n",
            "        [ 30.,  15.,  49.,  32.],\n",
            "        [ 48.,  53.,  58.,  55.]], device='cuda:0'), 'labels': ['3', '4', '2', '0', '1']}, {'boxes': tensor([[104.,  34., 116.,  45.],\n",
            "        [  8.,  82.,  27.,  97.],\n",
            "        [ 84.,  51., 100.,  66.],\n",
            "        [ 65.,  89.,  84., 102.],\n",
            "        [ 88.,  27., 105.,  29.]], device='cuda:0'), 'labels': ['3', '3', '3', '2', '1']}, {'boxes': tensor([[ 16.,  30.,  35.,  45.],\n",
            "        [ 99.,  56., 109.,  63.],\n",
            "        [ 30.,  92.,  44., 108.]], device='cuda:0'), 'labels': ['0', '2', '0']}, {'boxes': tensor([[ 20.,  52.,  30.,  63.],\n",
            "        [104.,  96., 117.,  99.],\n",
            "        [ 42.,  49.,  58.,  58.],\n",
            "        [ 34.,  73.,  53.,  92.],\n",
            "        [  6.,  38.,  24.,  57.]], device='cuda:0'), 'labels': ['2', '1', '4', '2', '2']}, {'boxes': tensor([[ 89.,  28., 103.,  37.],\n",
            "        [ 37.,  84.,  51.,  92.],\n",
            "        [ 90.,  52., 109.,  65.],\n",
            "        [ 81.,  76., 100.,  93.],\n",
            "        [ 14.,  33.,  33.,  48.],\n",
            "        [ 75.,  15.,  94.,  31.]], device='cuda:0'), 'labels': ['4', '4', '3', '2', '0', '0']}, {'boxes': tensor([[ 46.,  51.,  65.,  64.],\n",
            "        [ 79.,  76.,  98.,  95.],\n",
            "        [ 12., 106.,  31., 111.],\n",
            "        [ 72., 109.,  82., 116.],\n",
            "        [ 17., 113.,  36., 124.],\n",
            "        [ 23.,  46.,  41.,  62.]], device='cuda:0'), 'labels': ['4', '0', '1', '4', '4', '2']}, {'boxes': tensor([[ 98.,  51., 117.,  69.],\n",
            "        [ 91.,  94., 100.,  96.],\n",
            "        [ 64.,  47.,  76.,  59.],\n",
            "        [ 81.,  42., 100.,  45.],\n",
            "        [ 12.,  16.,  31.,  27.]], device='cuda:0'), 'labels': ['3', '1', '2', '1', '4']}, {'boxes': tensor([[ 8., 69., 27., 84.],\n",
            "        [43.,  8., 62., 27.],\n",
            "        [29., 76., 48., 89.],\n",
            "        [65., 43., 84., 56.],\n",
            "        [52., 39., 64., 45.],\n",
            "        [78., 33., 92., 37.]], device='cuda:0'), 'labels': ['2', '2', '2', '3', '4', '1']}, {'boxes': tensor([[ 35., 109.,  46., 116.],\n",
            "        [ 24.,   0.,  43.,  17.],\n",
            "        [  5.,   9.,  23.,  23.]], device='cuda:0'), 'labels': ['4', '3', '2']}, {'boxes': tensor([[ 41.,  93.,  60., 108.],\n",
            "        [  9.,   7.,  26.,  17.],\n",
            "        [ 62.,  62.,  81.,  77.],\n",
            "        [ 58.,  18.,  77.,  31.]], device='cuda:0'), 'labels': ['4', '3', '2', '2']}, {'boxes': tensor([[56., 56., 71., 61.],\n",
            "        [58.,  3., 76.,  9.],\n",
            "        [47., 64., 66., 79.],\n",
            "        [27.,  1., 46., 10.]], device='cuda:0'), 'labels': ['1', '1', '0', '3']}, {'boxes': tensor([[ 71.,  29.,  90.,  43.],\n",
            "        [ 89., 104.,  99., 117.],\n",
            "        [ 92.,  92., 102., 101.],\n",
            "        [ 35.,  79.,  45.,  80.]], device='cuda:0'), 'labels': ['3', '3', '2', '1']}, {'boxes': tensor([[ 82.,  90., 101., 102.],\n",
            "        [ 23.,  21.,  37.,  32.],\n",
            "        [ 85.,  46.,  95.,  49.],\n",
            "        [ 17.,  72.,  34.,  83.],\n",
            "        [ 86.,   2., 105.,  19.],\n",
            "        [ 13.,  35.,  32.,  54.]], device='cuda:0'), 'labels': ['0', '2', '1', '3', '2', '4']}]\n",
            "[{'boxes': tensor([[ 79.,  86.,  98., 101.],\n",
            "        [ 21.,  70.,  36.,  74.],\n",
            "        [ 38.,  11.,  57.,  30.]], device='cuda:0'), 'labels': ['0', '1', '2']}, {'boxes': tensor([[  5., 101.,  17., 106.],\n",
            "        [ 74.,   7.,  90.,  23.],\n",
            "        [ 40.,   4.,  55.,  19.],\n",
            "        [ 37.,  96.,  56., 112.]], device='cuda:0'), 'labels': ['1', '2', '3', '0']}, {'boxes': tensor([[42., 31., 61., 44.],\n",
            "        [34., 50., 44., 58.],\n",
            "        [15., 38., 26., 45.]], device='cuda:0'), 'labels': ['3', '2', '4']}, {'boxes': tensor([[  0.,  24.,  19.,  41.],\n",
            "        [ 28.,  39.,  47.,  51.],\n",
            "        [104.,  91., 120.,  99.],\n",
            "        [  5.,  69.,  18.,  78.]], device='cuda:0'), 'labels': ['3', '4', '4', '4']}, {'boxes': tensor([[ 6.,  5., 16.,  9.],\n",
            "        [50., 10., 69., 23.],\n",
            "        [13., 33., 23., 40.],\n",
            "        [33., 39., 43., 48.]], device='cuda:0'), 'labels': ['1', '4', '0', '4']}, {'boxes': tensor([[ 47.,  32.,  59.,  36.],\n",
            "        [ 15.,  43.,  34.,  57.],\n",
            "        [105.,  99., 124., 118.],\n",
            "        [ 32., 108.,  51., 121.],\n",
            "        [ 34.,  32.,  53.,  51.],\n",
            "        [ 38.,  10.,  51.,  20.]], device='cuda:0'), 'labels': ['1', '4', '3', '3', '3', '4']}, {'boxes': tensor([[109.,  25., 117.,  38.],\n",
            "        [ 99.,  72., 113.,  74.],\n",
            "        [ 77.,  45.,  86.,  54.],\n",
            "        [ 65.,  80.,  80.,  88.],\n",
            "        [  1.,   0.,  20.,   7.],\n",
            "        [ 11.,  26.,  25.,  36.]], device='cuda:0'), 'labels': ['0', '1', '2', '3', '1', '3']}, {'boxes': tensor([[ 51.,  82.,  70.,  97.],\n",
            "        [ 95.,  22., 114.,  31.],\n",
            "        [ 34.,  85.,  50.,  87.],\n",
            "        [ 94.,   1., 113.,  17.],\n",
            "        [ 30.,   5.,  44.,  17.]], device='cuda:0'), 'labels': ['2', '1', '1', '4', '0']}, {'boxes': tensor([[ 17.,  62.,  28.,  74.],\n",
            "        [  0.,  10.,  11.,  18.],\n",
            "        [ 87., 103., 106., 120.],\n",
            "        [ 44.,  82.,  59.,  95.],\n",
            "        [  2.,  97.,  14., 105.]], device='cuda:0'), 'labels': ['2', '4', '0', '2', '3']}, {'boxes': tensor([[ 72.,  34.,  81.,  38.],\n",
            "        [ 72.,  73.,  91.,  88.],\n",
            "        [ 51.,  11.,  62.,  24.],\n",
            "        [ 92.,  88., 111., 104.]], device='cuda:0'), 'labels': ['1', '0', '2', '2']}, {'boxes': tensor([[ 95., 101., 105., 105.],\n",
            "        [ 57., 104.,  70., 108.],\n",
            "        [ 67.,  46.,  79.,  60.],\n",
            "        [ 11.,  17.,  22.,  25.]], device='cuda:0'), 'labels': ['1', '1', '0', '2']}, {'boxes': tensor([[ 57.,  71.,  74.,  79.],\n",
            "        [ 74.,  17.,  93.,  30.],\n",
            "        [ 83.,  45., 102.,  59.],\n",
            "        [ 73., 101.,  92., 109.],\n",
            "        [ 99.,  36., 118.,  50.],\n",
            "        [  8.,  89.,  22.,  94.]], device='cuda:0'), 'labels': ['0', '0', '0', '4', '1', '1']}, {'boxes': tensor([[ 66.,  17.,  75.,  31.],\n",
            "        [ 31., 114.,  44., 124.],\n",
            "        [  6.,  54.,  25.,  58.],\n",
            "        [ 47.,   4.,  66.,  22.],\n",
            "        [ 24.,  51.,  39.,  61.],\n",
            "        [ 49., 102.,  65., 106.]], device='cuda:0'), 'labels': ['4', '2', '1', '2', '3', '1']}, {'boxes': tensor([[ 82.,  22.,  91.,  28.],\n",
            "        [ 69.,  99.,  86., 110.],\n",
            "        [ 90.,  14., 108.,  20.]], device='cuda:0'), 'labels': ['3', '4', '1']}, {'boxes': tensor([[ 32.,  61.,  51.,  76.],\n",
            "        [ 48.,  54.,  67.,  73.],\n",
            "        [ 17.,  75.,  32.,  88.],\n",
            "        [ 45., 117.,  55., 121.]], device='cuda:0'), 'labels': ['3', '0', '2', '1']}, {'boxes': tensor([[101.,  61., 115.,  75.],\n",
            "        [ 64., 109.,  83., 124.],\n",
            "        [ 26.,  99.,  45., 104.],\n",
            "        [ 42.,  18.,  52.,  27.]], device='cuda:0'), 'labels': ['0', '3', '1', '2']}, {'boxes': tensor([[17., 38., 34., 57.],\n",
            "        [ 1., 50., 20., 63.],\n",
            "        [46., 18., 65., 31.],\n",
            "        [56., 80., 72., 91.]], device='cuda:0'), 'labels': ['4', '3', '2', '3']}, {'boxes': tensor([[ 32., 104.,  49., 116.],\n",
            "        [ 24.,  84.,  43.,  87.],\n",
            "        [ 96.,  79., 115.,  96.],\n",
            "        [104.,  47., 123.,  57.],\n",
            "        [ 56.,  28.,  73.,  34.]], device='cuda:0'), 'labels': ['4', '1', '4', '1', '1']}, {'boxes': tensor([[ 80.,  68.,  97.,  84.],\n",
            "        [ 23.,  28.,  42.,  33.],\n",
            "        [ 96.,  35., 110.,  43.],\n",
            "        [100.,  54., 115.,  68.],\n",
            "        [ 78.,  94.,  93.,  98.]], device='cuda:0'), 'labels': ['3', '1', '0', '2', '1']}, {'boxes': tensor([[ 50., 100.,  66., 111.],\n",
            "        [ 58.,  46.,  75.,  61.],\n",
            "        [  2.,  87.,  13.,  95.],\n",
            "        [ 50.,  63.,  67.,  76.],\n",
            "        [ 73., 113.,  91., 119.],\n",
            "        [ 50.,  28.,  68.,  40.]], device='cuda:0'), 'labels': ['3', '4', '3', '2', '1', '4']}, {'boxes': tensor([[ 59.,  97.,  78., 113.],\n",
            "        [ 26.,  49.,  45.,  63.],\n",
            "        [109.,  46., 119.,  55.],\n",
            "        [ 15.,  85.,  34.,  98.]], device='cuda:0'), 'labels': ['2', '4', '2', '0']}, {'boxes': tensor([[ 76.,  72.,  95.,  86.],\n",
            "        [ 57.,  67.,  76.,  76.],\n",
            "        [  2.,  34.,  12.,  39.],\n",
            "        [ 69.,   2.,  82.,  12.],\n",
            "        [ 15.,  86.,  29., 102.]], device='cuda:0'), 'labels': ['0', '3', '1', '4', '0']}, {'boxes': tensor([[ 11.,  79.,  30.,  96.],\n",
            "        [ 80.,  72.,  99.,  80.],\n",
            "        [ 86.,  43., 105.,  54.]], device='cuda:0'), 'labels': ['0', '1', '4']}, {'boxes': tensor([[ 58.,  78.,  77.,  85.],\n",
            "        [ 55.,  37.,  70.,  51.],\n",
            "        [ 84.,   4., 103.,  21.]], device='cuda:0'), 'labels': ['1', '4', '0']}, {'boxes': tensor([[ 35.,  31.,  54.,  46.],\n",
            "        [ 10., 117.,  20., 119.],\n",
            "        [ 63.,  40.,  76.,  43.],\n",
            "        [ 87.,  55., 103.,  61.]], device='cuda:0'), 'labels': ['4', '1', '1', '3']}, {'boxes': tensor([[ 32.,  46.,  43.,  56.],\n",
            "        [ 76.,  46.,  95.,  49.],\n",
            "        [ 90.,   1., 106.,  13.],\n",
            "        [ 70.,  14.,  80.,  19.],\n",
            "        [ 86.,  85., 100.,  94.]], device='cuda:0'), 'labels': ['4', '1', '0', '1', '0']}, {'boxes': tensor([[ 96.,  42., 115.,  47.],\n",
            "        [  6.,  88.,  25., 100.],\n",
            "        [ 25.,  41.,  42.,  49.]], device='cuda:0'), 'labels': ['1', '0', '1']}, {'boxes': tensor([[ 78.,  91.,  97., 104.],\n",
            "        [  0.,  41.,  19.,  54.],\n",
            "        [ 20.,  10.,  31.,  11.]], device='cuda:0'), 'labels': ['3', '4', '1']}, {'boxes': tensor([[102.,  99., 115., 103.],\n",
            "        [ 90.,  18.,  99.,  20.],\n",
            "        [ 21.,   4.,  35.,   7.],\n",
            "        [ 15.,  38.,  32.,  47.],\n",
            "        [ 54., 111.,  73., 118.],\n",
            "        [104.,  19., 123.,  36.]], device='cuda:0'), 'labels': ['1', '1', '1', '3', '1', '4']}, {'boxes': tensor([[ 97.,  63., 109.,  76.],\n",
            "        [ 40.,  45.,  50.,  55.],\n",
            "        [ 26.,  14.,  45.,  28.]], device='cuda:0'), 'labels': ['0', '0', '0']}, {'boxes': tensor([[ 59., 107.,  78., 120.],\n",
            "        [ 17.,  91.,  28.,  93.],\n",
            "        [ 88.,  82., 107.,  93.],\n",
            "        [ 77.,  34.,  96.,  53.],\n",
            "        [ 67.,  11.,  82.,  25.]], device='cuda:0'), 'labels': ['3', '1', '3', '2', '4']}, {'boxes': tensor([[ 70.,  46.,  89.,  58.],\n",
            "        [  9.,  80.,  23.,  91.],\n",
            "        [ 71.,  75.,  90.,  92.],\n",
            "        [ 46.,  92.,  62.,  95.],\n",
            "        [ 88.,  48., 107.,  51.]], device='cuda:0'), 'labels': ['0', '2', '2', '1', '1']}, {'boxes': tensor([[ 66., 102.,  85., 113.],\n",
            "        [ 35.,  55.,  54.,  70.],\n",
            "        [ 21.,  18.,  40.,  23.],\n",
            "        [ 38.,  26.,  48.,  36.]], device='cuda:0'), 'labels': ['4', '4', '1', '2']}, {'boxes': tensor([[106.,  42., 124.,  61.],\n",
            "        [ 60.,  86.,  79.,  96.],\n",
            "        [ 18., 102.,  37., 116.],\n",
            "        [ 49., 117.,  67., 121.],\n",
            "        [ 16.,  47.,  34.,  63.]], device='cuda:0'), 'labels': ['2', '1', '3', '1', '3']}, {'boxes': tensor([[ 13., 111.,  30., 124.],\n",
            "        [ 17.,  14.,  32.,  24.],\n",
            "        [ 63.,  36.,  74.,  46.],\n",
            "        [ 25., 101.,  44., 120.]], device='cuda:0'), 'labels': ['2', '4', '0', '3']}, {'boxes': tensor([[ 51., 109.,  60., 119.],\n",
            "        [ 35.,  27.,  53.,  35.],\n",
            "        [ 74.,  99.,  90., 101.],\n",
            "        [ 93.,  85., 111., 103.],\n",
            "        [ 66.,  73.,  77.,  82.]], device='cuda:0'), 'labels': ['4', '1', '1', '0', '3']}, {'boxes': tensor([[38., 26., 50., 34.],\n",
            "        [60., 76., 79., 89.],\n",
            "        [41., 80., 50., 82.]], device='cuda:0'), 'labels': ['0', '3', '1']}, {'boxes': tensor([[ 62.,  36.,  81.,  53.],\n",
            "        [ 68.,  94.,  87., 107.],\n",
            "        [ 85.,  97., 104., 114.],\n",
            "        [ 72.,  74.,  91.,  78.],\n",
            "        [ 13.,  12.,  24.,  14.],\n",
            "        [  6.,  33.,  25.,  46.]], device='cuda:0'), 'labels': ['3', '4', '0', '1', '1', '0']}, {'boxes': tensor([[73.,  8., 92., 23.],\n",
            "        [ 9., 12., 24., 21.],\n",
            "        [61., 44., 80., 59.],\n",
            "        [67., 32., 81., 46.]], device='cuda:0'), 'labels': ['2', '1', '2', '0']}, {'boxes': tensor([[ 80.,  67.,  99.,  84.],\n",
            "        [  7., 103.,  21., 119.],\n",
            "        [ 45., 101.,  64., 113.],\n",
            "        [ 51.,  56.,  70.,  73.]], device='cuda:0'), 'labels': ['4', '3', '3', '4']}, {'boxes': tensor([[ 84.,  15., 103.,  32.],\n",
            "        [ 27.,  33.,  40.,  49.],\n",
            "        [ 75.,  38.,  88.,  45.],\n",
            "        [ 22.,  18.,  41.,  36.],\n",
            "        [ 72.,  79.,  91.,  90.],\n",
            "        [108.,  13., 119.,  22.]], device='cuda:0'), 'labels': ['4', '3', '4', '0', '1', '3']}, {'boxes': tensor([[ 21.,  78.,  34.,  90.],\n",
            "        [ 88.,  31., 107.,  36.],\n",
            "        [ 16.,  54.,  25.,  64.],\n",
            "        [104.,  53., 121.,  72.]], device='cuda:0'), 'labels': ['3', '1', '3', '0']}, {'boxes': tensor([[ 60.,  66.,  79.,  78.],\n",
            "        [ 71.,  51.,  89.,  62.],\n",
            "        [ 83.,   6., 102.,  22.],\n",
            "        [ 77.,  39.,  96.,  44.]], device='cuda:0'), 'labels': ['3', '3', '4', '1']}, {'boxes': tensor([[18., 76., 37., 89.],\n",
            "        [29., 11., 39., 19.],\n",
            "        [49., 24., 68., 41.],\n",
            "        [69., 54., 88., 71.],\n",
            "        [10., 51., 23., 64.],\n",
            "        [64., 75., 83., 94.]], device='cuda:0'), 'labels': ['4', '4', '0', '2', '4', '2']}, {'boxes': tensor([[ 92.,  59., 110.,  78.],\n",
            "        [ 56.,  37.,  70.,  48.],\n",
            "        [104.,   7., 123.,  25.],\n",
            "        [ 84.,  24., 103.,  39.],\n",
            "        [ 57.,  85.,  74.,  94.]], device='cuda:0'), 'labels': ['2', '2', '3', '4', '3']}, {'boxes': tensor([[  1.,  94.,  20., 105.],\n",
            "        [ 84., 100., 103., 109.],\n",
            "        [ 90.,  30., 103.,  43.],\n",
            "        [ 16.,  76.,  35.,  95.],\n",
            "        [ 70.,   0.,  89.,  13.],\n",
            "        [ 41., 105.,  60., 124.]], device='cuda:0'), 'labels': ['4', '1', '2', '4', '4', '2']}, {'boxes': tensor([[ 87.,  61., 101.,  68.],\n",
            "        [ 58., 107.,  73., 110.],\n",
            "        [ 59.,  12.,  78.,  25.],\n",
            "        [ 95., 103., 114., 114.],\n",
            "        [ 30.,  32.,  46.,  41.]], device='cuda:0'), 'labels': ['4', '1', '0', '2', '4']}, {'boxes': tensor([[104.,   3., 117.,  17.],\n",
            "        [  5.,   6.,  18.,  25.],\n",
            "        [ 96., 119., 108., 123.],\n",
            "        [ 41.,  85.,  58.,  98.],\n",
            "        [ 59.,  40.,  70.,  50.],\n",
            "        [ 91.,  71., 110.,  88.]], device='cuda:0'), 'labels': ['4', '0', '1', '3', '0', '3']}, {'boxes': tensor([[ 95.,  90., 114., 109.],\n",
            "        [ 59., 111.,  74., 121.],\n",
            "        [ 21.,  95.,  40., 112.],\n",
            "        [  4.,  89.,  23., 108.],\n",
            "        [ 21.,  67.,  40.,  76.],\n",
            "        [110.,  87., 124.,  98.]], device='cuda:0'), 'labels': ['2', '3', '3', '2', '1', '4']}, {'boxes': tensor([[ 94.,  80., 106.,  89.],\n",
            "        [ 73.,  19.,  92.,  25.],\n",
            "        [ 40., 101.,  56., 114.],\n",
            "        [ 81.,  33.,  99.,  35.],\n",
            "        [ 81.,   1., 100.,  18.],\n",
            "        [ 87.,  96., 106., 101.]], device='cuda:0'), 'labels': ['3', '1', '0', '1', '2', '1']}, {'boxes': tensor([[ 73.,  94.,  92., 108.],\n",
            "        [ 60.,  13.,  79.,  26.],\n",
            "        [105.,  45., 119.,  61.],\n",
            "        [ 42.,  36.,  55.,  47.]], device='cuda:0'), 'labels': ['4', '0', '3', '4']}, {'boxes': tensor([[ 92.,  17., 111.,  32.],\n",
            "        [ 13.,  83.,  32.,  98.],\n",
            "        [ 41., 101.,  60., 109.],\n",
            "        [ 94.,  44., 104.,  49.]], device='cuda:0'), 'labels': ['4', '0', '1', '3']}, {'boxes': tensor([[ 72.,  64.,  86.,  79.],\n",
            "        [ 30.,  74.,  48.,  93.],\n",
            "        [104.,   9., 121.,  28.],\n",
            "        [ 37.,  44.,  55.,  61.],\n",
            "        [103.,  65., 122.,  74.],\n",
            "        [ 21.,  85.,  33.,  97.]], device='cuda:0'), 'labels': ['0', '2', '4', '0', '3', '0']}, {'boxes': tensor([[ 83.,  74., 102.,  93.],\n",
            "        [ 81.,   4.,  89.,  18.],\n",
            "        [ 58.,  10.,  68.,  15.],\n",
            "        [  0.,  89.,  17.,  98.],\n",
            "        [ 36.,  95.,  53., 104.]], device='cuda:0'), 'labels': ['0', '0', '1', '4', '4']}, {'boxes': tensor([[104., 107., 123., 121.],\n",
            "        [ 90.,  85., 103.,  91.],\n",
            "        [ 53.,  79.,  72.,  82.]], device='cuda:0'), 'labels': ['0', '3', '1']}, {'boxes': tensor([[ 71.,  55.,  90.,  72.],\n",
            "        [ 20.,  35.,  33.,  38.],\n",
            "        [  2.,  28.,  16.,  36.],\n",
            "        [ 85.,  86., 104., 101.]], device='cuda:0'), 'labels': ['3', '1', '3', '3']}, {'boxes': tensor([[ 31.,  43.,  50.,  46.],\n",
            "        [ 58.,  62.,  68.,  71.],\n",
            "        [ 33., 101.,  52., 118.],\n",
            "        [ 82.,  21.,  97.,  40.],\n",
            "        [ 26.,  80.,  45.,  96.]], device='cuda:0'), 'labels': ['1', '3', '2', '0', '0']}, {'boxes': tensor([[ 14.,  33.,  33.,  52.],\n",
            "        [ 13.,  50.,  32.,  67.],\n",
            "        [ 93.,   0., 112.,  13.],\n",
            "        [ 96.,  51., 115.,  70.],\n",
            "        [ 51., 109.,  63., 121.],\n",
            "        [105.,  98., 123., 117.]], device='cuda:0'), 'labels': ['2', '2', '2', '4', '2', '2']}, {'boxes': tensor([[ 60.,  36.,  71.,  45.],\n",
            "        [ 35., 110.,  50., 114.],\n",
            "        [ 66., 114.,  77., 122.]], device='cuda:0'), 'labels': ['4', '1', '0']}, {'boxes': tensor([[101., 107., 120., 122.],\n",
            "        [ 39.,  27.,  49.,  37.],\n",
            "        [ 11.,  18.,  30.,  31.],\n",
            "        [ 13.,  37.,  21.,  49.],\n",
            "        [ 61.,  44.,  80.,  62.],\n",
            "        [ 32.,  99.,  51., 114.]], device='cuda:0'), 'labels': ['4', '2', '3', '4', '2', '3']}, {'boxes': tensor([[ 36.,   2.,  55.,  21.],\n",
            "        [ 51.,  72.,  61.,  80.],\n",
            "        [ 95.,  18., 114.,  22.]], device='cuda:0'), 'labels': ['3', '2', '1']}, {'boxes': tensor([[ 53.,  23.,  72.,  39.],\n",
            "        [  6.,  92.,  15.,  97.],\n",
            "        [ 19.,  79.,  38.,  93.],\n",
            "        [ 64., 102.,  81., 115.],\n",
            "        [ 48.,  42.,  67.,  49.],\n",
            "        [ 20.,  93.,  37., 108.]], device='cuda:0'), 'labels': ['4', '4', '0', '4', '4', '3']}, {'boxes': tensor([[ 37., 102.,  53., 117.],\n",
            "        [108.,  19., 119.,  30.],\n",
            "        [ 43.,  65.,  57.,  71.],\n",
            "        [102., 119., 121., 124.],\n",
            "        [ 97.,  65., 111.,  77.]], device='cuda:0'), 'labels': ['0', '3', '1', '1', '2']}, {'boxes': tensor([[  0., 107.,  12., 110.],\n",
            "        [ 77.,   4.,  96.,  23.],\n",
            "        [ 44.,  50.,  54.,  57.],\n",
            "        [ 23.,   2.,  42.,  13.],\n",
            "        [ 62.,  14.,  79.,  28.],\n",
            "        [ 97.,  20., 107.,  29.]], device='cuda:0'), 'labels': ['1', '2', '3', '4', '4', '2']}, {'boxes': tensor([[ 50.,  51.,  63.,  54.],\n",
            "        [ 26.,  81.,  44., 100.],\n",
            "        [ 22.,  59.,  40.,  63.]], device='cuda:0'), 'labels': ['1', '3', '1']}, {'boxes': tensor([[  7.,  29.,  26.,  36.],\n",
            "        [ 58., 105.,  77., 119.],\n",
            "        [ 26.,  99.,  40., 109.],\n",
            "        [105.,  46., 120.,  59.],\n",
            "        [ 84.,  59.,  96.,  62.],\n",
            "        [ 46.,  50.,  61.,  69.]], device='cuda:0'), 'labels': ['1', '4', '3', '2', '1', '2']}, {'boxes': tensor([[ 14.,  77.,  33.,  86.],\n",
            "        [  2.,  29.,  21.,  48.],\n",
            "        [ 85.,  15., 100.,  30.],\n",
            "        [ 71., 110.,  86., 116.],\n",
            "        [ 85.,  34., 104.,  49.],\n",
            "        [102., 101., 121., 116.]], device='cuda:0'), 'labels': ['0', '0', '3', '1', '0', '2']}, {'boxes': tensor([[44.,  2., 63., 17.],\n",
            "        [80., 13., 95., 25.],\n",
            "        [16., 29., 35., 45.]], device='cuda:0'), 'labels': ['3', '3', '0']}, {'boxes': tensor([[ 13.,  58.,  29.,  61.],\n",
            "        [ 80.,   1.,  99.,  11.],\n",
            "        [ 65.,  37.,  84.,  53.],\n",
            "        [ 83., 111., 102., 124.]], device='cuda:0'), 'labels': ['1', '4', '2', '0']}, {'boxes': tensor([[ 47.,  54.,  60.,  64.],\n",
            "        [ 74., 102.,  86., 111.],\n",
            "        [ 15.,  16.,  30.,  29.],\n",
            "        [ 81.,  86.,  95.,  98.],\n",
            "        [ 91.,  79., 110.,  82.]], device='cuda:0'), 'labels': ['3', '4', '3', '4', '1']}, {'boxes': tensor([[ 80.,  71.,  99.,  84.],\n",
            "        [ 82.,  84., 101.,  90.],\n",
            "        [ 39.,  91.,  50., 103.],\n",
            "        [ 54., 116.,  73., 122.],\n",
            "        [ 14.,  79.,  32.,  97.],\n",
            "        [ 24.,   3.,  43.,  22.]], device='cuda:0'), 'labels': ['2', '1', '0', '1', '3', '0']}, {'boxes': tensor([[112.,  99., 121., 106.],\n",
            "        [ 96.,  44., 112.,  49.],\n",
            "        [ 67.,  47.,  86.,  64.]], device='cuda:0'), 'labels': ['2', '1', '2']}, {'boxes': tensor([[ 63.,  44.,  82.,  61.],\n",
            "        [ 44.,  67.,  53.,  76.],\n",
            "        [ 76., 105.,  95., 110.],\n",
            "        [  6.,  82.,  24.,  93.],\n",
            "        [ 31.,  51.,  50.,  70.]], device='cuda:0'), 'labels': ['4', '2', '1', '0', '2']}, {'boxes': tensor([[ 63.,  84.,  78.,  88.],\n",
            "        [ 74.,  69.,  87.,  84.],\n",
            "        [ 98.,  25., 117.,  30.]], device='cuda:0'), 'labels': ['1', '2', '1']}, {'boxes': tensor([[ 39., 104.,  58., 117.],\n",
            "        [ 16.,  26.,  30.,  38.],\n",
            "        [ 53.,  41.,  72.,  50.],\n",
            "        [ 77.,  40.,  95.,  50.]], device='cuda:0'), 'labels': ['4', '0', '1', '1']}, {'boxes': tensor([[ 62., 105.,  80., 115.],\n",
            "        [ 79.,  87.,  92.,  94.],\n",
            "        [101.,  95., 111., 103.],\n",
            "        [ 41.,   6.,  60.,  11.],\n",
            "        [ 20.,  75.,  39.,  86.]], device='cuda:0'), 'labels': ['4', '3', '0', '1', '4']}, {'boxes': tensor([[ 46.,  42.,  59.,  51.],\n",
            "        [ 58.,  85.,  77., 102.],\n",
            "        [ 80., 106.,  99., 123.],\n",
            "        [105.,  22., 124.,  33.]], device='cuda:0'), 'labels': ['3', '3', '2', '1']}, {'boxes': tensor([[100.,  26., 117.,  39.],\n",
            "        [ 66., 116.,  80., 118.],\n",
            "        [ 94.,  55., 108.,  64.],\n",
            "        [ 59.,  28.,  75.,  43.]], device='cuda:0'), 'labels': ['0', '1', '3', '2']}, {'boxes': tensor([[ 67., 102.,  79., 115.],\n",
            "        [ 15.,  67.,  28.,  74.],\n",
            "        [107.,  84., 118.,  94.],\n",
            "        [ 56.,  63.,  72.,  72.],\n",
            "        [ 41.,  52.,  59.,  60.]], device='cuda:0'), 'labels': ['3', '4', '0', '2', '3']}, {'boxes': tensor([[ 82., 104., 101., 110.],\n",
            "        [ 36., 104.,  47., 110.],\n",
            "        [ 57.,  96.,  76., 111.],\n",
            "        [ 14.,  54.,  33.,  57.],\n",
            "        [ 17.,  26.,  36.,  43.]], device='cuda:0'), 'labels': ['1', '4', '4', '1', '2']}, {'boxes': tensor([[ 48.,  27.,  67.,  32.],\n",
            "        [ 38.,  86.,  56., 102.],\n",
            "        [ 83.,  90.,  95., 103.],\n",
            "        [ 15.,  29.,  34.,  42.],\n",
            "        [ 72.,  29.,  83.,  36.],\n",
            "        [ 79.,   6.,  98.,  25.]], device='cuda:0'), 'labels': ['1', '2', '2', '3', '0', '4']}, {'boxes': tensor([[109.,  56., 124.,  60.],\n",
            "        [ 11.,  40.,  30.,  48.],\n",
            "        [ 58.,   0.,  72.,   9.],\n",
            "        [ 83.,  48.,  99.,  51.],\n",
            "        [ 22.,  30.,  41.,  41.]], device='cuda:0'), 'labels': ['1', '1', '4', '1', '4']}, {'boxes': tensor([[ 64., 102.,  83., 119.],\n",
            "        [ 69.,  60.,  88.,  65.],\n",
            "        [ 21.,   8.,  36.,  27.],\n",
            "        [ 46.,  16.,  65.,  19.],\n",
            "        [ 51.,  84.,  70.,  99.]], device='cuda:0'), 'labels': ['4', '1', '3', '1', '3']}, {'boxes': tensor([[ 42.,  50.,  61.,  54.],\n",
            "        [ 85.,  29.,  96.,  31.],\n",
            "        [ 32., 119.,  50., 122.],\n",
            "        [ 87.,  99., 104., 112.],\n",
            "        [  1.,  57.,  20.,  60.]], device='cuda:0'), 'labels': ['1', '1', '1', '0', '1']}, {'boxes': tensor([[ 66.,  39.,  76.,  45.],\n",
            "        [ 73.,  87.,  89.,  94.],\n",
            "        [ 98.,  73., 116.,  89.]], device='cuda:0'), 'labels': ['3', '3', '2']}, {'boxes': tensor([[ 65.,  43.,  83.,  58.],\n",
            "        [ 59.,  64.,  78.,  77.],\n",
            "        [ 67.,  28.,  86.,  46.],\n",
            "        [ 84.,  17., 102.,  33.],\n",
            "        [ 14.,  11.,  30.,  16.],\n",
            "        [ 99.,  18., 117.,  34.]], device='cuda:0'), 'labels': ['2', '4', '0', '2', '1', '2']}, {'boxes': tensor([[ 86.,  33., 105.,  52.],\n",
            "        [ 45.,  53.,  60.,  55.],\n",
            "        [113., 103., 122., 107.],\n",
            "        [  9.,  84.,  28.,  97.]], device='cuda:0'), 'labels': ['4', '1', '1', '3']}, {'boxes': tensor([[ 70.,  26.,  86.,  42.],\n",
            "        [ 11.,  61.,  30.,  68.],\n",
            "        [ 34.,  53.,  47.,  59.],\n",
            "        [ 82.,  84., 101.,  89.],\n",
            "        [  3.,  39.,  17.,  53.],\n",
            "        [  6., 107.,  25., 118.]], device='cuda:0'), 'labels': ['2', '1', '3', '1', '2', '3']}, {'boxes': tensor([[ 85.,  55., 104.,  74.],\n",
            "        [ 57.,  13.,  76.,  28.],\n",
            "        [ 85.,   8., 104.,  27.],\n",
            "        [  0., 102.,   7., 113.],\n",
            "        [ 42.,  51.,  52.,  62.],\n",
            "        [ 45.,  81.,  64.,  99.]], device='cuda:0'), 'labels': ['2', '1', '0', '0', '2', '0']}, {'boxes': tensor([[ 36.,   4.,  49.,  17.],\n",
            "        [ 56.,  41.,  75.,  58.],\n",
            "        [ 30., 109.,  49., 113.],\n",
            "        [ 83.,  45., 101.,  50.],\n",
            "        [ 12.,  58.,  25.,  67.],\n",
            "        [  0.,  31.,  19.,  50.]], device='cuda:0'), 'labels': ['2', '4', '1', '1', '0', '0']}, {'boxes': tensor([[17.,  9., 36., 23.],\n",
            "        [67., 44., 86., 56.],\n",
            "        [45., 59., 62., 72.]], device='cuda:0'), 'labels': ['3', '3', '0']}, {'boxes': tensor([[83., 68., 97., 76.],\n",
            "        [25., 23., 40., 33.],\n",
            "        [50., 64., 69., 79.],\n",
            "        [68., 74., 87., 85.]], device='cuda:0'), 'labels': ['3', '3', '2', '3']}, {'boxes': tensor([[ 67.,  42.,  86.,  54.],\n",
            "        [ 78.,  90.,  88., 100.],\n",
            "        [ 15.,  50.,  27.,  54.],\n",
            "        [ 90.,  62., 102.,  71.],\n",
            "        [ 37.,  51.,  56.,  67.]], device='cuda:0'), 'labels': ['0', '2', '1', '4', '0']}, {'boxes': tensor([[ 16.,  23.,  32.,  38.],\n",
            "        [ 75.,  68.,  89.,  70.],\n",
            "        [ 57.,  16.,  76.,  24.],\n",
            "        [105.,  53., 122.,  65.],\n",
            "        [ 43.,  89.,  61., 108.]], device='cuda:0'), 'labels': ['4', '1', '1', '4', '2']}, {'boxes': tensor([[ 38., 107.,  57., 112.],\n",
            "        [104., 104., 118., 114.],\n",
            "        [ 65.,  24.,  78.,  33.]], device='cuda:0'), 'labels': ['1', '2', '3']}, {'boxes': tensor([[102.,  50., 111.,  61.],\n",
            "        [ 70.,  95.,  84., 106.],\n",
            "        [105.,   6., 121.,  22.],\n",
            "        [ 69.,  24.,  85.,  31.],\n",
            "        [ 14., 102.,  33., 117.]], device='cuda:0'), 'labels': ['3', '4', '3', '1', '0']}, {'boxes': tensor([[ 14.,  83.,  33., 100.],\n",
            "        [ 93.,  91., 104.,  94.],\n",
            "        [ 69.,  18.,  88.,  27.],\n",
            "        [  5.,  13.,  24.,  30.],\n",
            "        [ 65.,  45.,  75.,  51.],\n",
            "        [ 35., 105.,  54., 124.]], device='cuda:0'), 'labels': ['3', '1', '1', '3', '2', '3']}, {'boxes': tensor([[ 36.,  64.,  55.,  76.],\n",
            "        [ 58., 103.,  70., 113.],\n",
            "        [ 59.,  34.,  69.,  43.],\n",
            "        [ 35.,   6.,  54.,  23.]], device='cuda:0'), 'labels': ['4', '0', '3', '2']}, {'boxes': tensor([[ 36.,  19.,  51.,  31.],\n",
            "        [ 88.,  24., 100.,  25.],\n",
            "        [ 47., 113.,  56., 114.]], device='cuda:0'), 'labels': ['4', '1', '1']}, {'boxes': tensor([[ 7., 39., 23., 46.],\n",
            "        [86.,  1., 97.,  4.],\n",
            "        [32., 41., 51., 54.],\n",
            "        [21.,  3., 38., 16.],\n",
            "        [66., 39., 85., 46.],\n",
            "        [37., 94., 46., 99.]], device='cuda:0'), 'labels': ['1', '1', '3', '3', '1', '4']}]\n",
            "in: tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0'), tar<class 'dict'>\n",
            "torch.Size([1, 64, 128, 128])\n",
            "torch.Size([1, 64, 128, 128])\n",
            "torch.Size([1, 5, 128, 128])\n",
            "torch.Size([1, 64, 64, 64])\n",
            "torch.Size([1, 64, 64, 64])\n",
            "torch.Size([1, 5, 64, 64])\n",
            "torch.Size([1, 64, 32, 32])\n",
            "torch.Size([1, 64, 32, 32])\n",
            "torch.Size([1, 5, 32, 32])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-259-e004e5a4332d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-201-93ee592ba517>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m           \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'in: {train_input}, tar{type(train_target)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m           \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-240-fd5fd58966d4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# compute the fcos heads outputs using the features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mhead_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;31m# create the set of anchors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-236-c0f8f362a62f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mcls_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mbbox_regression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_ctrness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         return {\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-258-16f5bf0149dd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'class_head: {output.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 128 but got size 64 for tensor number 1 in the list."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot example results of matched and mismatched predictions\n",
        "def plot_results(good, bad):\n",
        "  total = len(good) + len(bad)\n",
        "  print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                    total, 100 * (len(good)) / total))\n",
        "  \n",
        "  print('Correct object detection:')\n",
        "  for g in good:\n",
        "    g.plot()\n",
        "\n",
        "  print('Incorrect object detection:')\n",
        "  for b in bad:\n",
        "    b.plot()"
      ],
      "metadata": {
        "id": "F3fH9tQpS-Zk"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot particular losses and evaluation score per\n",
        "def plot_losses(train_loss, test_loss):\n",
        "    xs = range(1, len(train_loss) + 1)\n",
        "    plt.clf()\n",
        "    plt.plot(xs, train_loss, label='train loss')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Number of epochs')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "0Xx90TE6S-rL"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n"
      ],
      "metadata": {
        "id": "KqupNuvv3S3i"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JkOLl12nkiI8"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}