{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asia281/dnn2022/blob/main/Copy_of_Copy_of_Asia_of_hw2_fcos_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VBJpAMzglRo"
      },
      "source": [
        "# Anchor-free single-stage object detection with FCOS (v2)\n",
        "\n",
        "In this exercise your goal will be to solve an object detection training and prediction task using the anchor-free single-stage approach.\n",
        "\n",
        "There are 10 points to get in total.\n",
        "\n",
        "## TLDR; overview\n",
        "\n",
        "In this task one should:\n",
        "- build an object detection model using the variant of `FCOS`,\n",
        "- train an object detection model.\n",
        "\n",
        "Hints and comments:\n",
        "\n",
        "- Model architecture and loss are heavily inspired by [FCOS](https://arxiv.org/pdf/1904.01355.pdf) paper,\n",
        "- you can freely subclass and extend the interface of classes in this exercise,\n",
        "- be sure that you understand the concept of anchor-free object detection. There are many tutorials and articles about it (e.g. [this](https://medium.com/swlh/fcos-walkthrough-the-fully-convolutional-approach-to-object-detection-777f614268c) one)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzcKkG56SItK"
      },
      "source": [
        "### Notebook changelog (compared to the initial version)\n",
        "Changed in v2:\n",
        "- Added definition of $\\sigma$ in the scoring formula.\n",
        "- Added the description how the `target` variable should look like.\n",
        "- Fixed the typo about mismatched `in_channels` and `out_channels` in the classification head description and added the whole info about it in the regression head description.\n",
        "- Added information about 1-element batch.\n",
        "- Fixed typehint in `BackboneWithFPN` (`forward(self, x: MnistCanvas)` -> `forward(self, x: torch.Tensor)`)\n",
        "- Removed info about non-existing exercise (\"...so use the foreground mask from the previous excercise.\" -> \"... so use the foreground mask.\")\n",
        "- Fixed typo: \"use `self.box_coder.decode_single` and `self.box_coder.decode_single`\" -> use \"`self.box_coder.encode_single` and `self.box_coder.decode_single`\"\n",
        "- Removed mentions of non-existing `TargetDecoder.get_predictions` and rotation.\n",
        "- Removed additional TODO placeholder from detection post-processing.\n",
        "- Added the information about using the different `evaluate` parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PsyO2OdlLLE"
      },
      "source": [
        "### Data description\n",
        "\n",
        "In this task we will paste bounding boxes with digits **from 1 to 5** randomly selected from `MNIST` dataset on a canvas of size `(128, 128)` and **randomly scaled by a factor between 0.5 and 1.0**. We assume that:\n",
        "\n",
        "- the two boxes from a canvas should have no more than `0.1` of `iou` overlap,\n",
        "- the digits are fully contained in canvas,\n",
        "- boxes are modeled using `MnistBox` class,\n",
        "- canvas is modeled using `MnistCanvas` class.\n",
        "\n",
        "Let us have a look at definition of these classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L1rAdIiRq2G8"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing import Optional\n",
        "from typing import Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MnistBox:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        x_max: int,\n",
        "        y_max: int,\n",
        "        class_nb: Optional[int] = None,\n",
        "        rotated: Optional[bool] = None,\n",
        "    ):\n",
        "        self.x_min = x_min\n",
        "        self.x_max = x_max\n",
        "        self.y_min = y_min\n",
        "        self.y_max = y_max\n",
        "        self.class_nb = class_nb\n",
        "        self.rotated = rotated\n",
        "    \n",
        "    @property\n",
        "    def x_diff(self):\n",
        "        return self.x_max - self.x_min\n",
        "    \n",
        "    @property\n",
        "    def y_diff(self):\n",
        "        return self.y_max - self.y_min\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'Mnist Box: x_min = {self.x_min},' +\\\n",
        "               f' x_max = {self.x_max}, y_min = {self.y_min},' +\\\n",
        "               f' y_max = {self.y_max}. Class = {self.class_nb}.' +\\\n",
        "               f' Rotated = {self.rotated}.'\n",
        "\n",
        "    def plot_on_ax(self, ax, color: Optional[str] = 'r'):\n",
        "        ax.add_patch(\n",
        "            patches.Rectangle(\n",
        "                (self.y_min, self.x_min),\n",
        "                 self.y_diff,\n",
        "                 self.x_diff,\n",
        "                 linewidth=1,\n",
        "                 edgecolor=color,\n",
        "                 facecolor='none',\n",
        "            )\n",
        "        )\n",
        "        ax.text(\n",
        "            self.y_min,\n",
        "            self.x_min,\n",
        "            f'{self.class_nb}' if not self.rotated else f'{self.class_nb}*',\n",
        "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
        "            clip_box=ax.clipbox,\n",
        "            clip_on=True,\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def area(self):\n",
        "        return max((self.x_max - self.x_min), 0) * max((self.y_max - self.y_min), 0)\n",
        "\n",
        "    def iou_with(self, other_box: \"MnistBox\"):\n",
        "        aux_box = MnistBox(\n",
        "            x_min=max(self.x_min, other_box.x_min),\n",
        "            x_max=min(self.x_max, other_box.x_max),\n",
        "            y_min=max(self.y_min, other_box.y_min),\n",
        "            y_max=min(self.y_max, other_box.y_max),\n",
        "        ) \n",
        "        return aux_box.area / (self.area + other_box.area - aux_box.area)\n",
        "\n",
        "    def eq(self, other: \"MnistBox\", iou: float):\n",
        "      return self.class_nb == other.class_nb and self.iou_with(other) >= iou\n",
        "\n",
        "\n",
        "class MnistCanvas:\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        image: np.ndarray,\n",
        "        boxes: List[MnistBox],\n",
        "    ):\n",
        "        self.image = image\n",
        "        self.boxes = boxes\n",
        "        self.shape = (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "\n",
        "    def add_digit(\n",
        "        self,\n",
        "        digit: np.ndarray,\n",
        "        class_nb: int,\n",
        "        x_min: int,\n",
        "        y_min: int,\n",
        "        rotated=None,\n",
        "        iou_threshold=0.1,\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Add a digit to an image if it does not overlap with existing boxes\n",
        "        above iou_threshold.\n",
        "        \"\"\"\n",
        "        image_x, image_y = digit.shape\n",
        "        if x_min >= self.image.shape[0] and y_min >= self.image.shape[1]:\n",
        "            raise ValueError('Wrong initial corner box')\n",
        "        new_box_x_min = x_min\n",
        "        new_box_y_min = y_min\n",
        "        new_box_x_max = min(x_min + image_x, self.image.shape[0])\n",
        "        new_box_y_max = min(y_min + image_y, self.image.shape[1])\n",
        "        new_box = MnistBox(\n",
        "            x_min=new_box_x_min,\n",
        "            x_max=new_box_x_max,\n",
        "            y_min=new_box_y_min,\n",
        "            y_max=new_box_y_max,\n",
        "            class_nb=class_nb,\n",
        "            rotated=rotated,\n",
        "        )\n",
        "        old_background = self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ]\n",
        "        for box in self.boxes:\n",
        "            if new_box.iou_with(box) > iou_threshold:\n",
        "                return False\n",
        "        self.image[\n",
        "            new_box_x_min:new_box_x_max,\n",
        "            new_box_y_min:new_box_y_max\n",
        "        ] = np.maximum(old_background, digit)\n",
        "        self.boxes.append(\n",
        "            new_box\n",
        "        ) \n",
        "        return True\n",
        "        \n",
        "    def get_torch_tensor(self) -> torch.Tensor:\n",
        "        np_image = self.image.astype('float32')\n",
        "        np_image = np_image.reshape(\n",
        "            (1, 1, self.image.shape[0], self.image.shape[1])\n",
        "        )\n",
        "        return torch.from_numpy(np_image).to(DEVICE)\n",
        "\n",
        "    @classmethod\n",
        "    def get_empty_of_size(cls, size: Tuple[int, int]):\n",
        "        return cls(\n",
        "            image=np.zeros(size),\n",
        "            boxes=[],\n",
        "        )\n",
        "\n",
        "    def plot(self, boxes: Optional[List[MnistBox]] = None):\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(self.image)\n",
        "        boxes = boxes or self.boxes\n",
        "        for box in boxes:\n",
        "            box.plot_on_ax(ax)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sqrSxb66RK-f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWMxgsgFtlze"
      },
      "source": [
        "Each canvas has 3-6 boxes with randomly selected digits. The digits for training data are from first 10K examples from `MNIST` train data. The digits for test data are selected from first 1K examples from `MNIST` test data. The Dataset is generated using the following functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HezSZXw4z-cx"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "import skimage.transform as st\n",
        "\n",
        "\n",
        "mnist_data = mnist.load_data()\n",
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist_data\n",
        "\n",
        "\n",
        "def crop_insignificant_values(digit:np.ndarray, threshold=0.1):\n",
        "    bool_digit = digit > threshold\n",
        "    x_range = bool_digit.max(axis=0)\n",
        "    y_range = bool_digit.max(axis=1)\n",
        "    start_x = (x_range.cumsum() == 0).sum()\n",
        "    end_x = (x_range[::-1].cumsum() == 0).sum()\n",
        "    start_y = (y_range.cumsum() == 0).sum()\n",
        "    end_y = (y_range[::-1].cumsum() == 0).sum()\n",
        "    return digit[start_y:-end_y - 1, start_x:-end_x - 1]\n",
        "\n",
        "\n",
        "TRAIN_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_train[:10000])\n",
        "]\n",
        "TRAIN_CLASSES = mnist_y_train[:10000]\n",
        "\n",
        "TEST_DIGITS = [\n",
        "    crop_insignificant_values(digit) / 255.0\n",
        "    for digit_index, digit in enumerate(mnist_x_test[:1000])\n",
        "]\n",
        "TEST_CLASSES = mnist_y_test[:1000]\n",
        "\n",
        "\n",
        "def get_random_canvas(\n",
        "    digits: Optional[List[np.ndarray]] = None,\n",
        "    classes: Optional[List[int]] = None,\n",
        "    nb_of_digits: Optional[int] = None,\n",
        "    labels = [0, 1, 2, 3, 4]\n",
        "    ) -> MnistCanvas:\n",
        "    digits = digits if digits is not None else TRAIN_DIGITS\n",
        "    classes = classes if classes is not None else TRAIN_CLASSES\n",
        "    nb_of_digits = nb_of_digits if nb_of_digits is not None else np.random.randint(low=3, high=6 + 1)\n",
        "    new_canvas = MnistCanvas.get_empty_of_size(size=(128, 128))\n",
        "    attempts_done = 0\n",
        "    while attempts_done < nb_of_digits:\n",
        "        current_digit_index = np.random.randint(len(digits))\n",
        "        current_digit_class = classes[current_digit_index]\n",
        "        if current_digit_class not in labels:\n",
        "            continue\n",
        "        rescale = np.random.random() > 0.5\n",
        "        current_digit = digits[current_digit_index]\n",
        "        if rescale:\n",
        "            factor = (np.random.random() / 2) + 0.5\n",
        "            current_digit = st.resize(\n",
        "                current_digit, \n",
        "                (int(current_digit.shape[0] * factor), int(current_digit.shape[1] * factor)))\n",
        "            # current_digit = np.rot90(current_digit)\n",
        "        random_x_min = np.random.randint(0, 128 - current_digit.shape[0] - 3)\n",
        "        random_y_min = np.random.randint(0, 128 - current_digit.shape[1] - 3)\n",
        "        if new_canvas.add_digit(\n",
        "            digit=current_digit,\n",
        "            x_min=random_x_min,\n",
        "            y_min=random_y_min,\n",
        "            class_nb=current_digit_class,\n",
        "            rotated=rescale,\n",
        "        ):\n",
        "            attempts_done += 1\n",
        "    return new_canvas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i2OjUEC7eaC"
      },
      "source": [
        "Let us have a look at example canvas (rescaled digits have additional *added to description)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "OsLpINOtvhd8",
        "outputId": "5be41b06-c861-411d-fa2b-778370b1681b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "mnist_canvas = get_random_canvas()\n",
        "mnist_canvas.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgU2bKyJ3pVh"
      },
      "source": [
        "For training one can either:\n",
        "- generate `TRAIN_CANVAS` similarly to `TEST_CANVAS` creation,\n",
        "- use the fact that `get_random_canvas()` generates a random train canvas and generate training data on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GpJZUkDGJSi"
      },
      "source": [
        "### Model building (5 pt.)\n",
        "\n",
        "\n",
        "One should build a model for digit detection in $\\texttt{pytorch}$. Model should consist of:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qdCBH9PkT2C"
      },
      "source": [
        "#### $\\texttt{backbone}$:\n",
        "\n",
        "We provided you with a backbone model architecture paired with Feature Pyramid Network (`BackboneWithFPN`) that accepts a `MnistCanvas` instance and output a dictionary, which has a FPN group name as a keys and their tensors as value.\n",
        "For a FPN with strides set to [32, 64, 128] and number of output channels set to 64, the sizes of the tensors will be [1, 64, 128, 128], [1, 64, 64, 64], [1, 64, 32, 32] consecutively. This module should be trained together with the rest of your solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kXco8riNGHhl"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from torch import nn, Tensor\n",
        "from torchvision.ops.feature_pyramid_network import FeaturePyramidNetwork\n",
        "\n",
        "\n",
        "class Backbone(torch.nn.Module):\n",
        "    def __init__(self, strides = [8, 16, 32]):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.first_block = torch.nn.Sequential(\n",
        "            nn.Conv2d(1, strides[0], (3, 3), padding=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        \n",
        "        self.blocks = torch.nn.ModuleList(\n",
        "            [torch.nn.Sequential(*[\n",
        "                nn.Conv2d(strides[i-1], strides[i], (3, 3), padding=1),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(2, 2),\n",
        "              ]) for i in range(1, len(strides))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        image = x.to(DEVICE).view(1, 1, 128, 128)\n",
        "        x = self.first_block(image)\n",
        "        aux = [x]\n",
        "        for block in self.blocks:\n",
        "            x = block(aux[-1])\n",
        "            aux.append(x)\n",
        "        return aux\n",
        "\n",
        "\n",
        "class BackboneWithFPN(torch.nn.Module):\n",
        "    def __init__(self, strides, out_channels=32) -> None:\n",
        "        super().__init__()\n",
        "        self.strides = strides\n",
        "        self.out_channels = out_channels\n",
        "        self.backbone = Backbone(self.strides)\n",
        "        self.fpn = FeaturePyramidNetwork(self.strides, self.out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        output_backbone = self.backbone(x)\n",
        "        \n",
        "        x = OrderedDict()\n",
        "        for i, f in enumerate(output_backbone):\n",
        "            x[f'feat{i}'] = f\n",
        "        output_fpn = self.fpn(x)\n",
        "        return output_fpn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkOLl12nkiI8"
      },
      "source": [
        "#### $\\texttt{anchor generator}$:\n",
        "\n",
        "FCOS is anchor-free in a typical sense of this word, but it can also be said that there is one pixel-wise \"anchor\" per localisation on a given feature map.\n",
        "Therefore, anchor generator from `torchvision` is used for convenience.\n",
        "You will obtain $128^2 + 64^2 + 32^2 = 21504$ locations in total for the previously chosen strides.\n",
        "They will be called anchors in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dyd2-zOxf9VM",
        "outputId": "5e660215-ed38-414e-d3a3-48973aef92a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# example code - anchor generator is already included in the code later\n",
        "from torchvision.models.detection.anchor_utils import AnchorGenerator\n",
        "\n",
        "anchor_sizes = ((32,), (64,), (128,))  # equal to strides of FPN multi-level feature map\n",
        "aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one anchor for each level\n",
        "anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "anchor_generator.num_anchors_per_location()\n",
        "# notice that effectively one anchor is one location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "iM3oSesif-F5"
      },
      "outputs": [],
      "source": [
        "# Later in the code you will use the anchor generator in the following way:\n",
        "# anchors = anchor_generator(images, features)\n",
        "# [x.size(2) * x.size(3) for x in features] # recover level sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTYyCuMdmBk7"
      },
      "source": [
        "#### $\\texttt{FCOSClassificationHead}$ (1 pt.):\n",
        "\n",
        "Write a classification head to be used in FCOS.\n",
        "The input is is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$).\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The additional final block should be `nn.Conv2d` outputting `C` channels.\n",
        "The final output should consist of classification logits of shape `(N, A, C)`, where `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_all(tensors: List[Tensor]) -> Tensor: # (N, C, S, S)\n",
        "  tensors = [tensor.transpose(2, 3) for tensor in tensors]  # (N, C, S, S) to work with get_tensor\n",
        "  tensors = [tensor.reshape(*tensor.shape[:2], -1) for tensor in tensors]  # (N, C, S * S)\n",
        "  tensor =  torch.cat(tensors, dim=2) # (N, C, sum S*S)\n",
        "  tensor = tensor.transpose(1, 2) # (N, sum S*S, C)\n",
        "  return tensor \n"
      ],
      "metadata": {
        "id": "kz1Bui2q6NTr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kpDP9QVjnn05"
      },
      "outputs": [],
      "source": [
        "class FCOSClassificationHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_classes: int,\n",
        "        num_convs: int = 4,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        size = in_channels\n",
        "        c = num_classes\n",
        "\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs\n",
        "        \n",
        "\n",
        "        self.final = nn.Conv2d(size, c, kernel_size=3, padding=1)\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tensor:\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        \n",
        "        output = [self.blocks(layer) for layer in x]  # (N, in_channels, S, S)\n",
        "        output = [self.final(layer) for layer in output]  # (N, C, S, S)\n",
        "        output = reshape_all(output) # (N, S*S, C)\n",
        "        return output\n",
        "\n",
        "        ################################################################################################\n",
        "        # end of your code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcHAnQy2mFJU"
      },
      "source": [
        "#### $\\texttt{FCOSRegressionHead}$  (1 pt.):\n",
        "\n",
        "Write a regression head to be used in FCOS - both for bounding boxes and center-ness.\n",
        "The input is the output of `BackboneWithFPN` forward call.\n",
        "This module should contain $n$ blocks with `nn.Conv2d`, `nn.GroupNorm`, and `nn.ReLU` each (in the paper, $n=4$), which will be shared for regression and center-ness.\n",
        "Each convolutional layer should input and output `self.in_channels` channels.\n",
        "The final block for bounding box regression should be `nn.Conv2d` and have `4` channels and it should be followed by relu functional to get rid of negative values.\n",
        "The final block for center-ness regression should be `nn.Conv2d` and have `1` channel.\n",
        "The output should consist of a tuple of tensors (bounding box regression and center-ness).\n",
        "Bounding box regression logits should be of shape `(N, A, 4)`, whereas for center-ness that would be `(N, A, 1)`.\n",
        "Similarly, `N` means the number of samples in a batch, `A` is the sum of all FPN strides (21504 in the aforementioned case), and `C` is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3H6KNzi1mEhg"
      },
      "outputs": [],
      "source": [
        "class FCOSRegressionHead(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        num_convs: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        size = in_channels\n",
        "\n",
        "        self.blocks = torch.nn.Sequential(*[\n",
        "                nn.Conv2d(size, size, (3, 3), padding=1),\n",
        "                nn.GroupNorm(size, size),\n",
        "                nn.ReLU(),\n",
        "              ]) * num_convs\n",
        "\n",
        "        self.bounding = nn.Sequential(\n",
        "            nn.Conv2d(size, 4, kernel_size=3, padding=1), \n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.center = nn.Conv2d(size, 1, kernel_size=3, padding=1)\n",
        "        # end of your code\n",
        "        ################################################################################################\n",
        "        \n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Tuple[Tensor, Tensor]:\n",
        "        pass # TODO: your code here\n",
        "        ################################################################################################\n",
        "        output = [self.blocks(i) for i in x] # (N, in_channels, S, S)\n",
        "        bounding = [self.bounding(out) for out in output] # (N, 4, S, S)\n",
        "        center = [self.center(out) for out in output] # (N, 4, S, S)\n",
        "        \n",
        "        bounding = reshape_all(bounding) # (N, S*S, 4)\n",
        "        center = reshape_all(center) # (N, S*S, 1)\n",
        "\n",
        "        return bounding, center\n",
        "        ################################################################################################\n",
        "        # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds0I47ydkvUL"
      },
      "source": [
        "#### $\\texttt{FCOSHead}$ (2 pt.):\n",
        "\n",
        "Here, the computation of the foreground indices and losses takes place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PX8P0s-jOK-F"
      },
      "outputs": [],
      "source": [
        "class BoxLinearCoder:\n",
        "    \"\"\"\n",
        "    The linear box-to-box transform defined in FCOS. The transformation is parameterized\n",
        "    by the distance from the center of (square) src box to 4 edges of the target box.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, normalize_by_size: bool = True) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            normalize_by_size (bool): normalize deltas by the size of src (anchor) boxes.\n",
        "        \"\"\"\n",
        "        self.normalize_by_size = normalize_by_size\n",
        "\n",
        "    def encode_single(self, reference_boxes: Tensor, proposals: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Encode a set of proposals with respect to some reference boxes\n",
        "\n",
        "        Args:\n",
        "            reference_boxes (Tensor): reference boxes\n",
        "            proposals (Tensor): boxes to be encoded\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the encoded relative box offsets that can be used to\n",
        "            decode the boxes.\n",
        "        \"\"\"\n",
        "        # get the center of reference_boxes\n",
        "        reference_boxes_ctr_x = 0.5 * (reference_boxes[:, 0] + reference_boxes[:, 2])\n",
        "        reference_boxes_ctr_y = 0.5 * (reference_boxes[:, 1] + reference_boxes[:, 3])\n",
        "\n",
        "        # get box regression transformation deltas\n",
        "        target_l = reference_boxes_ctr_x - proposals[:, 0]\n",
        "        target_t = reference_boxes_ctr_y - proposals[:, 1]\n",
        "        target_r = proposals[:, 2] - reference_boxes_ctr_x\n",
        "        target_b = proposals[:, 3] - reference_boxes_ctr_y\n",
        "\n",
        "        targets = torch.stack((target_l, target_t, target_r, target_b), dim=1)\n",
        "        if self.normalize_by_size:\n",
        "            reference_boxes_w = reference_boxes[:, 2] - reference_boxes[:, 0]\n",
        "            reference_boxes_h = reference_boxes[:, 3] - reference_boxes[:, 1]\n",
        "            reference_boxes_size = torch.stack(\n",
        "                (reference_boxes_w, reference_boxes_h, reference_boxes_w, reference_boxes_h), dim=1\n",
        "            )\n",
        "            targets = targets / reference_boxes_size\n",
        "\n",
        "        return targets\n",
        "\n",
        "    def decode_single(self, rel_codes: Tensor, boxes: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        From a set of original boxes and encoded relative box offsets,\n",
        "        get the decoded boxes.\n",
        "\n",
        "        Args:\n",
        "            rel_codes (Tensor): encoded boxes\n",
        "            boxes (Tensor): reference boxes.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: the predicted boxes with the encoded relative box offsets.\n",
        "        \"\"\"\n",
        "\n",
        "        boxes = boxes.to(rel_codes.dtype)\n",
        "\n",
        "        ctr_x = 0.5 * (boxes[:, 0] + boxes[:, 2])\n",
        "        ctr_y = 0.5 * (boxes[:, 1] + boxes[:, 3])\n",
        "        if self.normalize_by_size:\n",
        "            boxes_w = boxes[:, 2] - boxes[:, 0]\n",
        "            boxes_h = boxes[:, 3] - boxes[:, 1]\n",
        "            boxes_size = torch.stack((boxes_w, boxes_h, boxes_w, boxes_h), dim=1)\n",
        "            rel_codes = rel_codes * boxes_size\n",
        "\n",
        "        pred_boxes1 = ctr_x - rel_codes[:, 0]\n",
        "        pred_boxes2 = ctr_y - rel_codes[:, 1]\n",
        "        pred_boxes3 = ctr_x + rel_codes[:, 2]\n",
        "        pred_boxes4 = ctr_y + rel_codes[:, 3]\n",
        "        pred_boxes = torch.stack((pred_boxes1, pred_boxes2, pred_boxes3, pred_boxes4), dim=1)\n",
        "        return pred_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loss calculation\n",
        "Compute the losses. \n",
        "They should be calculated on the positive locations/anchors, so use the foreground mask.\n",
        "For regression, use `self.box_coder.encode_single` and `self.box_coder.decode_single` to move between standard (x, y, x, y) and FCOS (l, t, r, b) bounding box format.\n",
        "There are three losses to be written.\n",
        "- classification loss (with `torchvision.ops.sigmoid_focal_loss`). (1 pt.)\n",
        "- Bounding box regression (with `torchvision.ops.generalized_box_iou_loss`). Decode predictions with `self.box_coder.decode_single` before regressing against the ground truth. (1 pt.)\n",
        "- ctrness loss (`torchvision.ops.sigmoid_focal_loss`). Use Equation 3 from the paper to calculate the grond truth for the center-ness. (2 pt.)"
      ],
      "metadata": {
        "id": "qIGit7Yp_zEG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nDX5c0lw9ZxM"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "\n",
        "from torchvision.ops import sigmoid_focal_loss, generalized_box_iou_loss\n",
        "from torchvision.ops import boxes as box_ops\n",
        "from torchvision.models.detection.transform import GeneralizedRCNNTransform\n",
        "\n",
        "class FCOSHead(nn.Module):\n",
        "    \"\"\"\n",
        "    A regression and classification head for use in FCOS.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): number of channels of the input feature\n",
        "        num_classes (int): number of classes to be predicted\n",
        "        num_convs (Optional[int]): number of conv layer of head. Default: 4.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, num_classes: int, num_convs: Optional[int] = 4) -> None:\n",
        "        super().__init__()\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.classification_head = FCOSClassificationHead(in_channels, num_classes, num_convs)\n",
        "        self.regression_head = FCOSRegressionHead(in_channels, num_convs)\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],             # anchors/locations\n",
        "        matched_idxs: List[Tensor],        # tells to which bounding box anchors are matched, -1 mean no matches\n",
        "    ) -> Dict[str, Tensor]:\n",
        "\n",
        "        cls_logits = head_outputs[\"cls_logits\"]  # [N, A, C]\n",
        "        bbox_regression = head_outputs[\"bbox_regression\"]  # [N, A, 4]\n",
        "        bbox_ctrness = head_outputs[\"bbox_ctrness\"]  # [N, A, 1]\n",
        "\n",
        "        # Correct shapes!!\n",
        "\n",
        "        all_gt_classes_targets = []\n",
        "        all_gt_boxes_targets = []\n",
        "        assert len(matched_idxs) == 1\n",
        "        \n",
        "        for targets_per_image, matched_idxs_per_image in zip(targets, matched_idxs):\n",
        "            im = targets_per_image[\"labels\"]\n",
        "            matches = matched_idxs_per_image\n",
        "            t = targets_per_image[\"boxes\"]\n",
        "            gt_classes_targets = targets_per_image[\"labels\"][matched_idxs_per_image.clip(min=0)]\n",
        "            \n",
        "            gt_boxes_targets = targets_per_image[\"boxes\"][matched_idxs_per_image.clip(min=0)]\n",
        "            \n",
        "            gt_classes_targets[matched_idxs_per_image < 0] = -1  # background\n",
        "            all_gt_classes_targets.append(gt_classes_targets)\n",
        "            all_gt_boxes_targets.append(gt_boxes_targets)\n",
        "\n",
        "        all_gt_classes_targets = torch.stack(all_gt_classes_targets)\n",
        "        \n",
        "        foregroud_mask = all_gt_classes_targets >= 0        \n",
        "        num_foreground = foregroud_mask.sum().item()\n",
        "\n",
        "        loss_bbox_reg = self.compute_loss_bbox_reg(anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask)\n",
        "        loss_bbox_ctrness = self.compute_loss_ctrness(anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask)\n",
        "        loss_cls = self.compute_loss_cls(cls_logits, all_gt_classes_targets, foregroud_mask)\n",
        "\n",
        "        return {\n",
        "            \"classification\": loss_cls / max(1, num_foreground),\n",
        "            \"bbox_regression\": loss_bbox_reg / max(1, num_foreground),\n",
        "            \"bbox_ctrness\": loss_bbox_ctrness / max(1, num_foreground),\n",
        "        }\n",
        "\n",
        "    def compute_loss_ctrness(self, anchors, bbox_ctrness, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = torch.stack(anchors)\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_ctrness = bbox_ctrness[foregroud_mask]\n",
        "        bbox_ctrness = torch.reshape(bbox_ctrness, (-1,))\n",
        "\n",
        "        all_gt_boxes_targets = torch.stack(all_gt_boxes_targets)\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        targets = self.box_coder.encode_single(anchors, all_gt_boxes_targets)\n",
        "        l = targets[:, 0]\n",
        "        t = targets[:, 1]\n",
        "        r = targets[:, 2]\n",
        "        b = targets[:, 3]\n",
        "        # Equation 3 from paper\n",
        "        targets = torch.sqrt((torch.min(l, r) / torch.max(l, r)) * (torch.min(t, b) / torch.max(t, b)))\n",
        "        \n",
        "        loss = torch.nn.BCEWithLogitsLoss(reduction=\"sum\")\n",
        "        return loss(bbox_ctrness, targets)\n",
        "\n",
        "    def compute_loss_bbox_reg(self, anchors, bbox_regression, all_gt_boxes_targets, foregroud_mask):\n",
        "        anchors = torch.stack(anchors)\n",
        "        anchors = anchors[foregroud_mask]\n",
        "        bbox_regression = bbox_regression[foregroud_mask]\n",
        "        bbox_regression = self.box_coder.decode_single(bbox_regression, anchors)\n",
        "        all_gt_boxes_targets = torch.stack(all_gt_boxes_targets)\n",
        "        all_gt_boxes_targets = all_gt_boxes_targets[foregroud_mask]\n",
        "        return torchvision.ops.generalized_box_iou_loss(bbox_regression, all_gt_boxes_targets, reduction=\"sum\")\n",
        "\n",
        "    def compute_loss_cls(self, cls_logits, all_gt_classes_targets, foregroud_mask):\n",
        "        masked = torch.zeros(cls_logits.shape, dtype=torch.int64, device=cls_logits.device)\n",
        "        masked[foregroud_mask] = nn.functional.one_hot(all_gt_classes_targets[foregroud_mask].to(torch.int64), num_classes=cls_logits.shape[-1])\n",
        "        masked = masked.type(torch.float32)\n",
        "        return torchvision.ops.sigmoid_focal_loss(cls_logits, masked, alpha=0.9, reduction=\"sum\")\n",
        "\n",
        "    def forward(self, x: List[Tensor]) -> Dict[str, Tensor]:\n",
        "        cls_logits = self.classification_head(x)\n",
        "        bbox_regression, bbox_ctrness = self.regression_head(x)\n",
        "        return {\n",
        "            \"cls_logits\": cls_logits,\n",
        "            \"bbox_regression\": bbox_regression,\n",
        "            \"bbox_ctrness\": bbox_ctrness,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFYNzC9KOK-G"
      },
      "source": [
        "#### Post-processing (1 pt.)\n",
        "Fill the gaps in the postprocessing routine.\n",
        "The paper states: \n",
        "\n",
        "> (...) the final score (used for ranking the detected bounding boxes) \n",
        "> is computed by multiplying the predicted center-ness with the corresponding classification score. \n",
        "> Thus the center-ness can downweight the scores of bounding boxes far from the center of an object. \n",
        "> As a result, with high probability, these low-quality bounding boxes might be filtered out by \n",
        "> the final non-maximum suppression (NMS) process, improving the detection performance remarkably.\n",
        "\n",
        "1. Remove boxes with score smaller than `self.score_thresh`. The score is given by `sqrt`($\\sigma$(`classification_score`) * $\\sigma$(`cente-ness_score`)), where $\\sigma$ stands for the sigmoid function. (1pt.)\n",
        "2. Keep only top `self.topk_candidates` scoring predictions (1pt.)\n",
        "\n",
        "The `compute_loss` function here calculates the indexes of matched classes for each anchor/location for your convenience in later calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EB17o3zs1JiR"
      },
      "outputs": [],
      "source": [
        "class FCOS(nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        backbone: nn.Module,\n",
        "        num_classes: int,\n",
        "        # transform parameters\n",
        "        image_mean: Optional[List[float]] = None,\n",
        "        image_std: Optional[List[float]] = None,\n",
        "        # Anchor parameters\n",
        "        anchor_generator: AnchorGenerator = None,\n",
        "        center_sampling_radius: float = 1.5,\n",
        "        score_thresh: float = 0.2,\n",
        "        nms_thresh: float = 0.6,\n",
        "        detections_per_img: int = 100,\n",
        "        topk_candidates: int = 1000,\n",
        "        num_convs_in_heads:int = 4,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.backbone = backbone\n",
        "        self.anchor_generator = anchor_generator\n",
        "        self.head = FCOSHead(backbone.out_channels, num_classes, num_convs=num_convs_in_heads)\n",
        "        self.box_coder = BoxLinearCoder(normalize_by_size=True)\n",
        "        self.transform = GeneralizedRCNNTransform(128, 128, image_mean, image_std, **kwargs)\n",
        "\n",
        "        self.center_sampling_radius = center_sampling_radius\n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.detections_per_img = detections_per_img\n",
        "        self.topk_candidates = topk_candidates\n",
        "\n",
        "\n",
        "    def compute_loss(\n",
        "        self,\n",
        "        targets: List[Dict[str, Tensor]],\n",
        "        head_outputs: Dict[str, Tensor],\n",
        "        anchors: List[Tensor],\n",
        "        num_anchors_per_level: List[int],\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        matched_idxs = []\n",
        "        for anchors_per_image, targets_per_image in zip(anchors, targets): # batch\n",
        "            if targets_per_image[\"boxes\"].numel() == 0:\n",
        "                matched_idxs.append(\n",
        "                    torch.full((anchors_per_image.size(0),), -1, dtype=torch.int64, device=anchors_per_image.device)\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            gt_boxes = targets_per_image[\"boxes\"]\n",
        "            gt_centers = (gt_boxes[:, :2] + gt_boxes[:, 2:]) / 2  # Nx2                 # Calculate centres of bounding boxes\n",
        "            anchor_centers = (anchors_per_image[:, :2] + anchors_per_image[:, 2:]) / 2  # N  \n",
        "            anchor_sizes = anchors_per_image[:, 2] - anchors_per_image[:, 0]            # Match anchors\n",
        "            # center sampling: anchor point must be close enough to gt center.\n",
        "            pairwise_match = (anchor_centers[:, None, :] - gt_centers[None, :, :]).abs_().max(\n",
        "                dim=2\n",
        "            ).values < self.center_sampling_radius * anchor_sizes[:, None]\n",
        "            # compute pairwise distance between N points and M boxes\n",
        "            x, y = anchor_centers.unsqueeze(dim=2).unbind(dim=1)  # (N, 1)\n",
        "            x0, y0, x1, y1 = gt_boxes.unsqueeze(dim=0).unbind(dim=2)  # (1, M)\n",
        "            pairwise_dist = torch.stack([x - x0, y - y0, x1 - x, y1 - y], dim=2)  # (N, M)\n",
        "\n",
        "            # anchor point must be inside gt\n",
        "            pairwise_match &= pairwise_dist.min(dim=2).values > 0\n",
        "\n",
        "            # each anchor is only responsible for certain scale range.\n",
        "            lower_bound = anchor_sizes * 4\n",
        "            lower_bound[: num_anchors_per_level[0]] = 0\n",
        "            upper_bound = anchor_sizes * 8\n",
        "            upper_bound[-num_anchors_per_level[-1] :] = float(\"inf\")\n",
        "            pairwise_dist = pairwise_dist.max(dim=2).values\n",
        "            pairwise_match &= (pairwise_dist > lower_bound[:, None]) & (pairwise_dist < upper_bound[:, None])\n",
        "\n",
        "            # match the GT box with minimum area, if there are multiple GT matches\n",
        "            gt_areas = (gt_boxes[:, 2] - gt_boxes[:, 0]) * (gt_boxes[:, 3] - gt_boxes[:, 1])  # N\n",
        "            pairwise_match = pairwise_match.to(torch.float32) * (1e8 - gt_areas[None, :])\n",
        "            min_values, matched_idx = pairwise_match.max(dim=1)  # R, per-anchor match\n",
        "            matched_idx[min_values < 1e-5] = -1  # unmatched anchors are assigned -1\n",
        "            matched_idxs.append(matched_idx)\n",
        "        # end of your code\n",
        "        # matched index - anchor-to-target match\n",
        "        return self.head.compute_loss(targets, head_outputs, anchors, matched_idxs)\n",
        "\n",
        "    def postprocess_detections(\n",
        "        self, head_outputs: Dict[str, List[Tensor]], anchors: List[List[Tensor]], image_shapes: List[Tuple[int, int]]\n",
        "    ) -> List[Dict[str, Tensor]]:\n",
        "        class_logits = head_outputs[\"cls_logits\"]\n",
        "        box_regression = head_outputs[\"bbox_regression\"]\n",
        "        box_ctrness = head_outputs[\"bbox_ctrness\"]\n",
        "\n",
        "        \n",
        "        num_images = len(image_shapes)\n",
        "\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "\n",
        "        for index in range(num_images):\n",
        "            box_regression_per_image = [br[index] for br in box_regression]\n",
        "            logits_per_image = [cl[index] for cl in class_logits]\n",
        "            box_ctrness_per_image = [bc[index] for bc in box_ctrness]\n",
        "            anchors_per_image, image_shape = anchors[index], image_shapes[index]\n",
        "\n",
        "            image_boxes = []\n",
        "            image_scores = []\n",
        "            image_labels = []\n",
        "\n",
        "            for box_regression_per_level, logits_per_level, box_ctrness_per_level, anchors_per_level in zip(\n",
        "                box_regression_per_image, logits_per_image, box_ctrness_per_image, anchors_per_image\n",
        "            ):\n",
        "                num_classes = logits_per_level.shape[-1]\n",
        "                \n",
        "                # TODO: your code here\n",
        "                # Remove low scoring boxes and keep only top k scoring predictions\n",
        "                ################################################################################################\n",
        "                max_logits = logits_per_level.max(dim=-1)\n",
        "                scores_per_level = torch.sqrt(\n",
        "                    torch.sigmoid(max_logits.values) * torch.sigmoid(box_ctrness_per_level.squeeze(dim=1)))\n",
        "                \n",
        "                top = scores_per_level > self.score_thresh\n",
        "\n",
        "                box_regression_per_level = box_regression_per_level[top]\n",
        "                anchors_per_level = anchors_per_level[top]\n",
        "                scores_per_level = scores_per_level[top]\n",
        "                #print(f\"cand: {self.topk_candidates}, sc: {scores_per_level}\")\n",
        "                \n",
        "                if self.topk_candidates >= scores_per_level.shape[0]:\n",
        "                    topk_idxs = torch.sort(scores_per_level, descending=True).indices\n",
        "                else:\n",
        "                    topk_idxs = torch.topk(scores_per_level, self.topk_candidates).indices\n",
        "                \n",
        "\n",
        "                scores_per_level = scores_per_level[topk_idxs]\n",
        "\n",
        "                topk_idxs = max_logits.indices[top][topk_idxs] + topk_idxs * num_classes\n",
        "                ################################################################################################\n",
        "                # end of your code\n",
        "\n",
        "                #print(topk_idxs)\n",
        "                anchor_idxs = torch.div(topk_idxs, num_classes, rounding_mode=\"floor\")\n",
        "                labels_per_level = topk_idxs % num_classes\n",
        "\n",
        "                boxes_per_level = self.box_coder.decode_single(\n",
        "                    box_regression_per_level[anchor_idxs], anchors_per_level[anchor_idxs]\n",
        "                )\n",
        "                #print(boxes_per_level.shape)\n",
        "                boxes_per_level = box_ops.clip_boxes_to_image(boxes_per_level, image_shape)\n",
        "\n",
        "                #print(boxes_per_level.shape)\n",
        "                image_boxes.append(boxes_per_level)\n",
        "                image_scores.append(scores_per_level)\n",
        "                image_labels.append(labels_per_level)\n",
        "\n",
        "            image_boxes = torch.cat(image_boxes, dim=0)\n",
        "            image_scores = torch.cat(image_scores, dim=0)\n",
        "            image_labels = torch.cat(image_labels, dim=0)\n",
        "\n",
        "            # non-maximum suppression\n",
        "            keep = box_ops.batched_nms(image_boxes, image_scores, image_labels, self.nms_thresh)\n",
        "            keep = keep[: self.detections_per_img]\n",
        "\n",
        "            detections.append(\n",
        "                {\n",
        "                    \"boxes\": image_boxes[keep],\n",
        "                    \"scores\": image_scores[keep],\n",
        "                    \"labels\": image_labels[keep],\n",
        "                }\n",
        "            )\n",
        "        return detections\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: List[Tensor],\n",
        "        targets: Optional[List[Dict[str, Tensor]]] = None,\n",
        "    ) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images (list[Tensor]): images to be processed\n",
        "            targets (list[Dict[Tensor]]): ground-truth boxes present in the image (optional)\n",
        "\n",
        "        Returns:\n",
        "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
        "                During training, it returns a dict[Tensor] which contains the losses.\n",
        "                During testing, it returns list[BoxList] contains additional fields\n",
        "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
        "        \"\"\"\n",
        "        \n",
        "        # transform the input (normalise with std and )\n",
        "        images, targets = self.transform(images, targets)\n",
        "\n",
        "        # get the features from the backbone\n",
        "        features = self.backbone(images.tensors)\n",
        "        if isinstance(features, torch.Tensor):\n",
        "            features = OrderedDict([(\"0\", features)])\n",
        "        features = list(features.values())\n",
        "\n",
        "        # compute the fcos heads outputs using the features\n",
        "        head_outputs = self.head(features)\n",
        "\n",
        "        # create the set of anchors\n",
        "        anchors = self.anchor_generator(images, features)\n",
        "        # recover level sizes\n",
        "        num_anchors_per_level = [x.size(2) * x.size(3) for x in features]\n",
        "        losses = {}\n",
        "        detections: List[Dict[str, Tensor]] = []\n",
        "        if self.training:\n",
        "\n",
        "            losses = self.compute_loss(targets, head_outputs, anchors, num_anchors_per_level)\n",
        "            return losses\n",
        "        else:\n",
        "            # split outputs per level\n",
        "            split_head_outputs: Dict[str, List[Tensor]] = {}\n",
        "            for k in head_outputs:\n",
        "                split_head_outputs[k] = list(head_outputs[k].split(num_anchors_per_level, dim=1))\n",
        "            split_anchors = [list(a.split(num_anchors_per_level)) for a in anchors]\n",
        "\n",
        "            # compute the detections\n",
        "            detections = self.postprocess_detections(split_head_outputs, split_anchors, images.image_sizes)\n",
        "            return detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA1Nvz6jagyP"
      },
      "source": [
        "### Metrics and evaluation (2 pt.)\n",
        "\n",
        "#### Digit Accuracy (1 pt.)\n",
        "\n",
        "This method shoud accept `canvas: MnistCanvas` and `predicted_boxes: List[MnistBox]`, and output whether there is a direct matching between boxes from `MnistCanvas` and predictions. There is a direct matching if:\n",
        "\n",
        "- for all boxes from `canvas`, there exist precisely one box from `predicted_boxes` with a matching class and `iou` overlap greater than `0.5`,\n",
        "- the number of `canvas` boxes match `len(predicted_boxes)`.\n",
        "\n",
        "The method shoud output `1` if there is a matching and `0` otherwise.\n",
        "\n",
        "#### Evaluation function (1 pt.)\n",
        "\n",
        "Write an evaluation function for your model.\n",
        "If needed, perform the final NMS with `torchvision.ops.nms` (threshold at `iou`) and remove redundant boxes with scores smaller than `min_score`.\n",
        "Then, calculate the average `DigitAccuracy` for each.\n",
        "You may experiment with parameters for this method, but the default ones are fine (this is not subject of our grading).\n",
        "If you are fine what you achiveded with `postprocess_detections`, you may focus solely on evaluation (although playing with this second stage NMS and score thresholding might be useful for diagnostics).\n",
        "\n",
        "The output of the method is the average digit accuracy on the test set. \n",
        "Use it to track your model performance over epochs.\n",
        "\n",
        "In principle, you can use different `iou` and `min_score`, although itd be slightly preferred to use the defaults here. Itll ease the comparison, we were able to get around 60% in 15 epochs with these values. With that said, if you tune these values for your model to improve the score there will be no point lost.\n",
        "unless you will do some sort of a hack to exploit the metric in a malicious way, but that will perhaps mean that theres something wrong in other parts of the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_targets(dataset: List[MnistCanvas]):\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  for input in dataset:\n",
        "    boxes = input.boxes\n",
        "    labels = torch.tensor([box.class_nb for box in boxes], dtype=torch.int32, device=DEVICE)\n",
        "    inputs.append(input.get_torch_tensor())\n",
        "    x_mins = [b.x_min for b in boxes]\n",
        "    y_mins = [b.y_min for b in boxes]\n",
        "    x_maxs = [b.x_max for b in boxes]\n",
        "    y_maxs = [b.y_max for b in boxes]\n",
        "\n",
        "    d = {}\n",
        "    ten = torch.tensor([[x_min, y_min, x_max, y_max] for x_min, y_min, x_max, y_max in zip(x_mins, y_mins, x_maxs, y_maxs)], dtype=torch.int32, device=DEVICE)\n",
        "    d[\"boxes\"] = ten\n",
        "    d[\"labels\"] = labels\n",
        "    targets.append(d)\n",
        "  return inputs, targets"
      ],
      "metadata": {
        "id": "duTtHwPHXVNM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot example results of matched and mismatched predictions\n",
        "def plot_results(canvas, bools):\n",
        "  good = []\n",
        "  bad = []\n",
        "  for c, b in zip(canvas, bools):\n",
        "    if b:\n",
        "      good.append(c)\n",
        "    else:\n",
        "      bad.append(c)\n",
        "  total = len(good) + len(bad)\n",
        "  print('Accuracy of the network on the {} test images: {} %'.format(\n",
        "                    total, 100 * (len(good)) / total))\n",
        "  \n",
        "  print('Correct object detection:')\n",
        "  for g in good:\n",
        "    g.plot()\n",
        "\n",
        "  print('Incorrect object detection:')\n",
        "#  for b in bad:\n",
        "#    b.plot()"
      ],
      "metadata": {
        "id": "F3fH9tQpS-Zk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bfRZnbAP-XGG"
      },
      "outputs": [],
      "source": [
        "class DigitAccuracy:\n",
        "    def compute_metric  (\n",
        "        self,\n",
        "        predicted_boxes: List[MnistBox],\n",
        "        canvas: MnistCanvas,\n",
        "        iou: float = 0.5,\n",
        "    ):\n",
        "        # TODO: your code here\n",
        "        ################################################################################################\n",
        "        \n",
        "        if len(predicted_boxes) != len(canvas.boxes):\n",
        "          return False\n",
        "\n",
        "        for true_box in canvas.boxes:\n",
        "          match_box = False\n",
        "          for predicted_box in predicted_boxes:\n",
        "            if true_box.eq(predicted_box, iou):\n",
        "              if match_box:\n",
        "                return False\n",
        "              match_box = True\n",
        "          if not match_box:\n",
        "            return False\n",
        "            \n",
        "        return True\n",
        "        ################################################################################################\n",
        "        # end of your code\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module, TEST_CANVAS, min_score: float = .2, iou: float = .05) -> float:\n",
        "    # TODO: your code here\n",
        "    ################################################################################################\n",
        "    \n",
        "    test_inputs, test_targets = get_targets(TEST_CANVAS)\n",
        "    model_outputs = [model(inp, [tar]) for inp, tar in zip(test_inputs, test_targets)]\n",
        "    #print(model_outputs[0][0].keys())\n",
        "    scores = [d[0]['scores'] for d in model_outputs]\n",
        "    boxes = [d[0]['boxes'] for d in model_outputs]\n",
        "    labels = [d[0]['labels'] for d in model_outputs]\n",
        "\n",
        "    final_boxes = []\n",
        "\n",
        "    for box, score, label, canvas in zip(boxes, scores, labels, TEST_CANVAS):\n",
        "      #print(f\"b: {len(box), len(score)}\")\n",
        "      ids = torchvision.ops.nms(box, score, iou_threshold=iou)\n",
        "      top_boxes = []\n",
        "      for b, s, l in zip(box[ids], score[ids], label[ids]):\n",
        "        if s > min_score:\n",
        "          top_box = MnistBox(b[0], b[1], b[2], b[3], class_nb=l)\n",
        "          top_boxes.append(top_box)\n",
        "\n",
        "      new_canvas = MnistCanvas(image=np.zeros((128, 128)), boxes=top_boxes)\n",
        "     # print(f\"new: {len(new_canvas.boxes)}, {len(canvas.boxes)}\")\n",
        "     # print(f\"c: {canvas}\")\n",
        "      new_canvas.plot()\n",
        "      canvas.plot()\n",
        "      fin = DigitAccuracy().compute_metric(top_boxes, canvas)\n",
        "      final_boxes.append(fin)\n",
        "\n",
        "    plot_results(TEST_CANVAS, final_boxes)\n",
        "\n",
        "\n",
        "    return final_boxes\n",
        "\n",
        "    ################################################################################################\n",
        "    # end of your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15n5w-hhvRbS"
      },
      "source": [
        "### Train your model (3pt)\n",
        "\n",
        "One should use all classes defined above to train the model.\n",
        "\n",
        "- Train the model. A passing threshold is `10%` of a `DigitAccuracy` on a `TEST_CANVAS` data (2 pt.).\n",
        "\n",
        "What does the target variable look like?\n",
        "So, as in the typehint, the target is `List[Dict[str, Tensor]]`. Assuming a 1-element batch, it should then be a 1-element list containing a dictionary with two keys: `boxes` and their corresponding `labels`. The values are tensors. Assuming that we have 5 boxes in GT, the shapes are `(5, 4)` for boxes and (5) for labels. Naturally, you have to fill these tensors using values from `MnistCanvas`.\n",
        "\n",
        "**Hint:** Training can take a while to achieve the expected accuracy. It is normal that for many epochs at the beginning accuracy is constantly $0$. Do not worry as long as the loss is on average decreasing across epochs. You may want to reduce number of digit classes (for example only to generate `0`s on canvas) to test the convergence (the hyperparameters might change, though!). A model with around 500k parameters should be able to hit 10% of the metric in 20 minutes (tested on a 2021 MacBook on CPU). On Google Colab with GPU it will be matter of 2 minutes, but notice that the free GPU is limited.\n",
        "\n",
        "**Hint:** Use the 1-element batches. Some portions of the code are not ready for higher values.\n",
        "\n",
        "**Even more important hint:** Pay attention to the ordering of the X/Y values of `get_torch_tensor` and align it with your target!\n",
        "\n",
        "Good luck and have fun!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "import torch\n",
        "import torchvision\n",
        "import pandas as pd\n",
        "\n",
        "TEST_SEED = 42 # DO NOT CHANGE THIS LINE.\n",
        "np.random.seed(TEST_SEED)\n",
        "\n",
        "N_TRAINING_EXAMPLES = 500\n",
        "LR = 0.0004 # for SGD with momentum=0.9\n",
        "EPOCHS = 16\n",
        "STRIDES = [32, 64, 128]\n",
        "CONVS_IN_HEADS = 4\n",
        "OUT_CHANNELS = 64\n",
        "LABELS = list(range(5))  # lower it for quick convergence testing\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "GyWNGqHhTJfa"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict\n",
        "# Plot particular losses and evaluation score per\n",
        "def plot_losses(xs, subplots: Dict[str, List[float]]):\n",
        "  plt.clf()\n",
        "  for label, subplot in subplots.items():\n",
        "   # subplot.Tensor.cpu(torch.contiguous_format)\n",
        "#    plt.plot(xs, subplot, label=label)\n",
        "    pass\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Number of epochs')\n",
        "  plt.grid()\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "0Xx90TE6S-rL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self, train_canvas_size: int, epochs: int, lr: int):\n",
        "    self.train_canvas_size = train_canvas_size\n",
        "    self.epochs = epochs\n",
        "    self.digit_accuracy = DigitAccuracy()\n",
        "\n",
        "    self.train_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "    self.train_inputs, self.train_targets = get_targets(self.train_set)\n",
        "\n",
        "    self.test_set = [\n",
        "        get_random_canvas(\n",
        "            digits=TEST_DIGITS,\n",
        "            classes=TEST_CLASSES,\n",
        "            labels=LABELS,\n",
        "        )\n",
        "        for _ in range(train_canvas_size)\n",
        "    ]\n",
        "\n",
        "    self.anchor_sizes = tuple([(x,) for x in STRIDES])  # equal to strides of multi-level feature map\n",
        "    self.aspect_ratios = ((1.0,),) * len(anchor_sizes)  # set only one \"anchor\" per location\n",
        "    self.anchor_generator = AnchorGenerator(anchor_sizes, aspect_ratios)\n",
        "\n",
        "    self.fcos = FCOS(\n",
        "        backbone = BackboneWithFPN(strides=STRIDES, out_channels=OUT_CHANNELS), \n",
        "        num_classes = len(LABELS), \n",
        "        image_mean = [0.1],\n",
        "        image_std = [0.2],\n",
        "        num_convs_in_heads = CONVS_IN_HEADS, \n",
        "        anchor_generator = anchor_generator,\n",
        "        detections_per_img = 100,\n",
        "        training = True,\n",
        "        )\n",
        "    self.fcos.to(DEVICE)\n",
        "    self.optimizer = optim.Adam(self.fcos.parameters(), lr=lr)\n",
        "\n",
        "# TODO: write your code here\n",
        "################################################################################################\n",
        "  def train(self):\n",
        "    train_loss, test_loss, test_acc = [], [], []\n",
        "    correct, not_correct = 0, 0\n",
        "    cl_loss = []\n",
        "    ctr_loss = []\n",
        "    reg_loss = []\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "        temp_loss = 0\n",
        "        temp_cl_loss = 0\n",
        "        temp_ctr_loss = 0\n",
        "        temp_reg_loss = 0\n",
        "        self.fcos.train()\n",
        "        for i in range(len(self.train_inputs)):\n",
        "          train_input = self.train_inputs[i]\n",
        "          train_target = self.train_targets[i]\n",
        "          model_output = self.fcos.forward(train_input, [train_target])\n",
        "          cl = model_output[\"classification\"]\n",
        "          reg = model_output[\"bbox_regression\"]\n",
        "          ctr = model_output[\"bbox_ctrness\"]\n",
        "          loss = cl + reg + ctr\n",
        "          temp_loss += loss\n",
        "          temp_cl_loss += cl\n",
        "          temp_reg_loss += reg\n",
        "          temp_ctr_loss += ctr\n",
        "\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "        \n",
        "        train_loss.append(temp_loss)\n",
        "        cl_loss.append(temp_cl_loss)\n",
        "        ctr_loss.append(temp_ctr_loss)\n",
        "        reg_loss.append(temp_reg_loss)\n",
        "        print(f'Epoch: {epoch}, loss: {temp_loss}, classification: {temp_cl_loss}')\n",
        "        print(f'bbox_ctrness: {temp_ctr_loss}, bbox_regression: {temp_reg_loss}')\n",
        "\n",
        "    self.fcos.eval()\n",
        "    with torch.no_grad():\n",
        "      evaluate(self.fcos, self.test_set)\n",
        "\n",
        "    plot_losses(len(train_loss), {\"train_loss\": train_loss, \"cl_loss\": cl_loss, \n",
        "                                  \"ctr_loss\": ctr_loss, \"reg_loss\": reg_loss})\n",
        "    return train_loss, test_loss, test_acc\n",
        "\n",
        "################################################################################################"
      ],
      "metadata": {
        "id": "u5kwfx3RTGGq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot example results of matched and mismatched predictions (0.5 pt.).\n",
        "- Plot particular losses and evaluation score per (0.5 pt.).\n"
      ],
      "metadata": {
        "id": "XmZgfAQbXIxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(1000, 20, lr=0.0004)\n",
        "train_loss, test_loss, test_acc = trainer.train()\n"
      ],
      "metadata": {
        "id": "OVw6fdTG1JPZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b882918d-da65-4240-cb7e-8c6c936bfe94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, loss: 1327.750732421875, classification: 268.1431884765625\n",
            "bbox_ctrness: 614.4462890625, bbox_regression: 445.1612243652344\n",
            "Epoch: 1, loss: 998.7020874023438, classification: 130.10958862304688\n",
            "bbox_ctrness: 595.53759765625, bbox_regression: 273.0548095703125\n",
            "Epoch: 2, loss: 916.1184692382812, classification: 88.82864379882812\n",
            "bbox_ctrness: 590.6956176757812, bbox_regression: 236.59422302246094\n",
            "Epoch: 3, loss: 869.8988037109375, classification: 66.78722381591797\n",
            "bbox_ctrness: 588.19287109375, bbox_regression: 214.91827392578125\n",
            "Epoch: 4, loss: 837.4880981445312, classification: 50.90030288696289\n",
            "bbox_ctrness: 586.5026245117188, bbox_regression: 200.0858612060547\n",
            "Epoch: 5, loss: 813.9441528320312, classification: 42.134952545166016\n",
            "bbox_ctrness: 585.1925048828125, bbox_regression: 186.61715698242188\n",
            "Epoch: 6, loss: 798.65673828125, classification: 36.4391975402832\n",
            "bbox_ctrness: 584.2291259765625, bbox_regression: 177.98867797851562\n",
            "Epoch: 7, loss: 784.4420776367188, classification: 31.346460342407227\n",
            "bbox_ctrness: 583.3961181640625, bbox_regression: 169.70018005371094\n",
            "Epoch: 8, loss: 770.4546508789062, classification: 25.693817138671875\n",
            "bbox_ctrness: 582.6790161132812, bbox_regression: 162.08265686035156\n",
            "Epoch: 9, loss: 760.3284912109375, classification: 22.39430046081543\n",
            "bbox_ctrness: 582.0701904296875, bbox_regression: 155.86351013183594\n",
            "Epoch: 10, loss: 761.1329345703125, classification: 26.88804817199707\n",
            "bbox_ctrness: 581.7175903320312, bbox_regression: 152.52740478515625\n",
            "Epoch: 11, loss: 745.4448852539062, classification: 18.964757919311523\n",
            "bbox_ctrness: 581.0604248046875, bbox_regression: 145.4191436767578\n",
            "Epoch: 12, loss: 741.0443115234375, classification: 18.96077537536621\n",
            "bbox_ctrness: 580.6660766601562, bbox_regression: 141.41737365722656\n",
            "Epoch: 13, loss: 732.677734375, classification: 16.12628173828125\n",
            "bbox_ctrness: 580.1753540039062, bbox_regression: 136.3756103515625\n",
            "Epoch: 14, loss: 735.5438232421875, classification: 21.255739212036133\n",
            "bbox_ctrness: 579.9296264648438, bbox_regression: 134.3576202392578\n",
            "Epoch: 15, loss: 722.1156616210938, classification: 13.044774055480957\n",
            "bbox_ctrness: 579.4005737304688, bbox_regression: 129.6699676513672\n",
            "Epoch: 16, loss: 729.7628173828125, classification: 20.831523895263672\n",
            "bbox_ctrness: 579.423095703125, bbox_regression: 129.5086212158203\n",
            "Epoch: 17, loss: 723.16650390625, classification: 16.262582778930664\n",
            "bbox_ctrness: 579.0159912109375, bbox_regression: 127.88768005371094\n",
            "Epoch: 18, loss: 711.9357299804688, classification: 10.61751937866211\n",
            "bbox_ctrness: 578.6192626953125, bbox_regression: 122.69904327392578\n",
            "Epoch: 19, loss: 706.3617553710938, classification: 9.536553382873535\n",
            "bbox_ctrness: 578.3803100585938, bbox_regression: 118.44551849365234\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-a3fb180336e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0004\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-e8dbb2d2363d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m       \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfcos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     plot_losses(len(train_loss), {\"train_loss\": train_loss, \"cl_loss\": cl_loss, \n",
            "\u001b[0;32m<ipython-input-16-d0b4216cd2ab>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, TEST_CANVAS, min_score, iou)\u001b[0m\n\u001b[1;32m     52\u001b[0m      \u001b[0;31m# print(f\"new: {len(new_canvas.boxes)}, {len(canvas.boxes)}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m      \u001b[0;31m# print(f\"c: {canvas}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mnew_canvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mfin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDigitAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanvas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fec35f5a729f>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, boxes)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_on_ax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-fec35f5a729f>\u001b[0m in \u001b[0;36mplot_on_ax\u001b[0;34m(self, ax, color)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mplot_on_ax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         ax.add_patch(\n\u001b[0m\u001b[1;32m     47\u001b[0m             patches.Rectangle(\n\u001b[1;32m     48\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_min\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36madd_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   1917\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1919\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_patch_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1920\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_update_patch_limits\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0mvertices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1939\u001b[0;31m             \u001b[0mxys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 patch_to_data = (patch.get_data_transform() -\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rect_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36m_update_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \"\"\"\n\u001b[1;32m    755\u001b[0m         \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_extents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0mrot_trans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mrot_trans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotate_deg_around\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mfrom_extents\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0maxis\u001b[0m \u001b[0mincreases\u001b[0m \u001b[0mupwards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \"\"\"\n\u001b[0;32m--> 789\u001b[0;31m         \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mBbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;31m# Wrap Numpy array again in a suitable tensor when done, to support e.g.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Report**\n"
      ],
      "metadata": {
        "id": "KqupNuvv3S3i"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5qdCBH9PkT2C"
      ],
      "provenance": [],
      "history_visible": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "3d597f4c481aa0f25dceb95d2a0067e73c0966dcbd003d741d821a7208527ecf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}